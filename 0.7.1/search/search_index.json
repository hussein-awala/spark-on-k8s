{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Spark On Kubernetes","text":"<p>Spark on Kubernetes is a python package that makes it easy to submit and manage spark apps on Kubernetes. It provides a Python client that can be used to submit apps in your API or scheduler of choice, and a CLI that can be used to submit apps from the command line, instead of  using spark-submit.</p> <p>It also provides an optional REST API with a web UI that can be used to list and manage apps, and access the spark UI through the reverse proxy.</p>"},{"location":"helm-chart/","title":"Chart: spark-on-k8s","text":"<p>A Helm chart for spark-on-k8s API and Web UI.</p> <p>To deploy the spark-on-k8s API and Web UI, use the following command:</p> <pre><code>helm repo add spark-on-k8s https://hussein.awala.fr/spark-on-k8s-chart\nhelm repo update\nhelm install &lt;release-name&gt; spark-on-k8s/spark-on-k8s --values &lt;path-to-values-file&gt;\n</code></pre>"},{"location":"helm-chart/#sections","title":"Sections","text":""},{"location":"helm-chart/#root","title":"root","text":"<p>alues that are defined in the root of the values file.</p>"},{"location":"helm-chart/#image","title":"image","text":"<p>Docker image configuration for the Spark-on-K8S deployment</p>"},{"location":"helm-chart/#serviceaccount","title":"serviceAccount","text":"<p>The service account configuration for the Spark-on-K8S deployment</p>"},{"location":"helm-chart/#service","title":"service","text":"<p>The configuration for the Spark-on-K8S service</p>"},{"location":"helm-chart/#ingress","title":"ingress","text":"<p>The configuration for the Spark-on-K8S ingress</p>"},{"location":"helm-chart/#autoscaling","title":"autoscaling","text":"<p>The autoscaling configuration for the Spark-on-K8S deployment</p>"},{"location":"helm-chart/#sparkhistory","title":"sparkHistory","text":"<p>Configuration for the Spark History Server</p> <p>application chart version (0.3.0) running application version (0.6.0)</p>"},{"location":"helm-chart/config/autoscaling/","title":"autoscaling","text":"<pre><code>The autoscaling configuration for the Spark-on-K8S deployment\n</code></pre> <p>Type: object</p> <p>Path: <code>autoscaling</code></p>"},{"location":"helm-chart/config/autoscaling/#enabled","title":"enabled","text":"<pre><code>Specifies whether autoscaling should be enabled\n</code></pre> <p>Default: <code>false</code></p> <p>Type: boolean</p> <p>Path: <code>autoscaling.enabled</code></p>"},{"location":"helm-chart/config/autoscaling/#minreplicas","title":"minReplicas","text":"<pre><code>The minimum number of replicas\n</code></pre> <p>Default: <code>1</code></p> <p>Type: number</p> <p>Path: <code>autoscaling.minReplicas</code></p>"},{"location":"helm-chart/config/autoscaling/#maxreplicas","title":"maxReplicas","text":"<pre><code>The maximum number of replicas\n</code></pre> <p>Default: <code>100</code></p> <p>Type: number</p> <p>Path: <code>autoscaling.maxReplicas</code></p>"},{"location":"helm-chart/config/autoscaling/#targetcpuutilizationpercentage","title":"targetCPUUtilizationPercentage","text":"<pre><code>The target CPU utilization percentage\n</code></pre> <p>Default: <code>80</code></p> <p>Type: number</p> <p>Path: <code>autoscaling.targetCPUUtilizationPercentage</code></p>"},{"location":"helm-chart/config/image/","title":"image","text":"<pre><code>Docker image configuration for the Spark-on-K8S deployment\n</code></pre> <p>Type: object</p> <p>Path: <code>image</code></p>"},{"location":"helm-chart/config/image/#repository","title":"repository","text":"<pre><code>The Docker image repository\n</code></pre> <p>Default: <code>ghcr.io/hussein-awala/spark-on-k8s</code></p> <p>Type: string</p> <p>Path: <code>image.repository</code></p>"},{"location":"helm-chart/config/image/#pullpolicy","title":"pullPolicy","text":"<pre><code>The deployment docker image pull policy\n</code></pre> <p>Default: <code>IfNotPresent</code></p> <p>Type: string</p> <p>Path: <code>image.pullPolicy</code></p>"},{"location":"helm-chart/config/image/#tag","title":"tag","text":"<pre><code>The Docker image tag\n</code></pre> <p>Default: <code>latest</code></p> <p>Type: string</p> <p>Path: <code>image.tag</code></p>"},{"location":"helm-chart/config/ingress/","title":"ingress","text":"<pre><code>The configuration for the Spark-on-K8S ingress\n</code></pre> <p>Type: object</p> <p>Path: <code>ingress</code></p>"},{"location":"helm-chart/config/ingress/#enabled","title":"enabled","text":"<pre><code>Specifies whether an ingress should be created\n</code></pre> <p>Default: <code>false</code></p> <p>Type: boolean</p> <p>Path: <code>ingress.enabled</code></p>"},{"location":"helm-chart/config/ingress/#classname","title":"className","text":"<pre><code>The ingress class to use\n</code></pre> <p>Default: <code>\"\"</code></p> <p>Type: string</p> <p>Path: <code>ingress.className</code></p>"},{"location":"helm-chart/config/ingress/#annotations","title":"annotations","text":"<pre><code>The list of ingress annotations\n</code></pre> <p>Default: <code>{}</code></p> <p>Type: string</p> <p>Path: <code>ingress.annotations</code></p>"},{"location":"helm-chart/config/ingress/#hosts","title":"hosts","text":"<pre><code>The list of hosts to be added to the ingress\n</code></pre> <p>Type: list</p> <p>Path: <code>ingress.hosts</code></p>"},{"location":"helm-chart/config/ingress/#0","title":"0","text":"<p>Type: object</p> <p>Path: <code>ingress.hosts.0</code></p>"},{"location":"helm-chart/config/ingress/#host","title":"host","text":"<p>Default: <code>chart-example.local</code></p> <p>Type: string</p> <p>Path: <code>ingress.hosts.0.host</code></p>"},{"location":"helm-chart/config/ingress/#paths","title":"paths","text":"<p>Type: list</p> <p>Path: <code>ingress.hosts.0.paths</code></p>"},{"location":"helm-chart/config/ingress/#0_1","title":"0","text":"<p>Type: object</p> <p>Path: <code>ingress.hosts.0.paths.0</code></p>"},{"location":"helm-chart/config/ingress/#path","title":"path","text":"<p>Default: <code>/</code></p> <p>Type: string</p> <p>Path: <code>ingress.hosts.0.paths.0.path</code></p>"},{"location":"helm-chart/config/ingress/#pathtype","title":"pathType","text":"<p>Default: <code>ImplementationSpecific</code></p> <p>Type: string</p> <p>Path: <code>ingress.hosts.0.paths.0.pathType</code></p>"},{"location":"helm-chart/config/ingress/#tls","title":"tls","text":"<pre><code>The list of tls to be added to the ingress\n</code></pre> <p>Default: <code>[]</code></p> <p>Type: list</p> <p>Path: <code>ingress.tls</code></p>"},{"location":"helm-chart/config/root/","title":"Root","text":"<p>chart</p> <pre><code>alues that are defined in the root of the values file.\n</code></pre> <p>Type: object</p> <p>Path: ``</p>"},{"location":"helm-chart/config/root/#replicacount","title":"replicaCount","text":"<pre><code>Number of replicas for the Spark-on-K8S deployment if autoscaling is disabled\n</code></pre> <p>Default: <code>1</code></p> <p>Type: number</p> <p>Path: <code>replicaCount</code></p>"},{"location":"helm-chart/config/root/#imagepullsecrets","title":"imagePullSecrets","text":"<pre><code>List of imagePullSecrets to be used\n</code></pre> <p>Default: <code>[]</code></p> <p>Type: list</p> <p>Path: <code>imagePullSecrets</code></p>"},{"location":"helm-chart/config/root/#nameoverride","title":"nameOverride","text":"<pre><code>Override the chart name in the release resources\n</code></pre> <p>Default: <code>\"\"</code></p> <p>Type: string</p> <p>Path: <code>nameOverride</code></p>"},{"location":"helm-chart/config/root/#fullnameoverride","title":"fullnameOverride","text":"<pre><code>Override the fullname of the release resources\n</code></pre> <p>Default: <code>\"\"</code></p> <p>Type: string</p> <p>Path: <code>fullnameOverride</code></p>"},{"location":"helm-chart/config/root/#podannotations","title":"podAnnotations","text":"<pre><code>Annotations to be added to the Spark-on-K8S deployment pods\n</code></pre> <p>Default: <code>{}</code></p> <p>Type: string</p> <p>Path: <code>podAnnotations</code></p>"},{"location":"helm-chart/config/root/#podlabels","title":"podLabels","text":"<pre><code>Labels to be added to the Spark-on-K8S deployment pods\n</code></pre> <p>Default: <code>{}</code></p> <p>Type: string</p> <p>Path: <code>podLabels</code></p>"},{"location":"helm-chart/config/root/#podsecuritycontext","title":"podSecurityContext","text":"<pre><code>The pod security context for the Spark-on-K8S deployment\n</code></pre> <p>Default: <code>{}</code></p> <p>Type: string</p> <p>Path: <code>podSecurityContext</code></p>"},{"location":"helm-chart/config/root/#securitycontext","title":"securityContext","text":"<pre><code>The security context for the Spark-on-K8S deployment\n</code></pre> <p>Default: <code>{}</code></p> <p>Type: string</p> <p>Path: <code>securityContext</code></p>"},{"location":"helm-chart/config/root/#resources","title":"resources","text":"<pre><code>The resource configuration for the Spark-on-K8S deployment\n</code></pre> <p>Default: <code>{}</code></p> <p>Type: string</p> <p>Path: <code>resources</code></p>"},{"location":"helm-chart/config/root/#volumes","title":"volumes","text":"<pre><code>The list of volumes to be added to the Spark-on-K8S deployment pods\n</code></pre> <p>Default: <code>[]</code></p> <p>Type: list</p> <p>Path: <code>volumes</code></p>"},{"location":"helm-chart/config/root/#volumemounts","title":"volumeMounts","text":"<pre><code>The list of volume mounts to be added to the Spark-on-K8S deployment pods\n</code></pre> <p>Default: <code>[]</code></p> <p>Type: list</p> <p>Path: <code>volumeMounts</code></p>"},{"location":"helm-chart/config/root/#nodeselector","title":"nodeSelector","text":"<pre><code>The node selector for the Spark-on-K8S deployment\n</code></pre> <p>Default: <code>{}</code></p> <p>Type: string</p> <p>Path: <code>nodeSelector</code></p>"},{"location":"helm-chart/config/root/#tolerations","title":"tolerations","text":"<pre><code>The tolerations for the Spark-on-K8S deployment\n</code></pre> <p>Default: <code>[]</code></p> <p>Type: list</p> <p>Path: <code>tolerations</code></p>"},{"location":"helm-chart/config/root/#affinity","title":"affinity","text":"<pre><code>The affinity for the Spark-on-K8S deployment\n</code></pre> <p>Default: <code>{}</code></p> <p>Type: string</p> <p>Path: <code>affinity</code></p>"},{"location":"helm-chart/config/root/#env","title":"env","text":"<pre><code>A list of environment variables to be added to the Spark-on-K8S deployment\n</code></pre> <p>Default: <code>[]</code></p> <p>Type: list</p> <p>Path: <code>env</code></p>"},{"location":"helm-chart/config/root/#namespaces","title":"namespaces","text":"<pre><code>List of namespaces to be managed by the Spark-on-K8S service\nIf equal to \"*\", all namespaces will be managed, then a cluster role binding will\nbe created. Otherwise, you can provide a list of namespaces as a string separated\nby commas in the form \"namespace1,namespace2,namespace3\", in this case, a role\nbinding will be created for each namespace\n</code></pre> <p>Default: <code>\"*\"</code></p> <p>Type: string</p> <p>Path: <code>namespaces</code></p>"},{"location":"helm-chart/config/root/#readonly","title":"readOnly","text":"<pre><code>If true, the Spark-on-K8S service will be able to read pods, services, pods logs,\nand if false, it will be able to exec into pods and create/delete pods and services.\n</code></pre> <p>Default: <code>true</code></p> <p>Type: boolean</p> <p>Path: <code>readOnly</code></p>"},{"location":"helm-chart/config/service/","title":"service","text":"<pre><code>The configuration for the Spark-on-K8S service\n</code></pre> <p>Type: object</p> <p>Path: <code>service</code></p>"},{"location":"helm-chart/config/service/#type","title":"type","text":"<pre><code>The type of service to create\n</code></pre> <p>Default: <code>ClusterIP</code></p> <p>Type: string</p> <p>Path: <code>service.type</code></p>"},{"location":"helm-chart/config/service/#port","title":"port","text":"<pre><code>The port to expose the service on\n</code></pre> <p>Default: <code>8000</code></p> <p>Type: number</p> <p>Path: <code>service.port</code></p>"},{"location":"helm-chart/config/serviceAccount/","title":"serviceAccount","text":"<pre><code>The service account configuration for the Spark-on-K8S deployment\n</code></pre> <p>Type: object</p> <p>Path: <code>serviceAccount</code></p>"},{"location":"helm-chart/config/serviceAccount/#create","title":"create","text":"<pre><code>Specifies whether a service account should be created\n</code></pre> <p>Default: <code>true</code></p> <p>Type: boolean</p> <p>Path: <code>serviceAccount.create</code></p>"},{"location":"helm-chart/config/serviceAccount/#automount","title":"automount","text":"<pre><code>Specifies whether the service account token should be auto mounted\n</code></pre> <p>Default: <code>true</code></p> <p>Type: boolean</p> <p>Path: <code>serviceAccount.automount</code></p>"},{"location":"helm-chart/config/serviceAccount/#annotations","title":"annotations","text":"<pre><code>Annotations to add to the service account\n</code></pre> <p>Default: <code>{}</code></p> <p>Type: string</p> <p>Path: <code>serviceAccount.annotations</code></p>"},{"location":"helm-chart/config/serviceAccount/#name","title":"name","text":"<pre><code>The name of the service account to use.\n</code></pre> <p>Default: <code>\"\"</code></p> <p>Type: string</p> <p>Path: <code>serviceAccount.name</code></p>"},{"location":"helm-chart/config/sparkHistory/","title":"sparkHistory","text":"<pre><code>Configuration for the Spark History Server\n</code></pre> <p>Type: object</p> <p>Path: <code>sparkHistory</code></p>"},{"location":"helm-chart/config/sparkHistory/#enabled","title":"enabled","text":"<pre><code>Specifies whether the Spark History Server should be enabled\n</code></pre> <p>Default: <code>false</code></p> <p>Type: boolean</p> <p>Path: <code>sparkHistory.enabled</code></p>"},{"location":"helm-chart/config/sparkHistory/#replicacount","title":"replicaCount","text":"<pre><code>The number of replicas for the Spark History Server if autoscaling is disabled\n</code></pre> <p>Default: <code>1</code></p> <p>Type: number</p> <p>Path: <code>sparkHistory.replicaCount</code></p>"},{"location":"helm-chart/config/sparkHistory/#autoscaling","title":"autoscaling","text":"<pre><code>The autoscaling configuration for the Spark History Server\n</code></pre> <p>Type: object</p> <p>Path: <code>sparkHistory.autoscaling</code></p>"},{"location":"helm-chart/config/sparkHistory/#enabled_1","title":"enabled","text":"<pre><code>Specifies whether autoscaling should be enabled\n</code></pre> <p>Default: <code>false</code></p> <p>Type: boolean</p> <p>Path: <code>sparkHistory.autoscaling.enabled</code></p>"},{"location":"helm-chart/config/sparkHistory/#minreplicas","title":"minReplicas","text":"<pre><code>The minimum number of replicas\n</code></pre> <p>Default: <code>1</code></p> <p>Type: number</p> <p>Path: <code>sparkHistory.autoscaling.minReplicas</code></p>"},{"location":"helm-chart/config/sparkHistory/#maxreplicas","title":"maxReplicas","text":"<pre><code>The maximum number of replicas\n</code></pre> <p>Default: <code>100</code></p> <p>Type: number</p> <p>Path: <code>sparkHistory.autoscaling.maxReplicas</code></p>"},{"location":"helm-chart/config/sparkHistory/#targetcpuutilizationpercentage","title":"targetCPUUtilizationPercentage","text":"<pre><code>The target CPU utilization percentage\n</code></pre> <p>Default: <code>80</code></p> <p>Type: number</p> <p>Path: <code>sparkHistory.autoscaling.targetCPUUtilizationPercentage</code></p>"},{"location":"helm-chart/config/sparkHistory/#image","title":"image","text":"<pre><code>Docker image configuration for the Spark History Server\n</code></pre> <p>Type: object</p> <p>Path: <code>sparkHistory.image</code></p>"},{"location":"helm-chart/config/sparkHistory/#repository","title":"repository","text":"<pre><code>The Docker image repository\n</code></pre> <p>Default: <code>ghcr.io/hussein-awala/spark-history</code></p> <p>Type: string</p> <p>Path: <code>sparkHistory.image.repository</code></p>"},{"location":"helm-chart/config/sparkHistory/#pullpolicy","title":"pullPolicy","text":"<pre><code>The deployment docker image pull policy\n</code></pre> <p>Default: <code>IfNotPresent</code></p> <p>Type: string</p> <p>Path: <code>sparkHistory.image.pullPolicy</code></p>"},{"location":"helm-chart/config/sparkHistory/#tag","title":"tag","text":"<pre><code>The Docker image tag\n</code></pre> <p>Default: <code>latest</code></p> <p>Type: string</p> <p>Path: <code>sparkHistory.image.tag</code></p>"},{"location":"helm-chart/config/sparkHistory/#env","title":"env","text":"<pre><code>Annotations to be added to the Spark History Server pods\n</code></pre> <p>Default: <code>[]</code></p> <p>Type: list</p> <p>Path: <code>sparkHistory.env</code></p>"},{"location":"helm-chart/config/sparkHistory/#resources","title":"resources","text":"<pre><code>The resource configuration for the Spark History Server\n</code></pre> <p>Default: <code>{}</code></p> <p>Type: string</p> <p>Path: <code>sparkHistory.resources</code></p>"},{"location":"helm-chart/config/sparkHistory/#volumemounts","title":"volumeMounts","text":"<pre><code>A list of volume mounts to be added to the Spark History Server pods\n</code></pre> <p>Default: <code>[]</code></p> <p>Type: list</p> <p>Path: <code>sparkHistory.volumeMounts</code></p>"},{"location":"helm-chart/config/sparkHistory/#volumes","title":"volumes","text":"<pre><code>A list of volumes to be added to the Spark History Server pods\n</code></pre> <p>Default: <code>[]</code></p> <p>Type: list</p> <p>Path: <code>sparkHistory.volumes</code></p>"},{"location":"helm-chart/config/sparkHistory/#podannotations","title":"podAnnotations","text":"<pre><code>Annotations to be added to the Spark History Server pods\n</code></pre> <p>Default: <code>{}</code></p> <p>Type: string</p> <p>Path: <code>sparkHistory.podAnnotations</code></p>"},{"location":"helm-chart/config/sparkHistory/#podlabels","title":"podLabels","text":"<pre><code>Labels to be added to the Spark History Server pods\n</code></pre> <p>Default: <code>{}</code></p> <p>Type: string</p> <p>Path: <code>sparkHistory.podLabels</code></p>"},{"location":"helm-chart/config/sparkHistory/#podsecuritycontext","title":"podSecurityContext","text":"<pre><code>The pod security context for the Spark History Server\n</code></pre> <p>Default: <code>{}</code></p> <p>Type: string</p> <p>Path: <code>sparkHistory.podSecurityContext</code></p>"},{"location":"helm-chart/config/sparkHistory/#securitycontext","title":"securityContext","text":"<pre><code>The security context for the Spark History Server\n</code></pre> <p>Default: <code>{}</code></p> <p>Type: string</p> <p>Path: <code>sparkHistory.securityContext</code></p>"},{"location":"helm-chart/config/sparkHistory/#serviceaccount","title":"serviceAccount","text":"<pre><code>The service account configuration for the Spark History Server\n</code></pre> <p>Type: object</p> <p>Path: <code>sparkHistory.serviceAccount</code></p>"},{"location":"helm-chart/config/sparkHistory/#create","title":"create","text":"<pre><code>Specifies whether a service account should be created\n</code></pre> <p>Default: <code>true</code></p> <p>Type: boolean</p> <p>Path: <code>sparkHistory.serviceAccount.create</code></p>"},{"location":"helm-chart/config/sparkHistory/#automount","title":"automount","text":"<pre><code>Specifies whether the service account token should be auto mounted\n</code></pre> <p>Default: <code>true</code></p> <p>Type: boolean</p> <p>Path: <code>sparkHistory.serviceAccount.automount</code></p>"},{"location":"helm-chart/config/sparkHistory/#annotations","title":"annotations","text":"<pre><code>Annotations to add to the service account\n</code></pre> <p>Default: <code>{}</code></p> <p>Type: string</p> <p>Path: <code>sparkHistory.serviceAccount.annotations</code></p>"},{"location":"helm-chart/config/sparkHistory/#name","title":"name","text":"<pre><code>The name of the service account to use.\n</code></pre> <p>Default: <code>\"\"</code></p> <p>Type: string</p> <p>Path: <code>sparkHistory.serviceAccount.name</code></p>"},{"location":"helm-chart/config/sparkHistory/#service","title":"service","text":"<pre><code>The configuration for the Spark History Server service\n</code></pre> <p>Type: object</p> <p>Path: <code>sparkHistory.service</code></p>"},{"location":"helm-chart/config/sparkHistory/#type","title":"type","text":"<pre><code>The type of service to create\n</code></pre> <p>Default: <code>ClusterIP</code></p> <p>Type: string</p> <p>Path: <code>sparkHistory.service.type</code></p>"},{"location":"helm-chart/config/sparkHistory/#port","title":"port","text":"<pre><code>The port to expose the service on\n</code></pre> <p>Default: <code>18080</code></p> <p>Type: number</p> <p>Path: <code>sparkHistory.service.port</code></p>"},{"location":"helm-chart/config/sparkHistory/#configuration","title":"configuration","text":"<pre><code>The Spark configuration for the Spark History Server\n</code></pre> <p>Default: <code>\"\"</code></p> <p>Type: string</p> <p>Path: <code>sparkHistory.configuration</code></p>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>airflow<ul> <li>operator_links</li> <li>operators</li> <li>provider_info</li> <li>scripts<ul> <li>spark_sql</li> </ul> </li> <li>triggers</li> </ul> </li> <li>api<ul> <li>app</li> <li>apps</li> <li>auth<ul> <li>auth_manager</li> <li>base_auth_manager</li> <li>base_user</li> <li>resources</li> </ul> </li> <li>configuration</li> <li>main</li> <li>utils</li> <li>webserver<ul> <li>security</li> </ul> </li> </ul> </li> <li>cli<ul> <li>api</li> <li>app</li> <li>apps</li> <li>namespace</li> <li>options</li> </ul> </li> <li>client</li> <li>k8s<ul> <li>async_client</li> <li>sync_client</li> </ul> </li> <li>utils<ul> <li>app_manager</li> <li>async_app_manager</li> <li>configuration</li> <li>logging_mixin</li> <li>setup_namespace</li> <li>spark_app_status</li> <li>types</li> <li>warnings</li> </ul> </li> </ul>"},{"location":"reference/spark_on_k8s/client/","title":"client","text":""},{"location":"reference/spark_on_k8s/client/#spark_on_k8s.client.ExecutorInstances","title":"<code>ExecutorInstances</code>  <code>dataclass</code>","text":"<p>Number of executors to request</p> <p>Attributes:</p> Name Type Description <code>min</code> <code>int | None</code> <p>Minimum number of executors. If provided, dynamic allocation is enabled</p> <code>max</code> <code>int | None</code> <p>Maximum number of executors. If provided, dynamic allocation is enabled</p> <code>initial</code> <code>int | None</code> <p>Initial number of executors. If max and min are not provided, defaults to 2, dynamic allocation will be disabled and the number of executors will be fixed.</p> Source code in <code>spark_on_k8s/client.py</code> <pre><code>@dataclass(**KW_ONLY_DATACLASS)\nclass ExecutorInstances:\n    \"\"\"Number of executors to request\n\n    Attributes:\n        min: Minimum number of executors. If provided, dynamic allocation is enabled\n        max: Maximum number of executors. If provided, dynamic allocation is enabled\n        initial: Initial number of executors. If max and min are not provided, defaults to 2,\n            dynamic allocation will be disabled and the number of executors will be fixed.\n    \"\"\"\n\n    min: int | None = None\n    max: int | None = None\n    initial: int | None = None\n</code></pre>"},{"location":"reference/spark_on_k8s/client/#spark_on_k8s.client.PodResources","title":"<code>PodResources</code>  <code>dataclass</code>","text":"<p>Resources to request for the Spark driver and executors</p> <p>Attributes:</p> Name Type Description <code>cpu</code> <code>int</code> <p>Number of CPU cores to request</p> <code>memory</code> <code>int</code> <p>Amount of memory to request in MB</p> <code>memory_overhead</code> <code>int</code> <p>Amount of memory overhead to request in MB</p> Source code in <code>spark_on_k8s/client.py</code> <pre><code>@dataclass(**KW_ONLY_DATACLASS)\nclass PodResources:\n    \"\"\"Resources to request for the Spark driver and executors\n\n    Attributes:\n        cpu: Number of CPU cores to request\n        memory: Amount of memory to request in MB\n        memory_overhead: Amount of memory overhead to request in MB\n    \"\"\"\n\n    cpu: int = 1\n    memory: int = 1024\n    memory_overhead: int = 512\n</code></pre>"},{"location":"reference/spark_on_k8s/client/#spark_on_k8s.client.SparkAppWait","title":"<code>SparkAppWait</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enum for the Spark app waiter options</p> Source code in <code>spark_on_k8s/client.py</code> <pre><code>class SparkAppWait(str, Enum):\n    \"\"\"Enum for the Spark app waiter options\"\"\"\n\n    NO_WAIT = \"no_wait\"\n    WAIT = \"wait\"\n    LOG = \"log\"\n</code></pre>"},{"location":"reference/spark_on_k8s/client/#spark_on_k8s.client.SparkOnK8S","title":"<code>SparkOnK8S</code>","text":"<p>               Bases: <code>LoggingMixin</code></p> <p>Client for submitting Spark apps to Kubernetes</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spark_on_k8s.client import SparkOnK8S\n&gt;&gt;&gt; spark = SparkOnK8S()\n&gt;&gt;&gt; spark.submit_app(\n...     image=\"husseinawala/spark:v3.5.0\",\n...     app_path=\"local:///opt/spark/examples/jars/spark-examples_2.12-3.5.0.jar\",\n...     class_name=\"org.apache.spark.examples.SparkPi\",\n...     app_name=\"spark-pi\",\n...     app_arguments=[\"1000\"],\n...     namespace=\"spark\",\n...     service_account=\"spark\",\n...     app_waiter=\"log\",\n... )\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>k8s_client_manager</code> <code>KubernetesClientManager | None</code> <p>Kubernetes client manager to use for creating Kubernetes clients</p> <code>None</code> <code>logger_name</code> <code>str | None</code> <p>Name of the logger to use for logging, defaults to \"SparkOnK8S\"</p> <code>None</code> Source code in <code>spark_on_k8s/client.py</code> <pre><code>class SparkOnK8S(LoggingMixin):\n    \"\"\"Client for submitting Spark apps to Kubernetes\n\n    Examples:\n        &gt;&gt;&gt; from spark_on_k8s.client import SparkOnK8S\n        &gt;&gt;&gt; spark = SparkOnK8S()\n        &gt;&gt;&gt; spark.submit_app(\n        ...     image=\"husseinawala/spark:v3.5.0\",\n        ...     app_path=\"local:///opt/spark/examples/jars/spark-examples_2.12-3.5.0.jar\",\n        ...     class_name=\"org.apache.spark.examples.SparkPi\",\n        ...     app_name=\"spark-pi\",\n        ...     app_arguments=[\"1000\"],\n        ...     namespace=\"spark\",\n        ...     service_account=\"spark\",\n        ...     app_waiter=\"log\",\n        ... )\n\n    Args:\n        k8s_client_manager: Kubernetes client manager to use for creating Kubernetes clients\n        logger_name: Name of the logger to use for logging, defaults to \"SparkOnK8S\"\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        k8s_client_manager: KubernetesClientManager | None = None,\n        logger_name: str | None = None,\n    ):\n        super().__init__(logger_name=logger_name or \"SparkOnK8S\")\n        self.k8s_client_manager = k8s_client_manager or KubernetesClientManager()\n        self.app_manager = SparkAppManager(k8s_client_manager=self.k8s_client_manager)\n\n    def submit_app(\n        self,\n        *,\n        image: str | ArgNotSet = NOTSET,\n        app_path: str | ArgNotSet = NOTSET,\n        namespace: str | ArgNotSet = NOTSET,\n        service_account: str | ArgNotSet = NOTSET,\n        app_name: str | ArgNotSet = NOTSET,\n        spark_conf: dict[str, str] | ArgNotSet = NOTSET,\n        class_name: str | ArgNotSet = NOTSET,\n        app_arguments: list[str] | ArgNotSet = NOTSET,\n        app_id_suffix: Callable[[], str] | ArgNotSet = NOTSET,\n        app_waiter: Literal[\"no_wait\", \"wait\", \"log\"] | ArgNotSet = NOTSET,\n        image_pull_policy: Literal[\"Always\", \"Never\", \"IfNotPresent\"] | ArgNotSet = NOTSET,\n        ui_reverse_proxy: bool | ArgNotSet = NOTSET,\n        driver_resources: PodResources | ArgNotSet = NOTSET,\n        executor_resources: PodResources | ArgNotSet = NOTSET,\n        executor_instances: ExecutorInstances | ArgNotSet = NOTSET,\n        should_print: bool | ArgNotSet = NOTSET,\n        secret_values: dict[str, str] | ArgNotSet = NOTSET,\n        driver_env_vars_from_secrets: list[str] | ArgNotSet = NOTSET,\n        volumes: list[k8s.V1Volume] | ArgNotSet = NOTSET,\n        driver_volume_mounts: list[k8s.V1VolumeMount] | ArgNotSet = NOTSET,\n        executor_volume_mounts: list[k8s.V1VolumeMount] | ArgNotSet = NOTSET,\n        driver_node_selector: dict[str, str] | ArgNotSet = NOTSET,\n        executor_node_selector: dict[str, str] | ArgNotSet = NOTSET,\n        driver_annotations: dict[str, str] | ArgNotSet = NOTSET,\n        executor_annotations: dict[str, str] | ArgNotSet = NOTSET,\n        driver_labels: dict[str, str] | ArgNotSet = NOTSET,\n        executor_labels: dict[str, str] | ArgNotSet = NOTSET,\n        driver_tolerations: list[k8s.V1Toleration] | ArgNotSet = NOTSET,\n        executor_pod_template_path: str | ArgNotSet = NOTSET,\n    ) -&gt; str:\n        \"\"\"Submit a Spark app to Kubernetes\n\n        Args:\n            image: Docker image to use for the Spark driver and executors\n            app_path: Path to the application JAR / Python / R file\n            namespace: Kubernetes namespace to use, defaults to \"default\"\n            service_account: Kubernetes service account to use for the Spark driver,\n                defaults to \"spark\"\n            app_name: Name of the Spark application, defaults to a generated name as\n                `spark-app{app_id_suffix()}`\n            spark_conf: Dictionary of spark configuration to pass to the application\n            class_name: Name of the class to execute\n            app_arguments: List of arguments to pass to the application\n            app_id_suffix: Function to generate a suffix for the application ID, defaults to\n                `default_app_id_suffix`\n            app_waiter: How to wait for the app to finish. One of \"no_wait\", \"wait\", or \"log\"\n            image_pull_policy: Image pull policy for the driver and executors, defaults to \"IfNotPresent\"\n            ui_reverse_proxy: Whether to use a reverse proxy for the Spark UI, defaults to False\n            driver_resources: Resources to request for the Spark driver. Defaults to 1 CPU core, 1Gi of\n                memory and512Mi of memory overhead\n            executor_resources: Resources to request for the Spark executors. Defaults to 1 CPU core, 1Gi\n                of memory and 512Mi of memory overhead\n            executor_instances: Number of executors to request. If max and min are not provided, dynamic\n                allocation will be disabled and the number of executors will be fixed to initial or 2 if\n                initial is not provided. If max or min or both are provided, dynamic allocation will be\n                enabled and the number of executors will be between min and max (inclusive), and initial\n                will be the initial number of executors with a default of 0.\n            should_print: Whether to print logs instead of logging them, defaults to False\n            secret_values: Dictionary of secret values to pass to the application as environment variables\n            driver_env_vars_from_secrets: List of secret names to load environment variables from for\n                the driver\n            volumes: List of volumes to mount to the driver and/or executors\n            driver_volume_mounts: List of volume mounts to mount to the driver\n            executor_volume_mounts: List of volume mounts to mount to the executors\n            driver_node_selector: Node selector for the driver\n            executor_node_selector: Node selector for the executors\n            driver_tolerations: List of tolerations for the driver\n            executor_pod_template_path: Path to the executor pod template file\n\n        Returns:\n            Name of the Spark application pod\n        \"\"\"\n        if image is NOTSET:\n            if Configuration.SPARK_ON_K8S_DOCKER_IMAGE is None:\n                raise ValueError(\n                    \"Docker image is not set.\"\n                    \"Please set the image argument or the environment variable SPARK_ON_K8S_DOCKER_IMAGE\"\n                )\n            image = Configuration.SPARK_ON_K8S_DOCKER_IMAGE\n        if app_path is NOTSET:\n            if Configuration.SPARK_ON_K8S_APP_PATH is None:\n                raise ValueError(\n                    \"Application path is not set.\"\n                    \"Please set the app_path argument or the environment variable SPARK_ON_K8S_APP_PATH\"\n                )\n            app_path = Configuration.SPARK_ON_K8S_APP_PATH\n        if namespace is NOTSET:\n            namespace = Configuration.SPARK_ON_K8S_NAMESPACE\n        if service_account is NOTSET:\n            service_account = Configuration.SPARK_ON_K8S_SERVICE_ACCOUNT\n        if app_name is NOTSET:\n            app_name = Configuration.SPARK_ON_K8S_APP_NAME\n        if spark_conf is NOTSET:\n            spark_conf = Configuration.SPARK_ON_K8S_SPARK_CONF\n        if class_name is NOTSET:\n            class_name = Configuration.SPARK_ON_K8S_CLASS_NAME\n        if app_arguments is NOTSET:\n            app_arguments = Configuration.SPARK_ON_K8S_APP_ARGUMENTS\n        if app_id_suffix is NOTSET:\n            app_id_suffix = default_app_id_suffix\n        if app_waiter is NOTSET:\n            app_waiter = Configuration.SPARK_ON_K8S_APP_WAITER\n        if image_pull_policy is NOTSET:\n            image_pull_policy = Configuration.SPARK_ON_K8S_IMAGE_PULL_POLICY\n        if ui_reverse_proxy is NOTSET:\n            ui_reverse_proxy = Configuration.SPARK_ON_K8S_UI_REVERSE_PROXY\n        if driver_resources is NOTSET:\n            driver_resources = PodResources(\n                cpu=Configuration.SPARK_ON_K8S_DRIVER_CPU,\n                memory=Configuration.SPARK_ON_K8S_DRIVER_MEMORY,\n                memory_overhead=Configuration.SPARK_ON_K8S_DRIVER_MEMORY_OVERHEAD,\n            )\n        if executor_resources is NOTSET:\n            executor_resources = PodResources(\n                cpu=Configuration.SPARK_ON_K8S_EXECUTOR_CPU,\n                memory=Configuration.SPARK_ON_K8S_EXECUTOR_MEMORY,\n                memory_overhead=Configuration.SPARK_ON_K8S_EXECUTOR_MEMORY_OVERHEAD,\n            )\n        if executor_instances is NOTSET:\n            executor_instances = ExecutorInstances(\n                min=Configuration.SPARK_ON_K8S_EXECUTOR_MIN_INSTANCES,\n                max=Configuration.SPARK_ON_K8S_EXECUTOR_MAX_INSTANCES,\n                initial=Configuration.SPARK_ON_K8S_EXECUTOR_INITIAL_INSTANCES,\n            )\n            if (\n                executor_instances.min is None\n                and executor_instances.max is None\n                and executor_instances.initial is None\n            ):\n                executor_instances.initial = 2\n        app_name, app_id = self._parse_app_name_and_id(\n            app_name=app_name, app_id_suffix=app_id_suffix, should_print=should_print\n        )\n        if secret_values is not NOTSET and secret_values:\n            env_from_secrets = [app_id]\n        else:\n            secret_values = Configuration.SPARK_ON_K8S_SECRET_ENV_VAR\n            env_from_secrets = [app_id] if secret_values else []\n        if driver_env_vars_from_secrets is NOTSET:\n            driver_env_vars_from_secrets = Configuration.SPARK_ON_K8S_DRIVER_ENV_VARS_FROM_SECRET\n        if driver_env_vars_from_secrets:\n            env_from_secrets.extend(driver_env_vars_from_secrets)\n        if volumes is NOTSET or volumes is None:\n            volumes = []\n        if driver_volume_mounts is NOTSET or driver_volume_mounts is None:\n            driver_volume_mounts = []\n        if executor_volume_mounts is NOTSET or executor_volume_mounts is None:\n            executor_volume_mounts = []\n        if driver_node_selector is NOTSET or driver_node_selector is None:\n            driver_node_selector = {}\n        if executor_node_selector is NOTSET or executor_node_selector is None:\n            executor_node_selector = {}\n        if driver_annotations is NOTSET or driver_annotations is None:\n            driver_annotations = {}\n        if executor_annotations is NOTSET or executor_annotations is None:\n            executor_annotations = {}\n        if driver_labels is NOTSET or driver_labels is None:\n            driver_labels = {}\n        if executor_labels is NOTSET or executor_labels is None:\n            executor_labels = {}\n        if driver_tolerations is NOTSET or driver_tolerations is None:\n            driver_tolerations = []\n        if executor_pod_template_path is NOTSET or executor_pod_template_path is None:\n            executor_pod_template_path = Configuration.SPARK_ON_K8S_EXECUTOR_POD_TEMPLATE_PATH\n\n        spark_conf = spark_conf or {}\n        main_class_parameters = app_arguments or []\n\n        driver_resources = driver_resources or PodResources()\n        executor_resources = executor_resources or PodResources()\n        executor_instances = executor_instances or ExecutorInstances(initial=2)\n\n        basic_conf = {\n            \"spark.app.name\": app_name,\n            \"spark.app.id\": app_id,\n            \"spark.kubernetes.namespace\": namespace,\n            \"spark.kubernetes.authenticate.driver.serviceAccountName\": service_account,\n            \"spark.kubernetes.container.image\": image,\n            \"spark.driver.host\": app_id,\n            \"spark.driver.port\": \"7077\",\n            \"spark.kubernetes.driver.pod.name\": f\"{app_id}-driver\",\n            \"spark.kubernetes.executor.podNamePrefix\": app_id,\n            \"spark.kubernetes.container.image.pullPolicy\": image_pull_policy,\n            \"spark.driver.memory\": f\"{driver_resources.memory}m\",\n            \"spark.executor.cores\": f\"{executor_resources.cpu}\",\n            \"spark.executor.memory\": f\"{executor_resources.memory}m\",\n            \"spark.executor.memoryOverhead\": f\"{executor_resources.memory_overhead}m\",\n            **self._executor_secrets_config(secret_values=secret_values, app_id=app_id),\n        }\n        extra_labels = {}\n        if ui_reverse_proxy:\n            basic_conf[\"spark.ui.proxyBase\"] = f\"/webserver/ui/{namespace}/{app_id}\"\n            basic_conf[\"spark.ui.proxyRedirectUri\"] = \"/\"\n            extra_labels[\"spark-ui-proxy\"] = \"true\"\n        if executor_instances.min is not None or executor_instances.max is not None:\n            basic_conf[\"spark.dynamicAllocation.enabled\"] = \"true\"\n            basic_conf[\"spark.dynamicAllocation.shuffleTracking.enabled\"] = \"true\"\n            basic_conf[\"spark.dynamicAllocation.minExecutors\"] = f\"{executor_instances.min or 0}\"\n            if executor_instances.max is not None:\n                basic_conf[\"spark.dynamicAllocation.maxExecutors\"] = f\"{executor_instances.max}\"\n            basic_conf[\"spark.dynamicAllocation.initialExecutors\"] = f\"{executor_instances.initial or 0}\"\n        else:\n            basic_conf[\n                \"spark.executor.instances\"\n            ] = f\"{executor_instances.initial if executor_instances.initial is not None else 2}\"\n        if executor_volume_mounts:\n            basic_conf.update(\n                self._executor_volumes_config(volumes=volumes, volume_mounts=executor_volume_mounts)\n            )\n        if executor_node_selector:\n            basic_conf.update(self._executor_node_selector(node_selector=executor_node_selector))\n        if executor_labels:\n            basic_conf.update(self._executor_labels(labels=executor_labels))\n        if executor_annotations:\n            basic_conf.update(self._executor_annotations(annotations=executor_annotations))\n        if executor_pod_template_path:\n            basic_conf.update(self._executor_pod_template_path(executor_pod_template_path))\n        driver_command_args = [\"driver\", \"--master\", \"k8s://https://kubernetes.default.svc.cluster.local:443\"]\n        if class_name:\n            driver_command_args.extend([\"--class\", class_name])\n        driver_command_args.extend(\n            self._spark_config_to_arguments({**basic_conf, **spark_conf}) + [app_path, *main_class_parameters]\n        )\n        pod = SparkAppManager.create_spark_pod_spec(\n            app_name=app_name,\n            app_id=app_id,\n            image=image,\n            image_pull_policy=image_pull_policy,\n            namespace=namespace,\n            args=driver_command_args,\n            extra_labels={**extra_labels, **driver_labels},\n            annotations=driver_annotations,\n            pod_resources={\n                \"requests\": {\n                    \"cpu\": f\"{driver_resources.cpu}\",\n                    \"memory\": f\"{driver_resources.memory + driver_resources.memory_overhead}Mi\",\n                },\n                \"limits\": {\n                    \"cpu\": f\"{driver_resources.cpu}\",\n                    \"memory\": f\"{driver_resources.memory + driver_resources.memory_overhead}Mi\",\n                },\n            },\n            env_from_secrets=env_from_secrets,\n            volumes=volumes,\n            volume_mounts=driver_volume_mounts,\n            node_selector=driver_node_selector,\n            tolerations=driver_tolerations,\n        )\n        with self.k8s_client_manager.client() as client:\n            api = k8s.CoreV1Api(client)\n            if secret_values:\n                application_secret = self.app_manager.create_secret_object(\n                    app_name=app_name,\n                    app_id=app_id,\n                    secrets_values=secret_values,\n                    namespace=namespace,\n                )\n                api.create_namespaced_secret(namespace=namespace, body=application_secret)\n            pod = api.create_namespaced_pod(\n                namespace=namespace,\n                body=pod,\n            )\n            if secret_values:\n                application_secret.metadata.owner_references = [\n                    k8s.V1OwnerReference(\n                        api_version=\"v1\",\n                        kind=\"Pod\",\n                        name=pod.metadata.name,\n                        uid=pod.metadata.uid,\n                    )\n                ]\n                api.patch_namespaced_secret(\n                    namespace=namespace,\n                    name=application_secret.metadata.name,\n                    body=application_secret,\n                )\n            api.create_namespaced_service(\n                namespace=namespace,\n                body=SparkAppManager.create_headless_service_object(\n                    app_name=app_name,\n                    app_id=app_id,\n                    namespace=namespace,\n                    pod_owner_uid=pod.metadata.uid,\n                    extra_labels=extra_labels,\n                ),\n            )\n        if app_waiter == SparkAppWait.LOG:\n            self.app_manager.stream_logs(\n                namespace=namespace,\n                pod_name=pod.metadata.name,\n                should_print=should_print,\n            )\n        elif app_waiter == SparkAppWait.WAIT:\n            self.app_manager.wait_for_app(\n                namespace=namespace, pod_name=pod.metadata.name, should_print=should_print\n            )\n        return pod.metadata.name\n\n    def _parse_app_name_and_id(\n        self,\n        *,\n        app_name: str | None = None,\n        app_id_suffix: Callable[[], str] = default_app_id_suffix,\n        should_print: bool = False,\n    ) -&gt; tuple[str, str]:\n        \"\"\"Parse the application name and ID\n\n        This function will generate a valid application name and ID from the provided application name.\n            It will ensure that the application name and ID respect the Kubernetes naming conventions\n            (e.g. no uppercase characters, no\n        special characters, start with a letter, etc.), and they are not too long\n            (less than 64 characters for service\n        names and labels values).\n\n        Args:\n            app_name: Name of the Spark application\n            app_id_suffix: Function to generate a suffix for the application ID,\n                defaults to `default_app_id_suffix`\n            should_print: Whether to print logs instead of logging them, defaults to False\n\n        Returns:\n            Tuple of the application name and ID\n        \"\"\"\n        if not app_name:\n            app_name = f\"spark-app{app_id_suffix()}\"\n            app_id = app_name\n        else:\n            original_app_name = app_name\n            # All to lowercase\n            app_name = app_name.lower()\n            app_id_suffix_str = app_id_suffix()\n            if len(app_name) &gt; (63 - len(app_id_suffix_str) + 1):\n                app_name = app_name[: (63 - len(app_id_suffix_str)) + 1]\n            # Replace all non-alphanumeric characters with dashes\n            app_name = re.sub(r\"[^0-9a-zA-Z]+\", \"-\", app_name)\n            # Remove leading non-alphabetic characters\n            app_name = re.sub(r\"^[^a-zA-Z]*\", \"\", app_name)\n            # Remove leading and trailing dashes\n            app_name = re.sub(r\"^-*\", \"\", app_name)\n            app_name = re.sub(r\"-*$\", \"\", app_name)\n            app_id = app_name + app_id_suffix_str\n            if app_name != original_app_name:\n                self.log(\n                    msg=(\n                        f\"Application name {original_app_name} is too long\"\n                        f\" and will be truncated to {app_name}\"\n                    ),\n                    level=logging.WARNING,\n                    should_print=should_print,\n                )\n        return app_name, app_id\n\n    @staticmethod\n    def _value_to_str(value: Any) -&gt; str:\n        if isinstance(value, bool):\n            return str(value).lower()\n        return str(value)\n\n    @staticmethod\n    def _spark_config_to_arguments(spark_conf: dict[str, str] | None) -&gt; list[str]:\n        \"\"\"Convert Spark configuration to a list of arguments\n\n        Args:\n            spark_conf: Spark configuration dictionary\n\n        Returns:\n            List of arguments\n        \"\"\"\n        if not spark_conf:\n            return []\n        args = []\n        for key, value in spark_conf.items():\n            args.extend([\"--conf\", f\"{key}={SparkOnK8S._value_to_str(value)}\"])\n        return args\n\n    @staticmethod\n    def _executor_secrets_config(\n        secret_values: dict[str, str] | None,\n        app_id: str,\n    ) -&gt; dict[str, str]:\n        \"\"\"Spark configuration to load environment variables from the secret\n\n        Args:\n            secret_values: Secret values to pass to the application as environment variables\n            app_id: Application ID\n\n        Returns:\n            Spark configuration dictionary\n        \"\"\"\n        if not secret_values:\n            return {}\n        return {\n            f\"spark.kubernetes.executor.secretKeyRef.{secret_name}\": f\"{app_id}:{secret_name}\"\n            for secret_name in secret_values.keys()\n        }\n\n    @staticmethod\n    def _executor_volumes_config(\n        volumes: list[k8s.V1Volume] | None,\n        volume_mounts: list[k8s.V1VolumeMount] | None,\n    ) -&gt; dict[str, str]:\n        \"\"\"Spark configuration to mount volumes to the executors\n\n        Args:\n            volumes: List of volumes to mount to the driver and/or executors\n            volume_mounts: List of volume mounts to mount to the executors\n\n        Returns:\n            Spark configuration dictionary\n        \"\"\"\n        if not volumes:\n            return {}\n        config = {}\n        # https://spark.apache.org/docs/latest/running-on-kubernetes.html#using-kubernetes-volumes\n        supported_volume_types = {\n            \"hostPath\",\n            \"emptyDir\",\n            \"nfs\",\n            \"persistentVolumeClaim\",\n        }\n        loaded_volumes = {}\n        volumes_config = {}\n        for volume in volumes:\n            volume_name = volume.name\n            volume_mapped_type: str | None = None\n            volume_type_str: str | None = None\n            for attr in k8s.V1Volume.attribute_map:\n                if attr != \"name\" and getattr(volume, attr) is not None:\n                    volume_mapped_type = k8s.V1Volume.attribute_map[attr]\n                    volume_type_str = k8s.V1Volume.openapi_types[attr]\n                    volume = getattr(volume, attr)\n                    break\n            volume_type = getattr(k8s, volume_type_str)\n            if volume_mapped_type not in supported_volume_types:\n                continue\n            loaded_volumes[volume_name] = volume_mapped_type\n            volumes_config[volume_name] = {}\n            for attr in volume_type.attribute_map:\n                if getattr(volume, attr) is not None:\n                    option_name = volume_type.attribute_map[attr]\n                    volumes_config[volume_name][\n                        f\"spark.kubernetes.executor.volumes.{volume_mapped_type}.{volume_name}.{option_name}\"\n                    ] = getattr(volume, attr)\n        for volume_mount in volume_mounts:\n            if volume_mount.name not in loaded_volumes:\n                raise ValueError(\n                    f\"Volume {volume_mount.name} is not found in the volumes list or is not supported.\\n\"\n                    \"Please make sure to add the volume to the volumes list and use one of\"\n                    f\" the supported types: {supported_volume_types}\"\n                )\n            config.update(volumes_config[volume_mount.name])\n            volume_config_prefix = (\n                \"spark.kubernetes.executor.volumes.\"\n                f\"{loaded_volumes[volume_mount.name]}.{volume_mount.name}.mount\"\n            )\n            config[f\"{volume_config_prefix}.path\"] = volume_mount.mount_path\n            if volume_mount.sub_path:\n                config[f\"{volume_config_prefix}.subPath\"] = volume_mount.sub_path\n            if volume_mount.read_only:\n                config[f\"{volume_config_prefix}.readOnly\"] = True\n        return config\n\n    @staticmethod\n    def _executor_node_selector(\n        node_selector: dict[str, str] | None,\n    ) -&gt; dict[str, str]:\n        \"\"\"Spark configuration to set node selector for the executors\n\n        Args:\n            node_selector: Node selector for the executors\n\n        Returns:\n            Spark configuration dictionary\n        \"\"\"\n        if not node_selector:\n            return {}\n        return {\n            f\"spark.kubernetes.executor.node.selector.{key}\": value for key, value in node_selector.items()\n        }\n\n    @staticmethod\n    def _executor_labels(\n        labels: dict[str, str] | None,\n    ) -&gt; dict[str, str]:\n        \"\"\"Spark configuration to set labels for the executors\n\n        Args:\n            labels: Labels for the executors\n\n        Returns:\n            Spark configuration dictionary\n        \"\"\"\n        if not labels:\n            return {}\n        return {f\"spark.kubernetes.executor.label.{key}\": value for key, value in labels.items()}\n\n    @staticmethod\n    def _executor_annotations(\n        annotations: dict[str, str] | None,\n    ) -&gt; dict[str, str]:\n        \"\"\"Spark configuration to set annotations for the executors\n\n        Args:\n            annotations: Annotations for the executors\n\n        Returns:\n            Spark configuration dictionary\n        \"\"\"\n        if not annotations:\n            return {}\n        return {f\"spark.kubernetes.executor.annotation.{key}\": value for key, value in annotations.items()}\n\n    @staticmethod\n    def _executor_pod_template_path(\n        executor_pod_template_path: str | None,\n    ) -&gt; dict[str, str]:\n        \"\"\"Spark configuration to set the executor pod template file\n\n        Args:\n            executor_pod_template_path: Path to the executor pod template file\n\n        Returns:\n            Spark configuration dictionary\n        \"\"\"\n        if not executor_pod_template_path:\n            return {}\n        return {\"spark.kubernetes.executor.podTemplateFile\": executor_pod_template_path}\n</code></pre>"},{"location":"reference/spark_on_k8s/client/#spark_on_k8s.client.SparkOnK8S.submit_app","title":"<code>submit_app(*, image=NOTSET, app_path=NOTSET, namespace=NOTSET, service_account=NOTSET, app_name=NOTSET, spark_conf=NOTSET, class_name=NOTSET, app_arguments=NOTSET, app_id_suffix=NOTSET, app_waiter=NOTSET, image_pull_policy=NOTSET, ui_reverse_proxy=NOTSET, driver_resources=NOTSET, executor_resources=NOTSET, executor_instances=NOTSET, should_print=NOTSET, secret_values=NOTSET, driver_env_vars_from_secrets=NOTSET, volumes=NOTSET, driver_volume_mounts=NOTSET, executor_volume_mounts=NOTSET, driver_node_selector=NOTSET, executor_node_selector=NOTSET, driver_annotations=NOTSET, executor_annotations=NOTSET, driver_labels=NOTSET, executor_labels=NOTSET, driver_tolerations=NOTSET, executor_pod_template_path=NOTSET)</code>","text":"<p>Submit a Spark app to Kubernetes</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>str | ArgNotSet</code> <p>Docker image to use for the Spark driver and executors</p> <code>NOTSET</code> <code>app_path</code> <code>str | ArgNotSet</code> <p>Path to the application JAR / Python / R file</p> <code>NOTSET</code> <code>namespace</code> <code>str | ArgNotSet</code> <p>Kubernetes namespace to use, defaults to \"default\"</p> <code>NOTSET</code> <code>service_account</code> <code>str | ArgNotSet</code> <p>Kubernetes service account to use for the Spark driver, defaults to \"spark\"</p> <code>NOTSET</code> <code>app_name</code> <code>str | ArgNotSet</code> <p>Name of the Spark application, defaults to a generated name as <code>spark-app{app_id_suffix()}</code></p> <code>NOTSET</code> <code>spark_conf</code> <code>dict[str, str] | ArgNotSet</code> <p>Dictionary of spark configuration to pass to the application</p> <code>NOTSET</code> <code>class_name</code> <code>str | ArgNotSet</code> <p>Name of the class to execute</p> <code>NOTSET</code> <code>app_arguments</code> <code>list[str] | ArgNotSet</code> <p>List of arguments to pass to the application</p> <code>NOTSET</code> <code>app_id_suffix</code> <code>Callable[[], str] | ArgNotSet</code> <p>Function to generate a suffix for the application ID, defaults to <code>default_app_id_suffix</code></p> <code>NOTSET</code> <code>app_waiter</code> <code>Literal['no_wait', 'wait', 'log'] | ArgNotSet</code> <p>How to wait for the app to finish. One of \"no_wait\", \"wait\", or \"log\"</p> <code>NOTSET</code> <code>image_pull_policy</code> <code>Literal['Always', 'Never', 'IfNotPresent'] | ArgNotSet</code> <p>Image pull policy for the driver and executors, defaults to \"IfNotPresent\"</p> <code>NOTSET</code> <code>ui_reverse_proxy</code> <code>bool | ArgNotSet</code> <p>Whether to use a reverse proxy for the Spark UI, defaults to False</p> <code>NOTSET</code> <code>driver_resources</code> <code>PodResources | ArgNotSet</code> <p>Resources to request for the Spark driver. Defaults to 1 CPU core, 1Gi of memory and512Mi of memory overhead</p> <code>NOTSET</code> <code>executor_resources</code> <code>PodResources | ArgNotSet</code> <p>Resources to request for the Spark executors. Defaults to 1 CPU core, 1Gi of memory and 512Mi of memory overhead</p> <code>NOTSET</code> <code>executor_instances</code> <code>ExecutorInstances | ArgNotSet</code> <p>Number of executors to request. If max and min are not provided, dynamic allocation will be disabled and the number of executors will be fixed to initial or 2 if initial is not provided. If max or min or both are provided, dynamic allocation will be enabled and the number of executors will be between min and max (inclusive), and initial will be the initial number of executors with a default of 0.</p> <code>NOTSET</code> <code>should_print</code> <code>bool | ArgNotSet</code> <p>Whether to print logs instead of logging them, defaults to False</p> <code>NOTSET</code> <code>secret_values</code> <code>dict[str, str] | ArgNotSet</code> <p>Dictionary of secret values to pass to the application as environment variables</p> <code>NOTSET</code> <code>driver_env_vars_from_secrets</code> <code>list[str] | ArgNotSet</code> <p>List of secret names to load environment variables from for the driver</p> <code>NOTSET</code> <code>volumes</code> <code>list[V1Volume] | ArgNotSet</code> <p>List of volumes to mount to the driver and/or executors</p> <code>NOTSET</code> <code>driver_volume_mounts</code> <code>list[V1VolumeMount] | ArgNotSet</code> <p>List of volume mounts to mount to the driver</p> <code>NOTSET</code> <code>executor_volume_mounts</code> <code>list[V1VolumeMount] | ArgNotSet</code> <p>List of volume mounts to mount to the executors</p> <code>NOTSET</code> <code>driver_node_selector</code> <code>dict[str, str] | ArgNotSet</code> <p>Node selector for the driver</p> <code>NOTSET</code> <code>executor_node_selector</code> <code>dict[str, str] | ArgNotSet</code> <p>Node selector for the executors</p> <code>NOTSET</code> <code>driver_tolerations</code> <code>list[V1Toleration] | ArgNotSet</code> <p>List of tolerations for the driver</p> <code>NOTSET</code> <code>executor_pod_template_path</code> <code>str | ArgNotSet</code> <p>Path to the executor pod template file</p> <code>NOTSET</code> <p>Returns:</p> Type Description <code>str</code> <p>Name of the Spark application pod</p> Source code in <code>spark_on_k8s/client.py</code> <pre><code>def submit_app(\n    self,\n    *,\n    image: str | ArgNotSet = NOTSET,\n    app_path: str | ArgNotSet = NOTSET,\n    namespace: str | ArgNotSet = NOTSET,\n    service_account: str | ArgNotSet = NOTSET,\n    app_name: str | ArgNotSet = NOTSET,\n    spark_conf: dict[str, str] | ArgNotSet = NOTSET,\n    class_name: str | ArgNotSet = NOTSET,\n    app_arguments: list[str] | ArgNotSet = NOTSET,\n    app_id_suffix: Callable[[], str] | ArgNotSet = NOTSET,\n    app_waiter: Literal[\"no_wait\", \"wait\", \"log\"] | ArgNotSet = NOTSET,\n    image_pull_policy: Literal[\"Always\", \"Never\", \"IfNotPresent\"] | ArgNotSet = NOTSET,\n    ui_reverse_proxy: bool | ArgNotSet = NOTSET,\n    driver_resources: PodResources | ArgNotSet = NOTSET,\n    executor_resources: PodResources | ArgNotSet = NOTSET,\n    executor_instances: ExecutorInstances | ArgNotSet = NOTSET,\n    should_print: bool | ArgNotSet = NOTSET,\n    secret_values: dict[str, str] | ArgNotSet = NOTSET,\n    driver_env_vars_from_secrets: list[str] | ArgNotSet = NOTSET,\n    volumes: list[k8s.V1Volume] | ArgNotSet = NOTSET,\n    driver_volume_mounts: list[k8s.V1VolumeMount] | ArgNotSet = NOTSET,\n    executor_volume_mounts: list[k8s.V1VolumeMount] | ArgNotSet = NOTSET,\n    driver_node_selector: dict[str, str] | ArgNotSet = NOTSET,\n    executor_node_selector: dict[str, str] | ArgNotSet = NOTSET,\n    driver_annotations: dict[str, str] | ArgNotSet = NOTSET,\n    executor_annotations: dict[str, str] | ArgNotSet = NOTSET,\n    driver_labels: dict[str, str] | ArgNotSet = NOTSET,\n    executor_labels: dict[str, str] | ArgNotSet = NOTSET,\n    driver_tolerations: list[k8s.V1Toleration] | ArgNotSet = NOTSET,\n    executor_pod_template_path: str | ArgNotSet = NOTSET,\n) -&gt; str:\n    \"\"\"Submit a Spark app to Kubernetes\n\n    Args:\n        image: Docker image to use for the Spark driver and executors\n        app_path: Path to the application JAR / Python / R file\n        namespace: Kubernetes namespace to use, defaults to \"default\"\n        service_account: Kubernetes service account to use for the Spark driver,\n            defaults to \"spark\"\n        app_name: Name of the Spark application, defaults to a generated name as\n            `spark-app{app_id_suffix()}`\n        spark_conf: Dictionary of spark configuration to pass to the application\n        class_name: Name of the class to execute\n        app_arguments: List of arguments to pass to the application\n        app_id_suffix: Function to generate a suffix for the application ID, defaults to\n            `default_app_id_suffix`\n        app_waiter: How to wait for the app to finish. One of \"no_wait\", \"wait\", or \"log\"\n        image_pull_policy: Image pull policy for the driver and executors, defaults to \"IfNotPresent\"\n        ui_reverse_proxy: Whether to use a reverse proxy for the Spark UI, defaults to False\n        driver_resources: Resources to request for the Spark driver. Defaults to 1 CPU core, 1Gi of\n            memory and512Mi of memory overhead\n        executor_resources: Resources to request for the Spark executors. Defaults to 1 CPU core, 1Gi\n            of memory and 512Mi of memory overhead\n        executor_instances: Number of executors to request. If max and min are not provided, dynamic\n            allocation will be disabled and the number of executors will be fixed to initial or 2 if\n            initial is not provided. If max or min or both are provided, dynamic allocation will be\n            enabled and the number of executors will be between min and max (inclusive), and initial\n            will be the initial number of executors with a default of 0.\n        should_print: Whether to print logs instead of logging them, defaults to False\n        secret_values: Dictionary of secret values to pass to the application as environment variables\n        driver_env_vars_from_secrets: List of secret names to load environment variables from for\n            the driver\n        volumes: List of volumes to mount to the driver and/or executors\n        driver_volume_mounts: List of volume mounts to mount to the driver\n        executor_volume_mounts: List of volume mounts to mount to the executors\n        driver_node_selector: Node selector for the driver\n        executor_node_selector: Node selector for the executors\n        driver_tolerations: List of tolerations for the driver\n        executor_pod_template_path: Path to the executor pod template file\n\n    Returns:\n        Name of the Spark application pod\n    \"\"\"\n    if image is NOTSET:\n        if Configuration.SPARK_ON_K8S_DOCKER_IMAGE is None:\n            raise ValueError(\n                \"Docker image is not set.\"\n                \"Please set the image argument or the environment variable SPARK_ON_K8S_DOCKER_IMAGE\"\n            )\n        image = Configuration.SPARK_ON_K8S_DOCKER_IMAGE\n    if app_path is NOTSET:\n        if Configuration.SPARK_ON_K8S_APP_PATH is None:\n            raise ValueError(\n                \"Application path is not set.\"\n                \"Please set the app_path argument or the environment variable SPARK_ON_K8S_APP_PATH\"\n            )\n        app_path = Configuration.SPARK_ON_K8S_APP_PATH\n    if namespace is NOTSET:\n        namespace = Configuration.SPARK_ON_K8S_NAMESPACE\n    if service_account is NOTSET:\n        service_account = Configuration.SPARK_ON_K8S_SERVICE_ACCOUNT\n    if app_name is NOTSET:\n        app_name = Configuration.SPARK_ON_K8S_APP_NAME\n    if spark_conf is NOTSET:\n        spark_conf = Configuration.SPARK_ON_K8S_SPARK_CONF\n    if class_name is NOTSET:\n        class_name = Configuration.SPARK_ON_K8S_CLASS_NAME\n    if app_arguments is NOTSET:\n        app_arguments = Configuration.SPARK_ON_K8S_APP_ARGUMENTS\n    if app_id_suffix is NOTSET:\n        app_id_suffix = default_app_id_suffix\n    if app_waiter is NOTSET:\n        app_waiter = Configuration.SPARK_ON_K8S_APP_WAITER\n    if image_pull_policy is NOTSET:\n        image_pull_policy = Configuration.SPARK_ON_K8S_IMAGE_PULL_POLICY\n    if ui_reverse_proxy is NOTSET:\n        ui_reverse_proxy = Configuration.SPARK_ON_K8S_UI_REVERSE_PROXY\n    if driver_resources is NOTSET:\n        driver_resources = PodResources(\n            cpu=Configuration.SPARK_ON_K8S_DRIVER_CPU,\n            memory=Configuration.SPARK_ON_K8S_DRIVER_MEMORY,\n            memory_overhead=Configuration.SPARK_ON_K8S_DRIVER_MEMORY_OVERHEAD,\n        )\n    if executor_resources is NOTSET:\n        executor_resources = PodResources(\n            cpu=Configuration.SPARK_ON_K8S_EXECUTOR_CPU,\n            memory=Configuration.SPARK_ON_K8S_EXECUTOR_MEMORY,\n            memory_overhead=Configuration.SPARK_ON_K8S_EXECUTOR_MEMORY_OVERHEAD,\n        )\n    if executor_instances is NOTSET:\n        executor_instances = ExecutorInstances(\n            min=Configuration.SPARK_ON_K8S_EXECUTOR_MIN_INSTANCES,\n            max=Configuration.SPARK_ON_K8S_EXECUTOR_MAX_INSTANCES,\n            initial=Configuration.SPARK_ON_K8S_EXECUTOR_INITIAL_INSTANCES,\n        )\n        if (\n            executor_instances.min is None\n            and executor_instances.max is None\n            and executor_instances.initial is None\n        ):\n            executor_instances.initial = 2\n    app_name, app_id = self._parse_app_name_and_id(\n        app_name=app_name, app_id_suffix=app_id_suffix, should_print=should_print\n    )\n    if secret_values is not NOTSET and secret_values:\n        env_from_secrets = [app_id]\n    else:\n        secret_values = Configuration.SPARK_ON_K8S_SECRET_ENV_VAR\n        env_from_secrets = [app_id] if secret_values else []\n    if driver_env_vars_from_secrets is NOTSET:\n        driver_env_vars_from_secrets = Configuration.SPARK_ON_K8S_DRIVER_ENV_VARS_FROM_SECRET\n    if driver_env_vars_from_secrets:\n        env_from_secrets.extend(driver_env_vars_from_secrets)\n    if volumes is NOTSET or volumes is None:\n        volumes = []\n    if driver_volume_mounts is NOTSET or driver_volume_mounts is None:\n        driver_volume_mounts = []\n    if executor_volume_mounts is NOTSET or executor_volume_mounts is None:\n        executor_volume_mounts = []\n    if driver_node_selector is NOTSET or driver_node_selector is None:\n        driver_node_selector = {}\n    if executor_node_selector is NOTSET or executor_node_selector is None:\n        executor_node_selector = {}\n    if driver_annotations is NOTSET or driver_annotations is None:\n        driver_annotations = {}\n    if executor_annotations is NOTSET or executor_annotations is None:\n        executor_annotations = {}\n    if driver_labels is NOTSET or driver_labels is None:\n        driver_labels = {}\n    if executor_labels is NOTSET or executor_labels is None:\n        executor_labels = {}\n    if driver_tolerations is NOTSET or driver_tolerations is None:\n        driver_tolerations = []\n    if executor_pod_template_path is NOTSET or executor_pod_template_path is None:\n        executor_pod_template_path = Configuration.SPARK_ON_K8S_EXECUTOR_POD_TEMPLATE_PATH\n\n    spark_conf = spark_conf or {}\n    main_class_parameters = app_arguments or []\n\n    driver_resources = driver_resources or PodResources()\n    executor_resources = executor_resources or PodResources()\n    executor_instances = executor_instances or ExecutorInstances(initial=2)\n\n    basic_conf = {\n        \"spark.app.name\": app_name,\n        \"spark.app.id\": app_id,\n        \"spark.kubernetes.namespace\": namespace,\n        \"spark.kubernetes.authenticate.driver.serviceAccountName\": service_account,\n        \"spark.kubernetes.container.image\": image,\n        \"spark.driver.host\": app_id,\n        \"spark.driver.port\": \"7077\",\n        \"spark.kubernetes.driver.pod.name\": f\"{app_id}-driver\",\n        \"spark.kubernetes.executor.podNamePrefix\": app_id,\n        \"spark.kubernetes.container.image.pullPolicy\": image_pull_policy,\n        \"spark.driver.memory\": f\"{driver_resources.memory}m\",\n        \"spark.executor.cores\": f\"{executor_resources.cpu}\",\n        \"spark.executor.memory\": f\"{executor_resources.memory}m\",\n        \"spark.executor.memoryOverhead\": f\"{executor_resources.memory_overhead}m\",\n        **self._executor_secrets_config(secret_values=secret_values, app_id=app_id),\n    }\n    extra_labels = {}\n    if ui_reverse_proxy:\n        basic_conf[\"spark.ui.proxyBase\"] = f\"/webserver/ui/{namespace}/{app_id}\"\n        basic_conf[\"spark.ui.proxyRedirectUri\"] = \"/\"\n        extra_labels[\"spark-ui-proxy\"] = \"true\"\n    if executor_instances.min is not None or executor_instances.max is not None:\n        basic_conf[\"spark.dynamicAllocation.enabled\"] = \"true\"\n        basic_conf[\"spark.dynamicAllocation.shuffleTracking.enabled\"] = \"true\"\n        basic_conf[\"spark.dynamicAllocation.minExecutors\"] = f\"{executor_instances.min or 0}\"\n        if executor_instances.max is not None:\n            basic_conf[\"spark.dynamicAllocation.maxExecutors\"] = f\"{executor_instances.max}\"\n        basic_conf[\"spark.dynamicAllocation.initialExecutors\"] = f\"{executor_instances.initial or 0}\"\n    else:\n        basic_conf[\n            \"spark.executor.instances\"\n        ] = f\"{executor_instances.initial if executor_instances.initial is not None else 2}\"\n    if executor_volume_mounts:\n        basic_conf.update(\n            self._executor_volumes_config(volumes=volumes, volume_mounts=executor_volume_mounts)\n        )\n    if executor_node_selector:\n        basic_conf.update(self._executor_node_selector(node_selector=executor_node_selector))\n    if executor_labels:\n        basic_conf.update(self._executor_labels(labels=executor_labels))\n    if executor_annotations:\n        basic_conf.update(self._executor_annotations(annotations=executor_annotations))\n    if executor_pod_template_path:\n        basic_conf.update(self._executor_pod_template_path(executor_pod_template_path))\n    driver_command_args = [\"driver\", \"--master\", \"k8s://https://kubernetes.default.svc.cluster.local:443\"]\n    if class_name:\n        driver_command_args.extend([\"--class\", class_name])\n    driver_command_args.extend(\n        self._spark_config_to_arguments({**basic_conf, **spark_conf}) + [app_path, *main_class_parameters]\n    )\n    pod = SparkAppManager.create_spark_pod_spec(\n        app_name=app_name,\n        app_id=app_id,\n        image=image,\n        image_pull_policy=image_pull_policy,\n        namespace=namespace,\n        args=driver_command_args,\n        extra_labels={**extra_labels, **driver_labels},\n        annotations=driver_annotations,\n        pod_resources={\n            \"requests\": {\n                \"cpu\": f\"{driver_resources.cpu}\",\n                \"memory\": f\"{driver_resources.memory + driver_resources.memory_overhead}Mi\",\n            },\n            \"limits\": {\n                \"cpu\": f\"{driver_resources.cpu}\",\n                \"memory\": f\"{driver_resources.memory + driver_resources.memory_overhead}Mi\",\n            },\n        },\n        env_from_secrets=env_from_secrets,\n        volumes=volumes,\n        volume_mounts=driver_volume_mounts,\n        node_selector=driver_node_selector,\n        tolerations=driver_tolerations,\n    )\n    with self.k8s_client_manager.client() as client:\n        api = k8s.CoreV1Api(client)\n        if secret_values:\n            application_secret = self.app_manager.create_secret_object(\n                app_name=app_name,\n                app_id=app_id,\n                secrets_values=secret_values,\n                namespace=namespace,\n            )\n            api.create_namespaced_secret(namespace=namespace, body=application_secret)\n        pod = api.create_namespaced_pod(\n            namespace=namespace,\n            body=pod,\n        )\n        if secret_values:\n            application_secret.metadata.owner_references = [\n                k8s.V1OwnerReference(\n                    api_version=\"v1\",\n                    kind=\"Pod\",\n                    name=pod.metadata.name,\n                    uid=pod.metadata.uid,\n                )\n            ]\n            api.patch_namespaced_secret(\n                namespace=namespace,\n                name=application_secret.metadata.name,\n                body=application_secret,\n            )\n        api.create_namespaced_service(\n            namespace=namespace,\n            body=SparkAppManager.create_headless_service_object(\n                app_name=app_name,\n                app_id=app_id,\n                namespace=namespace,\n                pod_owner_uid=pod.metadata.uid,\n                extra_labels=extra_labels,\n            ),\n        )\n    if app_waiter == SparkAppWait.LOG:\n        self.app_manager.stream_logs(\n            namespace=namespace,\n            pod_name=pod.metadata.name,\n            should_print=should_print,\n        )\n    elif app_waiter == SparkAppWait.WAIT:\n        self.app_manager.wait_for_app(\n            namespace=namespace, pod_name=pod.metadata.name, should_print=should_print\n        )\n    return pod.metadata.name\n</code></pre>"},{"location":"reference/spark_on_k8s/client/#spark_on_k8s.client.default_app_id_suffix","title":"<code>default_app_id_suffix()</code>","text":"<p>Default function to generate a suffix for the application ID</p> <p>Returns:</p> Type Description <code>str</code> <p>the current timestamp in the format %Y%m%d%H%M%S prefixed with a dash (e.g. -20240101123456)</p> Source code in <code>spark_on_k8s/client.py</code> <pre><code>def default_app_id_suffix() -&gt; str:\n    \"\"\"Default function to generate a suffix for the application ID\n\n    Returns:\n        the current timestamp in the format %Y%m%d%H%M%S prefixed with a dash (e.g. -20240101123456)\n    \"\"\"\n    return f\"-{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n</code></pre>"},{"location":"reference/spark_on_k8s/airflow/operator_links/","title":"operator_links","text":""},{"location":"reference/spark_on_k8s/airflow/operator_links/#spark_on_k8s.airflow.operator_links.SparkOnK8SOperatorLink","title":"<code>SparkOnK8SOperatorLink</code>","text":"<p>               Bases: <code>BaseOperatorLink</code></p> <p>Operator link for SparkOnK8SOperator.</p> <p>It allows users to access Spark job UI and spark history UI using SparkOnK8SOperator.</p> Source code in <code>spark_on_k8s/airflow/operator_links.py</code> <pre><code>class SparkOnK8SOperatorLink(BaseOperatorLink):\n    \"\"\"\n    Operator link for SparkOnK8SOperator.\n\n    It allows users to access Spark job UI and spark history UI using SparkOnK8SOperator.\n    \"\"\"\n\n    name = \"Spark Job UI\"\n\n    @staticmethod\n    def persist_spark_ui_link(\n        context: Context,\n        task_instance: BaseOperator,\n        spark_on_k8s_service_url: str,\n        namespace: str,\n        spark_app_id: str,\n    ):\n        \"\"\"\n        Persist Spark UI link to XCom.\n        \"\"\"\n        from spark_on_k8s.airflow.operators import SparkOnK8SOperator\n\n        spark_ui_link = f\"{spark_on_k8s_service_url}/webserver/ui/{namespace}/{spark_app_id}\"\n        task_instance.xcom_push(\n            context,\n            key=SparkOnK8SOperator.XCOM_SPARK_UI_LINK,\n            value=spark_ui_link,\n        )\n\n    @staticmethod\n    def persist_spark_history_ui_link(\n        context: Context, task_instance: BaseOperator, spark_on_k8s_service_url: str, spark_app_id: str\n    ):\n        \"\"\"\n        Persist Spark history UI link to XCom.\n        \"\"\"\n        from spark_on_k8s.airflow.operators import SparkOnK8SOperator\n\n        spark_history_ui_link = f\"{spark_on_k8s_service_url}/webserver/ui-history/history/{spark_app_id}\"\n        task_instance.xcom_push(\n            context,\n            key=SparkOnK8SOperator.XCOM_SPARK_UI_LINK,\n            value=spark_history_ui_link,\n        )\n\n    def get_link(self, operator: BaseOperator, *, ti_key: TaskInstanceKey) -&gt; str:\n        \"\"\"\n        Get link to Spark job UI or Spark history UI.\n        \"\"\"\n        from spark_on_k8s.airflow.operators import SparkOnK8SOperator\n\n        spark_ui_link = XCom.get_value(ti_key=ti_key, key=SparkOnK8SOperator.XCOM_SPARK_UI_LINK)\n        if spark_ui_link:\n            return spark_ui_link\n        else:\n            return \"\"\n</code></pre>"},{"location":"reference/spark_on_k8s/airflow/operator_links/#spark_on_k8s.airflow.operator_links.SparkOnK8SOperatorLink.get_link","title":"<code>get_link(operator, *, ti_key)</code>","text":"<p>Get link to Spark job UI or Spark history UI.</p> Source code in <code>spark_on_k8s/airflow/operator_links.py</code> <pre><code>def get_link(self, operator: BaseOperator, *, ti_key: TaskInstanceKey) -&gt; str:\n    \"\"\"\n    Get link to Spark job UI or Spark history UI.\n    \"\"\"\n    from spark_on_k8s.airflow.operators import SparkOnK8SOperator\n\n    spark_ui_link = XCom.get_value(ti_key=ti_key, key=SparkOnK8SOperator.XCOM_SPARK_UI_LINK)\n    if spark_ui_link:\n        return spark_ui_link\n    else:\n        return \"\"\n</code></pre>"},{"location":"reference/spark_on_k8s/airflow/operator_links/#spark_on_k8s.airflow.operator_links.SparkOnK8SOperatorLink.persist_spark_history_ui_link","title":"<code>persist_spark_history_ui_link(context, task_instance, spark_on_k8s_service_url, spark_app_id)</code>  <code>staticmethod</code>","text":"<p>Persist Spark history UI link to XCom.</p> Source code in <code>spark_on_k8s/airflow/operator_links.py</code> <pre><code>@staticmethod\ndef persist_spark_history_ui_link(\n    context: Context, task_instance: BaseOperator, spark_on_k8s_service_url: str, spark_app_id: str\n):\n    \"\"\"\n    Persist Spark history UI link to XCom.\n    \"\"\"\n    from spark_on_k8s.airflow.operators import SparkOnK8SOperator\n\n    spark_history_ui_link = f\"{spark_on_k8s_service_url}/webserver/ui-history/history/{spark_app_id}\"\n    task_instance.xcom_push(\n        context,\n        key=SparkOnK8SOperator.XCOM_SPARK_UI_LINK,\n        value=spark_history_ui_link,\n    )\n</code></pre>"},{"location":"reference/spark_on_k8s/airflow/operator_links/#spark_on_k8s.airflow.operator_links.SparkOnK8SOperatorLink.persist_spark_ui_link","title":"<code>persist_spark_ui_link(context, task_instance, spark_on_k8s_service_url, namespace, spark_app_id)</code>  <code>staticmethod</code>","text":"<p>Persist Spark UI link to XCom.</p> Source code in <code>spark_on_k8s/airflow/operator_links.py</code> <pre><code>@staticmethod\ndef persist_spark_ui_link(\n    context: Context,\n    task_instance: BaseOperator,\n    spark_on_k8s_service_url: str,\n    namespace: str,\n    spark_app_id: str,\n):\n    \"\"\"\n    Persist Spark UI link to XCom.\n    \"\"\"\n    from spark_on_k8s.airflow.operators import SparkOnK8SOperator\n\n    spark_ui_link = f\"{spark_on_k8s_service_url}/webserver/ui/{namespace}/{spark_app_id}\"\n    task_instance.xcom_push(\n        context,\n        key=SparkOnK8SOperator.XCOM_SPARK_UI_LINK,\n        value=spark_ui_link,\n    )\n</code></pre>"},{"location":"reference/spark_on_k8s/airflow/operators/","title":"operators","text":""},{"location":"reference/spark_on_k8s/airflow/operators/#spark_on_k8s.airflow.operators.SparkOnK8SOperator","title":"<code>SparkOnK8SOperator</code>","text":"<p>               Bases: <code>BaseOperator</code></p> <p>Submit a Spark application on Kubernetes.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>str</code> <p>Spark application image.</p> required <code>app_path</code> <code>str</code> <p>Path to the Spark application.</p> required <code>namespace</code> <code>str</code> <p>Kubernetes namespace. Defaults to \"default\".</p> <code>'default'</code> <code>service_account</code> <code>str</code> <p>Kubernetes service account. Defaults to \"spark\".</p> <code>'spark'</code> <code>app_name</code> <code>str</code> <p>Spark application name. Defaults to None.</p> <code>None</code> <code>app_id_suffix</code> <code>str</code> <p>A suffix for the application ID, defaults to current timestamp in the format %Y%m%d%H%M%S prefixed with a dash.</p> <code>None</code> <code>spark_conf</code> <code>dict[str, str]</code> <p>Spark configuration. Defaults to None.</p> <code>None</code> <code>class_name</code> <code>str</code> <p>Spark application class name. Defaults to None.</p> <code>None</code> <code>app_arguments</code> <code>list[str]</code> <p>Spark application arguments. Defaults to None.</p> <code>None</code> <code>app_waiter</code> <code>Literal['no_wait', 'wait', 'log']</code> <p>Spark application waiter. Defaults to \"wait\".</p> <code>'wait'</code> <code>image_pull_policy</code> <code>Literal['Always', 'Never', 'IfNotPresent']</code> <p>Image pull policy. Defaults to \"IfNotPresent\".</p> <code>'IfNotPresent'</code> <code>ui_reverse_proxy</code> <code>bool</code> <p>Whether to use a reverse proxy for the Spark UI. Defaults to False.</p> <code>False</code> <code>driver_resources</code> <code>PodResources</code> <p>Driver pod resources. Defaults to None.</p> <code>None</code> <code>executor_resources</code> <code>PodResources</code> <p>Executor pod resources. Defaults to None.</p> <code>None</code> <code>executor_instances</code> <code>ExecutorInstances</code> <p>Executor instances. Defaults to None.</p> <code>None</code> <code>secret_values</code> <code>dict[str, str]</code> <p>Dictionary of secret values to pass to the application as environment variables. Defaults to None.</p> <code>None</code> <code>volumes</code> <code>list[V1Volume] | None</code> <p>List of volumes to mount to the driver and/or executors.</p> <code>None</code> <code>driver_volume_mounts</code> <code>list[V1VolumeMount] | None</code> <p>List of volume mounts to mount to the driver.</p> <code>None</code> <code>executor_volume_mounts</code> <code>list[V1VolumeMount] | None</code> <p>List of volume mounts to mount to the executors.</p> <code>None</code> <code>driver_node_selector</code> <code>dict[str, str] | None</code> <p>Node selector for the driver pod.</p> <code>None</code> <code>executor_node_selector</code> <code>dict[str, str] | None</code> <p>Node selector for the executor pods.</p> <code>None</code> <code>driver_tolerations</code> <code>list[V1Toleration] | None</code> <p>Tolerations for the driver pod.</p> <code>None</code> <code>spark_on_k8s_service_url</code> <code>str | None</code> <p>URL of the Spark On K8S service. Defaults to None.</p> <code>None</code> <code>kubernetes_conn_id</code> <code>str</code> <p>Kubernetes connection ID. Defaults to \"kubernetes_default\".</p> <code>'kubernetes_default'</code> <code>poll_interval</code> <code>int</code> <p>Poll interval for checking the Spark application status. Defaults to 10.</p> <code>10</code> <code>deferrable</code> <code>bool</code> <p>Whether the operator is deferrable. Defaults to False.</p> <code>False</code> <code>on_kill_action</code> <code>Literal['keep', 'delete', 'kill']</code> <p>Action to take when the operator is killed. Defaults to \"delete\".</p> <code>DELETE</code> <code>**kwargs</code> <p>Other keyword arguments for BaseOperator.</p> <code>{}</code> Source code in <code>spark_on_k8s/airflow/operators.py</code> <pre><code>class SparkOnK8SOperator(BaseOperator):\n    \"\"\"Submit a Spark application on Kubernetes.\n\n    Args:\n        image (str): Spark application image.\n        app_path (str): Path to the Spark application.\n        namespace (str, optional): Kubernetes namespace. Defaults to \"default\".\n        service_account (str, optional): Kubernetes service account. Defaults to \"spark\".\n        app_name (str, optional): Spark application name. Defaults to None.\n        app_id_suffix: A suffix for the application ID, defaults to current timestamp in\n            the format %Y%m%d%H%M%S prefixed with a dash.\n        spark_conf (dict[str, str], optional): Spark configuration. Defaults to None.\n        class_name (str, optional): Spark application class name. Defaults to None.\n        app_arguments (list[str], optional): Spark application arguments. Defaults to None.\n        app_waiter (Literal[\"no_wait\", \"wait\", \"log\"], optional): Spark application waiter.\n            Defaults to \"wait\".\n        image_pull_policy (Literal[\"Always\", \"Never\", \"IfNotPresent\"], optional): Image pull policy.\n            Defaults to \"IfNotPresent\".\n        ui_reverse_proxy (bool, optional): Whether to use a reverse proxy for the Spark UI.\n            Defaults to False.\n        driver_resources (PodResources, optional): Driver pod resources. Defaults to None.\n        executor_resources (PodResources, optional): Executor pod resources. Defaults to None.\n        executor_instances (ExecutorInstances, optional): Executor instances. Defaults to None.\n        secret_values (dict[str, str], optional): Dictionary of secret values to pass to the application\n            as environment variables. Defaults to None.\n        volumes: List of volumes to mount to the driver and/or executors.\n        driver_volume_mounts: List of volume mounts to mount to the driver.\n        executor_volume_mounts: List of volume mounts to mount to the executors.\n        driver_node_selector: Node selector for the driver pod.\n        executor_node_selector: Node selector for the executor pods.\n        driver_tolerations: Tolerations for the driver pod.\n        spark_on_k8s_service_url: URL of the Spark On K8S service. Defaults to None.\n        kubernetes_conn_id (str, optional): Kubernetes connection ID. Defaults to\n            \"kubernetes_default\".\n        poll_interval (int, optional): Poll interval for checking the Spark application status.\n            Defaults to 10.\n        deferrable (bool, optional): Whether the operator is deferrable. Defaults to False.\n        on_kill_action (Literal[\"keep\", \"delete\", \"kill\"], optional): Action to take when the\n            operator is killed. Defaults to \"delete\".\n        **kwargs: Other keyword arguments for BaseOperator.\n    \"\"\"\n\n    _XCOM_DRIVER_POD_NAMESPACE = \"driver_pod_namespace\"\n    _XCOM_DRIVER_POD_NAME = \"driver_pod_name\"\n    XCOM_SPARK_UI_LINK = \"spark_ui_link\"\n\n    _driver_pod_name: str | None = None\n\n    operator_extra_links = (SparkOnK8SOperatorLink(),)\n\n    template_fields = (\n        \"image\",\n        \"app_path\",\n        \"namespace\",\n        \"service_account\",\n        \"app_name\",\n        \"app_id_suffix\",\n        \"spark_conf\",\n        \"class_name\",\n        \"app_arguments\",\n        \"app_waiter\",\n        \"image_pull_policy\",\n        \"driver_resources\",\n        \"executor_resources\",\n        \"executor_instances\",\n        \"secret_values\",\n        \"kubernetes_conn_id\",\n    )\n\n    def __init__(\n        self,\n        *,\n        image: str,\n        app_path: str,\n        namespace: str = \"default\",\n        service_account: str = \"spark\",\n        app_name: str | None = None,\n        app_id_suffix: str = None,\n        spark_conf: dict[str, str] | None = None,\n        class_name: str | None = None,\n        app_arguments: list[str] | None = None,\n        app_waiter: Literal[\"no_wait\", \"wait\", \"log\"] = \"wait\",\n        image_pull_policy: Literal[\"Always\", \"Never\", \"IfNotPresent\"] = \"IfNotPresent\",\n        ui_reverse_proxy: bool = False,\n        driver_resources: PodResources | None = None,\n        executor_resources: PodResources | None = None,\n        executor_instances: ExecutorInstances | None = None,\n        secret_values: dict[str, str] | None = None,\n        volumes: list[k8s.V1Volume] | None = None,\n        driver_volume_mounts: list[k8s.V1VolumeMount] | None = None,\n        executor_volume_mounts: list[k8s.V1VolumeMount] | None = None,\n        driver_node_selector: dict[str, str] | None = None,\n        executor_node_selector: dict[str, str] | None = None,\n        driver_labels: dict[str, str] | None = None,\n        executor_labels: dict[str, str] | None = None,\n        driver_annotations: dict[str, str] | None = None,\n        executor_annotations: dict[str, str] | None = None,\n        driver_tolerations: list[k8s.V1Toleration] | None = None,\n        executor_pod_template_path: str | None = None,\n        spark_on_k8s_service_url: str | None = None,\n        kubernetes_conn_id: str = \"kubernetes_default\",\n        poll_interval: int = 10,\n        deferrable: bool = False,\n        on_kill_action: Literal[\"keep\", \"delete\", \"kill\"] = OnKillAction.DELETE,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        self.image = image\n        self.app_path = app_path\n        self.namespace = namespace\n        self.service_account = service_account\n        self.app_name = app_name\n        self.app_id_suffix = app_id_suffix\n        self.spark_conf = spark_conf\n        self.class_name = class_name\n        self.app_arguments = app_arguments\n        self.app_waiter = app_waiter\n        self.image_pull_policy = image_pull_policy\n        self.ui_reverse_proxy = ui_reverse_proxy\n        self.driver_resources = driver_resources\n        self.executor_resources = executor_resources\n        self.executor_instances = executor_instances\n        self.secret_values = secret_values\n        self.volumes = volumes\n        self.driver_volume_mounts = driver_volume_mounts\n        self.executor_volume_mounts = executor_volume_mounts\n        self.driver_node_selector = driver_node_selector\n        self.executor_node_selector = executor_node_selector\n        self.driver_labels = driver_labels\n        self.executor_labels = executor_labels\n        self.driver_annotations = driver_annotations\n        self.executor_annotations = executor_annotations\n        self.driver_tolerations = driver_tolerations\n        self.executor_pod_template_path = executor_pod_template_path\n        self.spark_on_k8s_service_url = spark_on_k8s_service_url\n        self.kubernetes_conn_id = kubernetes_conn_id\n        self.poll_interval = poll_interval\n        self.deferrable = deferrable\n        self.on_kill_action = on_kill_action\n\n    def _render_nested_template_fields(\n        self,\n        content: Any,\n        context: Context,\n        jinja_env: jinja2.Environment,\n        seen_oids: set,\n    ) -&gt; None:\n        \"\"\"Render nested template fields.\"\"\"\n        from spark_on_k8s.client import ExecutorInstances, PodResources\n\n        if id(content) not in seen_oids:\n            template_fields: tuple | None\n\n            if isinstance(content, PodResources):\n                template_fields = (\"cpu\", \"memory\", \"memory_overhead\")\n            elif isinstance(content, ExecutorInstances):\n                template_fields = (\"min\", \"max\", \"initial\")\n            else:\n                template_fields = None\n\n            if template_fields:\n                seen_oids.add(id(content))\n                self._do_render_template_fields(content, template_fields, context, jinja_env, seen_oids)\n                return\n\n        super()._render_nested_template_fields(content, context, jinja_env, seen_oids)\n\n    def _persist_pod_name(self, context: Context):\n        context[\"ti\"].xcom_push(key=self._XCOM_DRIVER_POD_NAMESPACE, value=self.namespace)\n        context[\"ti\"].xcom_push(key=self._XCOM_DRIVER_POD_NAME, value=self._driver_pod_name)\n\n    def _persist_spark_ui_link(self, context: Context):\n        if self.spark_on_k8s_service_url:\n            SparkOnK8SOperatorLink.persist_spark_ui_link(\n                context,\n                self,\n                spark_on_k8s_service_url=self.spark_on_k8s_service_url,\n                namespace=self.namespace,\n                spark_app_id=self._driver_pod_name[: -len(\"-driver\")],\n            )\n\n    def _persist_spark_history_ui_link(self, context: Context):\n        if self.spark_on_k8s_service_url:\n            SparkOnK8SOperatorLink.persist_spark_history_ui_link(\n                context,\n                self,\n                spark_on_k8s_service_url=self.spark_on_k8s_service_url,\n                spark_app_id=self._driver_pod_name,\n            )\n\n    def _try_to_adopt_job(self, context: Context, spark_app_manager: SparkAppManager) -&gt; bool:\n        from spark_on_k8s.utils.spark_app_status import SparkAppStatus\n\n        xcom_driver_namespace = context[\"ti\"].xcom_pull(key=self._XCOM_DRIVER_POD_NAMESPACE)\n        if not xcom_driver_namespace or xcom_driver_namespace != self.namespace:\n            return False\n        xcom_driver_pod_name = context[\"ti\"].xcom_pull(key=self._XCOM_DRIVER_POD_NAME)\n        if xcom_driver_pod_name:\n            with contextlib.suppress(Exception):\n                app_status = spark_app_manager.app_status(\n                    namespace=xcom_driver_namespace,\n                    pod_name=xcom_driver_pod_name,\n                )\n                if app_status == SparkAppStatus.Running:\n                    self._driver_pod_name = xcom_driver_pod_name\n                    return True\n        return False\n\n    def _submit_new_job(self, context: Context):\n        from spark_on_k8s.client import ExecutorInstances, PodResources, SparkOnK8S\n\n        # post-process template fields\n        if self.driver_resources:\n            self.driver_resources = PodResources(\n                cpu=int(self.driver_resources.cpu) if self.driver_resources.cpu is not None else None,\n                memory=int(self.driver_resources.memory)\n                if self.driver_resources.memory is not None\n                else None,\n                memory_overhead=int(self.driver_resources.memory_overhead)\n                if self.driver_resources.memory_overhead is not None\n                else None,\n            )\n        if self.executor_resources:\n            self.executor_resources = PodResources(\n                cpu=int(self.executor_resources.cpu) if self.executor_resources.cpu is not None else None,\n                memory=int(self.executor_resources.memory)\n                if self.executor_resources.memory is not None\n                else None,\n                memory_overhead=int(self.executor_resources.memory_overhead),\n            )\n        if self.executor_instances:\n            self.executor_instances = ExecutorInstances(\n                min=int(self.executor_instances.min) if self.executor_instances.min is not None else None,\n                max=int(self.executor_instances.max) if self.executor_instances.max is not None else None,\n                initial=int(self.executor_instances.initial)\n                if self.executor_instances.initial is not None\n                else None,\n            )\n\n        k8s_client_manager = _AirflowKubernetesClientManager(\n            kubernetes_conn_id=self.kubernetes_conn_id,\n        )\n        spark_client = SparkOnK8S(\n            k8s_client_manager=k8s_client_manager,\n        )\n        submit_app_kwargs = {}\n        if self.app_id_suffix:\n            submit_app_kwargs[\"app_id_suffix\"] = lambda: self.app_id_suffix\n        self._driver_pod_name = spark_client.submit_app(\n            image=self.image,\n            app_path=self.app_path,\n            namespace=self.namespace,\n            service_account=self.service_account,\n            app_name=self.app_name,\n            spark_conf=self.spark_conf,\n            class_name=self.class_name,\n            app_arguments=self.app_arguments,\n            app_waiter=\"no_wait\",\n            image_pull_policy=self.image_pull_policy,\n            ui_reverse_proxy=self.ui_reverse_proxy,\n            driver_resources=self.driver_resources,\n            executor_resources=self.executor_resources,\n            executor_instances=self.executor_instances,\n            secret_values=self.secret_values,\n            volumes=self.volumes,\n            driver_volume_mounts=self.driver_volume_mounts,\n            executor_volume_mounts=self.executor_volume_mounts,\n            driver_node_selector=self.driver_node_selector,\n            executor_node_selector=self.executor_node_selector,\n            driver_labels=self.driver_labels,\n            executor_labels=self.executor_labels,\n            driver_annotations=self.driver_annotations,\n            executor_annotations=self.executor_annotations,\n            driver_tolerations=self.driver_tolerations,\n            executor_pod_template_path=self.executor_pod_template_path,\n            **submit_app_kwargs,\n        )\n\n    def execute(self, context: Context):\n        from spark_on_k8s.utils.app_manager import SparkAppManager\n\n        k8s_client_manager = _AirflowKubernetesClientManager(\n            kubernetes_conn_id=self.kubernetes_conn_id,\n        )\n        spark_app_manager = SparkAppManager(\n            k8s_client_manager=k8s_client_manager,\n        )\n        if not self._try_to_adopt_job(context, spark_app_manager):\n            self._submit_new_job(context)\n            self._persist_pod_name(context)\n        self._persist_spark_ui_link(context)\n        if self.app_waiter == \"no_wait\":\n            return\n        if self.deferrable:\n            self.defer(\n                trigger=SparkOnK8STrigger(\n                    driver_pod_name=self._driver_pod_name,\n                    namespace=self.namespace,\n                    kubernetes_conn_id=self.kubernetes_conn_id,\n                    poll_interval=self.poll_interval,\n                ),\n                method_name=\"execute_complete\",\n            )\n        if self.app_waiter == \"wait\":\n            spark_app_manager.wait_for_app(\n                namespace=self.namespace,\n                pod_name=self._driver_pod_name,\n                poll_interval=self.poll_interval,\n            )\n        elif self.app_waiter == \"log\":\n            spark_app_manager.stream_logs(\n                namespace=self.namespace,\n                pod_name=self._driver_pod_name,\n            )\n            # wait for termination status\n            spark_app_manager.wait_for_app(\n                namespace=self.namespace,\n                pod_name=self._driver_pod_name,\n                poll_interval=1,\n            )\n        app_status = spark_app_manager.app_status(\n            namespace=self.namespace,\n            pod_name=self._driver_pod_name,\n        )\n        self._persist_spark_history_ui_link(context)\n        if app_status == \"Succeeded\":\n            return app_status\n        raise AirflowException(f\"The job finished with status: {app_status}\")\n\n    def execute_complete(self, context: Context, event: dict, **kwargs):\n        if self.app_waiter == \"log\":\n            from spark_on_k8s.utils.app_manager import SparkAppManager\n\n            k8s_client_manager = _AirflowKubernetesClientManager(\n                kubernetes_conn_id=self.kubernetes_conn_id,\n            )\n            spark_app_manager = SparkAppManager(\n                k8s_client_manager=k8s_client_manager,\n            )\n            spark_app_manager.stream_logs(\n                namespace=event[\"namespace\"],\n                pod_name=event[\"pod_name\"],\n            )\n        self._persist_spark_history_ui_link(context)\n        if event[\"status\"] == \"Succeeded\":\n            return event[\"status\"]\n        if event[\"status\"] == \"error\":\n            raise AirflowException(\n                f\"SparkOnK8STrigger failed: with error: {event['error']}\\n\"\n                f\"Stacktrace: {event['stacktrace']}\"\n            )\n        raise AirflowException(f\"The job finished with status: {event['status']}\")\n\n    def on_kill(self) -&gt; None:\n        from airflow.operators.python import get_current_context\n\n        if self.on_kill_action == OnKillAction.KEEP:\n            return\n        self.log.warning(self._driver_pod_name)\n        if self._driver_pod_name:\n            from spark_on_k8s.utils.app_manager import SparkAppManager\n\n            k8s_client_manager = _AirflowKubernetesClientManager(\n                kubernetes_conn_id=self.kubernetes_conn_id,\n            )\n            spark_app_manager = SparkAppManager(\n                k8s_client_manager=k8s_client_manager,\n            )\n            if self.on_kill_action == OnKillAction.DELETE:\n                self.log.info(\"Deleting Spark application...\")\n                spark_app_manager.delete_app(\n                    namespace=self.namespace,\n                    pod_name=self._driver_pod_name,\n                )\n            elif self.on_kill_action == OnKillAction.KILL:\n                self.log.info(\"Killing Spark application...\")\n                spark_app_manager.kill_app(\n                    namespace=self.namespace,\n                    pod_name=self._driver_pod_name,\n                )\n            else:\n                raise AirflowException(f\"Invalid on_kill_action: {self.on_kill_action}\")\n\n            self._persist_spark_history_ui_link(get_current_context())\n</code></pre>"},{"location":"reference/spark_on_k8s/airflow/provider_info/","title":"provider_info","text":""},{"location":"reference/spark_on_k8s/airflow/triggers/","title":"triggers","text":""},{"location":"reference/spark_on_k8s/airflow/triggers/#spark_on_k8s.airflow.triggers.SparkOnK8STrigger","title":"<code>SparkOnK8STrigger</code>","text":"<p>               Bases: <code>BaseTrigger</code></p> <p>Watch a Spark application on Kubernetes.</p> Source code in <code>spark_on_k8s/airflow/triggers.py</code> <pre><code>class SparkOnK8STrigger(BaseTrigger):\n    \"\"\"Watch a Spark application on Kubernetes.\"\"\"\n\n    def __init__(\n        self,\n        *,\n        driver_pod_name: str,\n        namespace: str = \"default\",\n        kubernetes_conn_id: str = \"kubernetes_default\",\n        poll_interval: int = 10,\n    ):\n        super().__init__()\n        self.driver_pod_name = driver_pod_name\n        self.namespace = namespace\n        self.kubernetes_conn_id = kubernetes_conn_id\n        self.poll_interval = poll_interval\n\n    def serialize(self) -&gt; tuple[str, dict[str, Any]]:\n        return (\n            \"spark_on_k8s.airflow.triggers.SparkOnK8STrigger\",\n            {\n                \"driver_pod_name\": self.driver_pod_name,\n                \"namespace\": self.namespace,\n                \"kubernetes_conn_id\": self.kubernetes_conn_id,\n                \"poll_interval\": self.poll_interval,\n            },\n        )\n\n    async def run(self) -&gt; AsyncIterator[TriggerEvent]:\n        from spark_on_k8s.utils.async_app_manager import AsyncSparkAppManager\n\n        try:\n            k8s_client_manager = _AirflowKubernetesAsyncClientManager(\n                kubernetes_conn_id=self.kubernetes_conn_id,\n            )\n            async_spark_app_manager = AsyncSparkAppManager(\n                k8s_client_manager=k8s_client_manager,\n            )\n            await async_spark_app_manager.wait_for_app(\n                namespace=self.namespace,\n                pod_name=self.driver_pod_name,\n                poll_interval=self.poll_interval,\n            )\n            yield TriggerEvent(\n                {\n                    \"namespace\": self.namespace,\n                    \"pod_name\": self.driver_pod_name,\n                    \"status\": await async_spark_app_manager.app_status(\n                        namespace=self.namespace,\n                        pod_name=self.driver_pod_name,\n                    ),\n                }\n            )\n        except Exception as e:\n            yield TriggerEvent(\n                {\n                    \"namespace\": self.namespace,\n                    \"pod_name\": self.driver_pod_name,\n                    \"status\": \"error\",\n                    \"error\": str(e),\n                    \"stacktrace\": traceback.format_exc(),\n                }\n            )\n</code></pre>"},{"location":"reference/spark_on_k8s/airflow/scripts/spark_sql/","title":"spark_sql","text":""},{"location":"reference/spark_on_k8s/airflow/scripts/spark_sql/#spark_on_k8s.airflow.scripts.spark_sql.execute_sql_commands","title":"<code>execute_sql_commands(spark_session, commands)</code>","text":"<p>Executes a list of SQL commands on a Spark session.</p> Source code in <code>spark_on_k8s/airflow/scripts/spark_sql.py</code> <pre><code>def execute_sql_commands(spark_session: SparkSession, commands):\n    \"\"\"\n    Executes a list of SQL commands on a Spark session.\n    \"\"\"\n    for command in commands:\n        try:\n            logger.info(f\"Executing: {command}\")\n            spark_session.sql(command).show(truncate=False)\n        except Exception:\n            logger.exception(f\"Error executing SQL command: {command}\")\n</code></pre>"},{"location":"reference/spark_on_k8s/airflow/scripts/spark_sql/#spark_on_k8s.airflow.scripts.spark_sql.read_sql_file","title":"<code>read_sql_file(file_path)</code>","text":"<p>Reads an SQL file and splits commands by semicolon \";\".</p> Source code in <code>spark_on_k8s/airflow/scripts/spark_sql.py</code> <pre><code>def read_sql_file(file_path):\n    \"\"\"\n    Reads an SQL file and splits commands by semicolon \";\".\n    \"\"\"\n    with open(file_path) as f:\n        file_content = f.read()\n    # Split commands by semicolon while ignoring empty lines\n    return [command.strip() for command in file_content.split(\";\") if command.strip()]\n</code></pre>"},{"location":"reference/spark_on_k8s/api/app/","title":"app","text":""},{"location":"reference/spark_on_k8s/api/app/#spark_on_k8s.api.app.delete_app","title":"<code>delete_app(namespace, app_id, force=False)</code>  <code>async</code>","text":"<p>This endpoint deletes a spark application by its app_id.</p> <p>It deletes the driver pod, then the Kubernetes garbage collector will delete the executor pods and the other resources.</p> Source code in <code>spark_on_k8s/api/app.py</code> <pre><code>@router.delete(\n    \"/{namespace}/{app_id}\",\n    summary=\"Delete a spark application\",\n)\nasync def delete_app(namespace: str, app_id: str, force: bool = False):\n    \"\"\"This endpoint deletes a spark application by its app_id.\n\n    It deletes the driver pod, then the Kubernetes garbage collector\n    will delete the executor pods and the other resources.\n    \"\"\"\n    async_spark_app_manager = AsyncSparkAppManager(\n        k8s_client_manager=KubernetesClientSingleton.client_manager\n    )\n    try:\n        await async_spark_app_manager.delete_app(namespace=namespace, app_id=app_id)\n        return Response(status_code=200)\n    except Exception as e:\n        # TODO: handle exceptions properly and return proper status code\n        return handle_exception(e, 500)\n</code></pre>"},{"location":"reference/spark_on_k8s/api/app/#spark_on_k8s.api.app.kill_app","title":"<code>kill_app(namespace, app_id)</code>  <code>async</code>","text":"<p>This endpoint kills a spark application by its app_id.</p> <p>It sends a SIGTERM to the init process of the driver pod (PID 1).</p> Source code in <code>spark_on_k8s/api/app.py</code> <pre><code>@router.patch(\n    \"/{namespace}/{app_id}\",\n    summary=\"Kill a spark application\",\n)\nasync def kill_app(namespace: str, app_id: str):\n    \"\"\"This endpoint kills a spark application by its app_id.\n\n    It sends a SIGTERM to the init process of the driver pod (PID 1).\n    \"\"\"\n    async_spark_app_manager = AsyncSparkAppManager(\n        k8s_client_manager=KubernetesClientSingleton.client_manager\n    )\n    try:\n        await async_spark_app_manager.kill_app(namespace=namespace, app_id=app_id)\n        return Response(status_code=200)\n    except Exception as e:\n        # TODO: handle exceptions properly and return proper status code\n        return handle_exception(e, 500)\n</code></pre>"},{"location":"reference/spark_on_k8s/api/apps/","title":"apps","text":""},{"location":"reference/spark_on_k8s/api/apps/#spark_on_k8s.api.apps.SparkApp","title":"<code>SparkApp</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>App status.</p> Source code in <code>spark_on_k8s/api/apps.py</code> <pre><code>class SparkApp(BaseModel):\n    \"\"\"App status.\"\"\"\n\n    app_id: str\n    status: SparkAppStatus\n    driver_logs: bool = False\n    spark_ui_proxy: bool = False\n    spark_history_proxy: bool = False\n</code></pre>"},{"location":"reference/spark_on_k8s/api/apps/#spark_on_k8s.api.apps.list_apps","title":"<code>list_apps(namespace)</code>  <code>async</code>","text":"<p>List spark apps in a namespace.</p> Source code in <code>spark_on_k8s/api/apps.py</code> <pre><code>@router.get(\"/list_apps/{namespace}\")\nasync def list_apps(namespace: str) -&gt; list[SparkApp]:\n    \"\"\"List spark apps in a namespace.\"\"\"\n    core_client = CoreV1Api(await KubernetesClientSingleton.client())\n    driver_pods = await core_client.list_namespaced_pod(\n        namespace=namespace, label_selector=\"spark-role=driver\"\n    )\n    return [\n        SparkApp(\n            app_id=pod.metadata.labels.get(\"spark-app-id\", pod.metadata.name),\n            status=get_app_status(pod),\n            driver_logs=True,\n            spark_ui_proxy=pod.metadata.labels.get(\"spark-ui-proxy\", False),\n        )\n        for pod in driver_pods.items\n    ]\n</code></pre>"},{"location":"reference/spark_on_k8s/api/apps/#spark_on_k8s.api.apps.list_apps_default_namespace","title":"<code>list_apps_default_namespace()</code>  <code>async</code>","text":"<p>List spark apps in the default namespace.</p> Source code in <code>spark_on_k8s/api/apps.py</code> <pre><code>@router.get(\"/list_apps\")\nasync def list_apps_default_namespace() -&gt; list[SparkApp]:\n    \"\"\"List spark apps in the default namespace.\"\"\"\n    return await list_apps(namespace=APIConfiguration.SPARK_ON_K8S_API_DEFAULT_NAMESPACE)\n</code></pre>"},{"location":"reference/spark_on_k8s/api/configuration/","title":"configuration","text":""},{"location":"reference/spark_on_k8s/api/configuration/#spark_on_k8s.api.configuration.APIConfiguration","title":"<code>APIConfiguration</code>","text":"<p>API configuration.</p> Source code in <code>spark_on_k8s/api/configuration.py</code> <pre><code>class APIConfiguration:\n    \"\"\"API configuration.\"\"\"\n\n    # API configuration\n    SPARK_ON_K8S_API_DEFAULT_NAMESPACE = getenv(\"SPARK_ON_K8S_API_DEFAULT_NAMESPACE\", \"default\")\n    SPARK_ON_K8S_API_HOST = getenv(\"SPARK_ON_K8S_API_HOST\", \"127.0.0.1\")\n    SPARK_ON_K8S_API_PORT = int(getenv(\"SPARK_ON_K8S_API_PORT\", \"8000\"))\n    SPARK_ON_K8S_API_WORKERS = int(getenv(\"SPARK_ON_K8S_API_WORKERS\", \"4\"))\n    SPARK_ON_K8S_API_LOG_LEVEL = getenv(\"SPARK_ON_K8S_API_LOG_LEVEL\", \"info\")\n    SPARK_ON_K8S_API_LIMIT_CONCURRENCY = int(getenv(\"SPARK_ON_K8S_API_LIMIT_CONCURRENCY\", \"1000\"))\n    SPARK_ON_K8S_API_SPARK_HISTORY_HOST = getenv(\"SPARK_ON_K8S_API_SPARK_HISTORY_HOST\", None)\n</code></pre>"},{"location":"reference/spark_on_k8s/api/main/","title":"main","text":""},{"location":"reference/spark_on_k8s/api/utils/","title":"utils","text":""},{"location":"reference/spark_on_k8s/api/auth/auth_manager/","title":"auth_manager","text":""},{"location":"reference/spark_on_k8s/api/auth/auth_manager/#spark_on_k8s.api.auth.auth_manager.AppPermission","title":"<code>AppPermission</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Permissions for the app resource.</p> Source code in <code>spark_on_k8s/api/auth/auth_manager.py</code> <pre><code>class AppPermission(StrEnum):\n    \"\"\"Permissions for the app resource.\"\"\"\n    LIST = \"LIST\"\n    GET = \"GET\"\n    KILL = \"KILL\"\n    DELETE = \"DELETE\"\n</code></pre>"},{"location":"reference/spark_on_k8s/api/auth/auth_manager/#spark_on_k8s.api.auth.auth_manager.BaseAuthManager","title":"<code>BaseAuthManager</code>","text":"<p>               Bases: <code>ABC</code>, <code>Generic[UserInfo]</code></p> <p>Base class for the authentication manager.</p> Source code in <code>spark_on_k8s/api/auth/auth_manager.py</code> <pre><code>class BaseAuthManager(ABC, Generic[UserInfo]):\n    \"\"\"\n    Base class for the authentication manager.\n    \"\"\"\n    @abstractmethod\n    async def authenticate(self, request: Request) -&gt; UserInfo | None:\n        \"\"\"Parse the request and authenticate the user.\"\"\"\n        raise NotImplementedError()\n\n    @abstractmethod\n    async def is_authorized(self, user: UserInfo, resource: str, actions: list[PermissionActions]) -&gt; bool:\n        \"\"\"Check if the user is authorized to perform the actions on the resource.\"\"\"\n        raise NotImplementedError()\n\n    @final\n    async def check_permissions(self, request: Request, security_scopes: SecurityScopes):\n        print(security_scopes.scopes)\n        user = await self.authenticate(request)\n        if not user:\n            raise HTTPException(status_code=401, detail=\"Unauthorized\")\n        # TODO: get the resource and actions from the request\n        if not await self.is_authorized(user, \"test_resource\", [PermissionActions.GET]):\n            raise HTTPException(status_code=403, detail=\"Forbidden\")\n        return True\n</code></pre>"},{"location":"reference/spark_on_k8s/api/auth/auth_manager/#spark_on_k8s.api.auth.auth_manager.BaseAuthManager.authenticate","title":"<code>authenticate(request)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Parse the request and authenticate the user.</p> Source code in <code>spark_on_k8s/api/auth/auth_manager.py</code> <pre><code>@abstractmethod\nasync def authenticate(self, request: Request) -&gt; UserInfo | None:\n    \"\"\"Parse the request and authenticate the user.\"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"reference/spark_on_k8s/api/auth/auth_manager/#spark_on_k8s.api.auth.auth_manager.BaseAuthManager.is_authorized","title":"<code>is_authorized(user, resource, actions)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Check if the user is authorized to perform the actions on the resource.</p> Source code in <code>spark_on_k8s/api/auth/auth_manager.py</code> <pre><code>@abstractmethod\nasync def is_authorized(self, user: UserInfo, resource: str, actions: list[PermissionActions]) -&gt; bool:\n    \"\"\"Check if the user is authorized to perform the actions on the resource.\"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"reference/spark_on_k8s/api/auth/auth_manager/#spark_on_k8s.api.auth.auth_manager.HttpBasicAuthManager","title":"<code>HttpBasicAuthManager</code>","text":"<p>               Bases: <code>BaseAuthManager[BaseUser]</code></p> <p>HTTP basic authentication manager.</p> Source code in <code>spark_on_k8s/api/auth/auth_manager.py</code> <pre><code>class HttpBasicAuthManager(BaseAuthManager[BaseUser]):\n    \"\"\"HTTP basic authentication manager.\"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.authenticator = HTTPBasic()\n\n    async def authenticate(self, request: Request) -&gt; HTTPBasicCredentials | None:\n        credentials = await self.authenticator(request)\n        if not credentials:\n            return None\n        return credentials\n\n    async def is_authorized(self, user: HTTPBasicCredentials, resource: str, actions: list[PermissionActions]) -&gt; bool:\n        return False\n</code></pre>"},{"location":"reference/spark_on_k8s/api/auth/auth_manager/#spark_on_k8s.api.auth.auth_manager.PermissionActions","title":"<code>PermissionActions</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Permission actions.</p> Source code in <code>spark_on_k8s/api/auth/auth_manager.py</code> <pre><code>class PermissionActions(StrEnum):\n    \"\"\"Permission actions.\"\"\"\n    GET = \"GET\"\n    PUT = \"PUT\"\n    POST = \"POST\"\n    DELETE = \"DELETE\"\n    PATCH = \"PATCH\"\n    ALL = \"ALL\"\n</code></pre>"},{"location":"reference/spark_on_k8s/api/auth/auth_manager/#spark_on_k8s.api.auth.auth_manager.Resource","title":"<code>Resource</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Spark on K8s resources API resources.</p> Source code in <code>spark_on_k8s/api/auth/auth_manager.py</code> <pre><code>class Resource(StrEnum):\n    \"\"\"Spark on K8s resources API resources.\"\"\"\n    APP = \"APP\"\n</code></pre>"},{"location":"reference/spark_on_k8s/api/auth/base_auth_manager/","title":"base_auth_manager","text":""},{"location":"reference/spark_on_k8s/api/auth/base_auth_manager/#spark_on_k8s.api.auth.base_auth_manager.BaseAuthManager","title":"<code>BaseAuthManager</code>","text":"<p>               Bases: <code>ABC</code>, <code>Generic[T, S]</code></p> <p>Base class for the authentication manager.</p> Source code in <code>spark_on_k8s/api/auth/base_auth_manager.py</code> <pre><code>class BaseAuthManager(ABC, Generic[T, S]):\n    \"\"\"\n    Base class for the authentication manager.\n    \"\"\"\n    authenticator: Callable[[Request], Coroutine[Any, Any, Optional[T]]] = None\n\n    @final\n    async def authenticate(self, request: Request) -&gt; Coroutine[Any, Any, Optional[T]]:\n        if not self.authenticator:\n            raise NotImplementedError(\"Authenticator not defined.\")\n        return self.authenticator(request)\n\n    @final\n    async def needs_authentication(self, auth_info: Annotated[T, Depends(authenticate)]) -&gt; bool:\n        user = await self.get_user(auth_info)\n        # TODO: get the resource and actions from the request\n        return await self.is_authorized(user, \"needs_authentication\", [PermissionActions.GET])\n\n    @abstractmethod\n    async def get_user(self, auth_info: T) -&gt; S:\n        \"\"\"Get the user.\"\"\"\n        raise NotImplementedError()\n\n\n    async def is_authorized(self, user: S, resource: str, actions: list[PermissionActions]) -&gt; bool:\n        \"\"\"Check if the user is authorized to perform the actions on the resource.\"\"\"\n        raise NotImplementedError()\n</code></pre>"},{"location":"reference/spark_on_k8s/api/auth/base_auth_manager/#spark_on_k8s.api.auth.base_auth_manager.BaseAuthManager.get_user","title":"<code>get_user(auth_info)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Get the user.</p> Source code in <code>spark_on_k8s/api/auth/base_auth_manager.py</code> <pre><code>@abstractmethod\nasync def get_user(self, auth_info: T) -&gt; S:\n    \"\"\"Get the user.\"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"reference/spark_on_k8s/api/auth/base_auth_manager/#spark_on_k8s.api.auth.base_auth_manager.BaseAuthManager.is_authorized","title":"<code>is_authorized(user, resource, actions)</code>  <code>async</code>","text":"<p>Check if the user is authorized to perform the actions on the resource.</p> Source code in <code>spark_on_k8s/api/auth/base_auth_manager.py</code> <pre><code>async def is_authorized(self, user: S, resource: str, actions: list[PermissionActions]) -&gt; bool:\n    \"\"\"Check if the user is authorized to perform the actions on the resource.\"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"reference/spark_on_k8s/api/auth/base_auth_manager/#spark_on_k8s.api.auth.base_auth_manager.PermissionActions","title":"<code>PermissionActions</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Permission actions.</p> Source code in <code>spark_on_k8s/api/auth/base_auth_manager.py</code> <pre><code>class PermissionActions(StrEnum):\n    \"\"\"Permission actions.\"\"\"\n    GET = \"GET\"\n    PUT = \"PUT\"\n    POST = \"POST\"\n    DELETE = \"DELETE\"\n    PATCH = \"PATCH\"\n    ALL = \"ALL\"\n</code></pre>"},{"location":"reference/spark_on_k8s/api/auth/base_user/","title":"base_user","text":""},{"location":"reference/spark_on_k8s/api/auth/resources/","title":"resources","text":""},{"location":"reference/spark_on_k8s/api/auth/resources/#spark_on_k8s.api.auth.resources.PermissionActions","title":"<code>PermissionActions</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Permission actions.</p> Source code in <code>spark_on_k8s/api/auth/resources.py</code> <pre><code>class PermissionActions(StrEnum):\n    \"\"\"Permission actions.\"\"\"\n    GET = \"GET\"\n    PUT = \"PUT\"\n    POST = \"POST\"\n    DELETE = \"DELETE\"\n    PATCH = \"PATCH\"\n    ALL = \"ALL\"\n</code></pre>"},{"location":"reference/spark_on_k8s/api/auth/resources/#spark_on_k8s.api.auth.resources.Resources","title":"<code>Resources</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Spark on K8s resources API resources.</p> Source code in <code>spark_on_k8s/api/auth/resources.py</code> <pre><code>class Resources(StrEnum):\n    \"\"\"Spark on K8s resources API resources.\"\"\"\n    APP = \"APP\"\n</code></pre>"},{"location":"reference/spark_on_k8s/api/webserver/security/","title":"security","text":""},{"location":"reference/spark_on_k8s/cli/api/","title":"api","text":""},{"location":"reference/spark_on_k8s/cli/app/","title":"app","text":""},{"location":"reference/spark_on_k8s/cli/apps/","title":"apps","text":""},{"location":"reference/spark_on_k8s/cli/namespace/","title":"namespace","text":""},{"location":"reference/spark_on_k8s/cli/options/","title":"options","text":""},{"location":"reference/spark_on_k8s/k8s/async_client/","title":"async_client","text":""},{"location":"reference/spark_on_k8s/k8s/async_client/#spark_on_k8s.k8s.async_client.KubernetesAsyncClientManager","title":"<code>KubernetesAsyncClientManager</code>","text":"<p>Kubernetes async client manager.</p> <p>Parameters:</p> Name Type Description Default <code>config_file</code> <code>str</code> <p>Path to the Kubernetes configuration file. Defaults to None.</p> <code>NOTSET</code> <code>context</code> <code>str</code> <p>Kubernetes context. Defaults to None.</p> <code>NOTSET</code> <code>client_configuration</code> <code>Configuration</code> <p>Kubernetes client configuration. Defaults to None.</p> <code>NOTSET</code> <code>in_cluster</code> <code>bool</code> <p>Whether to load the in-cluster config. Defaults to False.</p> <code>NOTSET</code> Source code in <code>spark_on_k8s/k8s/async_client.py</code> <pre><code>class KubernetesAsyncClientManager:\n    \"\"\"Kubernetes async client manager.\n\n    Args:\n        config_file (str, optional): Path to the Kubernetes configuration file. Defaults to None.\n        context (str, optional): Kubernetes context. Defaults to None.\n        client_configuration (k8s.Configuration, optional): Kubernetes client configuration.\n            Defaults to None.\n        in_cluster (bool, optional): Whether to load the in-cluster config. Defaults to False.\n    \"\"\"\n\n    def __init__(\n        self,\n        config_file: str | ArgNotSet = NOTSET,\n        context: str | ArgNotSet = NOTSET,\n        client_configuration: k8s.Configuration | ArgNotSet = NOTSET,\n        in_cluster: bool | ArgNotSet = NOTSET,\n    ) -&gt; None:\n        self.config_file = (\n            config_file if config_file is not NOTSET else Configuration.SPARK_ON_K8S_CONFIG_FILE\n        )\n        self.context = context if context is not NOTSET else Configuration.SPARK_ON_K8S_CONTEXT\n        self.client_configuration = (\n            client_configuration\n            if client_configuration is not NOTSET\n            else Configuration.SPARK_ON_K8S_CLIENT_CONFIG\n        )\n        self.in_cluster = in_cluster if in_cluster is not NOTSET else Configuration.SPARK_ON_K8S_IN_CLUSTER\n\n    @asynccontextmanager\n    async def client(self) -&gt; k8s.ApiClient:\n        \"\"\"Create a Kubernetes client in a context manager.\n\n        Examples:\n            &gt;&gt;&gt; import asyncio\n            &gt;&gt;&gt; from spark_on_k8s.k8s.async_client import KubernetesAsyncClientManager\n            &gt;&gt;&gt; async def get_namespaces():\n            &gt;&gt;&gt;     async with KubernetesAsyncClientManager().client() as async_client:\n            ...         api = k8s.CoreV1Api(async_client)\n            ...         namespaces = [ns.metadata.name for ns in await api.list_namespace().items]\n            ...         print(namespaces)\n            &gt;&gt;&gt; asyncio.run(get_namespaces())\n            ['default', 'kube-node-lease', 'kube-public', 'kube-system', 'spark']\n\n        Yields:\n            k8s.ApiClient: Kubernetes client.\n        \"\"\"\n        async_client = await self.create_client()\n        try:\n            yield async_client\n        finally:\n            await async_client.close()\n\n    async def create_client(self) -&gt; k8s.ApiClient:\n        \"\"\"Load the Kubernetes configuration and create a Kubernetes client.\n\n        This method could be overridden to create a Kubernetes client in a different way without\n        overriding the client method.\n\n        Returns:\n            k8s.ApiClient: Kubernetes client.\n        \"\"\"\n        if not self.in_cluster:\n            await config.load_kube_config(\n                config_file=self.config_file,\n                context=self.context,\n                client_configuration=self.client_configuration,\n            )\n        else:\n            config.load_incluster_config()\n        return k8s.ApiClient()\n</code></pre>"},{"location":"reference/spark_on_k8s/k8s/async_client/#spark_on_k8s.k8s.async_client.KubernetesAsyncClientManager.client","title":"<code>client()</code>  <code>async</code>","text":"<p>Create a Kubernetes client in a context manager.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import asyncio\n&gt;&gt;&gt; from spark_on_k8s.k8s.async_client import KubernetesAsyncClientManager\n&gt;&gt;&gt; async def get_namespaces():\n&gt;&gt;&gt;     async with KubernetesAsyncClientManager().client() as async_client:\n...         api = k8s.CoreV1Api(async_client)\n...         namespaces = [ns.metadata.name for ns in await api.list_namespace().items]\n...         print(namespaces)\n&gt;&gt;&gt; asyncio.run(get_namespaces())\n['default', 'kube-node-lease', 'kube-public', 'kube-system', 'spark']\n</code></pre> <p>Yields:</p> Type Description <code>ApiClient</code> <p>k8s.ApiClient: Kubernetes client.</p> Source code in <code>spark_on_k8s/k8s/async_client.py</code> <pre><code>@asynccontextmanager\nasync def client(self) -&gt; k8s.ApiClient:\n    \"\"\"Create a Kubernetes client in a context manager.\n\n    Examples:\n        &gt;&gt;&gt; import asyncio\n        &gt;&gt;&gt; from spark_on_k8s.k8s.async_client import KubernetesAsyncClientManager\n        &gt;&gt;&gt; async def get_namespaces():\n        &gt;&gt;&gt;     async with KubernetesAsyncClientManager().client() as async_client:\n        ...         api = k8s.CoreV1Api(async_client)\n        ...         namespaces = [ns.metadata.name for ns in await api.list_namespace().items]\n        ...         print(namespaces)\n        &gt;&gt;&gt; asyncio.run(get_namespaces())\n        ['default', 'kube-node-lease', 'kube-public', 'kube-system', 'spark']\n\n    Yields:\n        k8s.ApiClient: Kubernetes client.\n    \"\"\"\n    async_client = await self.create_client()\n    try:\n        yield async_client\n    finally:\n        await async_client.close()\n</code></pre>"},{"location":"reference/spark_on_k8s/k8s/async_client/#spark_on_k8s.k8s.async_client.KubernetesAsyncClientManager.create_client","title":"<code>create_client()</code>  <code>async</code>","text":"<p>Load the Kubernetes configuration and create a Kubernetes client.</p> <p>This method could be overridden to create a Kubernetes client in a different way without overriding the client method.</p> <p>Returns:</p> Type Description <code>ApiClient</code> <p>k8s.ApiClient: Kubernetes client.</p> Source code in <code>spark_on_k8s/k8s/async_client.py</code> <pre><code>async def create_client(self) -&gt; k8s.ApiClient:\n    \"\"\"Load the Kubernetes configuration and create a Kubernetes client.\n\n    This method could be overridden to create a Kubernetes client in a different way without\n    overriding the client method.\n\n    Returns:\n        k8s.ApiClient: Kubernetes client.\n    \"\"\"\n    if not self.in_cluster:\n        await config.load_kube_config(\n            config_file=self.config_file,\n            context=self.context,\n            client_configuration=self.client_configuration,\n        )\n    else:\n        config.load_incluster_config()\n    return k8s.ApiClient()\n</code></pre>"},{"location":"reference/spark_on_k8s/k8s/sync_client/","title":"sync_client","text":""},{"location":"reference/spark_on_k8s/k8s/sync_client/#spark_on_k8s.k8s.sync_client.KubernetesClientManager","title":"<code>KubernetesClientManager</code>","text":"<p>Kubernetes client manager.</p> <p>Parameters:</p> Name Type Description Default <code>config_file</code> <code>str</code> <p>Path to the Kubernetes configuration file. Defaults to None.</p> <code>NOTSET</code> <code>context</code> <code>str</code> <p>Kubernetes context. Defaults to None.</p> <code>NOTSET</code> <code>client_configuration</code> <code>Configuration</code> <p>Kubernetes client configuration. Defaults to None.</p> <code>NOTSET</code> Source code in <code>spark_on_k8s/k8s/sync_client.py</code> <pre><code>class KubernetesClientManager:\n    \"\"\"Kubernetes client manager.\n\n    Args:\n        config_file (str, optional): Path to the Kubernetes configuration file. Defaults to None.\n        context (str, optional): Kubernetes context. Defaults to None.\n        client_configuration (k8s.Configuration, optional): Kubernetes client configuration.\n            Defaults to None.\n    \"\"\"\n\n    def __init__(\n        self,\n        config_file: str | ArgNotSet = NOTSET,\n        context: str | ArgNotSet = NOTSET,\n        client_configuration: k8s.Configuration | ArgNotSet = NOTSET,\n        in_cluster: bool | ArgNotSet = NOTSET,\n    ) -&gt; None:\n        self.config_file = (\n            config_file if config_file is not NOTSET else Configuration.SPARK_ON_K8S_CONFIG_FILE\n        )\n        self.context = context if context is not NOTSET else Configuration.SPARK_ON_K8S_CONTEXT\n        self.client_configuration = (\n            client_configuration\n            if client_configuration is not NOTSET\n            else Configuration.SPARK_ON_K8S_CLIENT_CONFIG\n        )\n        self.in_cluster = in_cluster if in_cluster is not NOTSET else Configuration.SPARK_ON_K8S_IN_CLUSTER\n\n    @contextmanager\n    def client(self) -&gt; k8s.ApiClient:\n        \"\"\"Create a Kubernetes client in a context manager.\n\n        Examples:\n            &gt;&gt;&gt; from spark_on_k8s.k8s.sync_client import KubernetesClientManager\n            &gt;&gt;&gt; with KubernetesClientManager().client() as client:\n            ...     api = k8s.CoreV1Api(client)\n            ...     namespaces = [ns.metadata.name for ns in api.list_namespace().items]\n            ...     print(namespaces)\n            ['default', 'kube-node-lease', 'kube-public', 'kube-system', 'spark']\n\n        Yields:\n            k8s.ApiClient: Kubernetes client.\n        \"\"\"\n        _client = None\n        try:\n            _client = self.create_client()\n            yield _client\n        finally:\n            if _client:\n                _client.close()\n\n    def create_client(self) -&gt; k8s.ApiClient:\n        \"\"\"Load the Kubernetes configuration and create a Kubernetes client.\n\n        This method could be overridden to create a Kubernetes client in a different way without\n        overriding the client method.\n\n        Returns:\n            k8s.ApiClient: Kubernetes client.\n        \"\"\"\n        if not self.in_cluster:\n            config.load_kube_config(\n                config_file=self.config_file,\n                context=self.context,\n                client_configuration=self.client_configuration,\n            )\n        else:\n            config.load_incluster_config()\n        return k8s.ApiClient()\n</code></pre>"},{"location":"reference/spark_on_k8s/k8s/sync_client/#spark_on_k8s.k8s.sync_client.KubernetesClientManager.client","title":"<code>client()</code>","text":"<p>Create a Kubernetes client in a context manager.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spark_on_k8s.k8s.sync_client import KubernetesClientManager\n&gt;&gt;&gt; with KubernetesClientManager().client() as client:\n...     api = k8s.CoreV1Api(client)\n...     namespaces = [ns.metadata.name for ns in api.list_namespace().items]\n...     print(namespaces)\n['default', 'kube-node-lease', 'kube-public', 'kube-system', 'spark']\n</code></pre> <p>Yields:</p> Type Description <code>ApiClient</code> <p>k8s.ApiClient: Kubernetes client.</p> Source code in <code>spark_on_k8s/k8s/sync_client.py</code> <pre><code>@contextmanager\ndef client(self) -&gt; k8s.ApiClient:\n    \"\"\"Create a Kubernetes client in a context manager.\n\n    Examples:\n        &gt;&gt;&gt; from spark_on_k8s.k8s.sync_client import KubernetesClientManager\n        &gt;&gt;&gt; with KubernetesClientManager().client() as client:\n        ...     api = k8s.CoreV1Api(client)\n        ...     namespaces = [ns.metadata.name for ns in api.list_namespace().items]\n        ...     print(namespaces)\n        ['default', 'kube-node-lease', 'kube-public', 'kube-system', 'spark']\n\n    Yields:\n        k8s.ApiClient: Kubernetes client.\n    \"\"\"\n    _client = None\n    try:\n        _client = self.create_client()\n        yield _client\n    finally:\n        if _client:\n            _client.close()\n</code></pre>"},{"location":"reference/spark_on_k8s/k8s/sync_client/#spark_on_k8s.k8s.sync_client.KubernetesClientManager.create_client","title":"<code>create_client()</code>","text":"<p>Load the Kubernetes configuration and create a Kubernetes client.</p> <p>This method could be overridden to create a Kubernetes client in a different way without overriding the client method.</p> <p>Returns:</p> Type Description <code>ApiClient</code> <p>k8s.ApiClient: Kubernetes client.</p> Source code in <code>spark_on_k8s/k8s/sync_client.py</code> <pre><code>def create_client(self) -&gt; k8s.ApiClient:\n    \"\"\"Load the Kubernetes configuration and create a Kubernetes client.\n\n    This method could be overridden to create a Kubernetes client in a different way without\n    overriding the client method.\n\n    Returns:\n        k8s.ApiClient: Kubernetes client.\n    \"\"\"\n    if not self.in_cluster:\n        config.load_kube_config(\n            config_file=self.config_file,\n            context=self.context,\n            client_configuration=self.client_configuration,\n        )\n    else:\n        config.load_incluster_config()\n    return k8s.ApiClient()\n</code></pre>"},{"location":"reference/spark_on_k8s/utils/app_manager/","title":"app_manager","text":""},{"location":"reference/spark_on_k8s/utils/app_manager/#spark_on_k8s.utils.app_manager.SparkAppManager","title":"<code>SparkAppManager</code>","text":"<p>               Bases: <code>LoggingMixin</code></p> <p>Manage Spark apps on Kubernetes.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spark_on_k8s.utils.app_manager import SparkAppManager\n&gt;&gt;&gt; app_manager = SparkAppManager()\n&gt;&gt;&gt; app_manager.stream_logs(\n...     namespace=\"spark\",\n...     pod_name=\"20240114225118-driver\",\n...     should_print=True,\n... )\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>k8s_client_manager</code> <code>KubernetesClientManager</code> <p>Kubernetes client manager. Defaults to None.</p> <code>None</code> <code>logger_name</code> <code>str</code> <p>logger name. Defaults to \"SparkAppManager\".</p> <code>None</code> Source code in <code>spark_on_k8s/utils/app_manager.py</code> <pre><code>class SparkAppManager(LoggingMixin):\n    \"\"\"Manage Spark apps on Kubernetes.\n\n    Examples:\n        &gt;&gt;&gt; from spark_on_k8s.utils.app_manager import SparkAppManager\n        &gt;&gt;&gt; app_manager = SparkAppManager()\n        &gt;&gt;&gt; app_manager.stream_logs(\n        ...     namespace=\"spark\",\n        ...     pod_name=\"20240114225118-driver\",\n        ...     should_print=True,\n        ... )\n\n    Args:\n        k8s_client_manager (KubernetesClientManager, optional): Kubernetes client manager. Defaults to None.\n        logger_name (str, optional): logger name. Defaults to \"SparkAppManager\".\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        k8s_client_manager: KubernetesClientManager | None = None,\n        logger_name: str | None = None,\n    ):\n        super().__init__(logger_name=logger_name or \"SparkAppManager\")\n        self.k8s_client_manager = k8s_client_manager or KubernetesClientManager()\n\n    def app_status(\n        self,\n        *,\n        namespace: str,\n        pod_name: str | None = None,\n        app_id: str | None = None,\n        client: k8s.CoreV1Api | None = None,\n    ) -&gt; SparkAppStatus:\n        \"\"\"Get app status.\n\n        Args:\n            namespace (str): Namespace.\n            pod_name (str): Pod name. Defaults to None.\n            app_id (str): App ID. Defaults to None.\n            client (k8s.CoreV1Api, optional): Kubernetes client. Defaults to None.\n\n        Returns:\n            SparkAppStatus: App status.\n        \"\"\"\n\n        def _app_status(_client: k8s.CoreV1Api) -&gt; SparkAppStatus:\n            if pod_name is None and app_id is None:\n                raise ValueError(\"Either pod_name or app_id must be specified\")\n            if pod_name is not None:\n                _pod = _client.read_namespaced_pod(\n                    namespace=namespace,\n                    name=pod_name,\n                )\n            else:\n                _pod = _client.list_namespaced_pod(\n                    namespace=namespace,\n                    label_selector=f\"spark-app-id={app_id}\",\n                ).items[0]\n            return get_app_status(_pod)\n\n        if client is None:\n            with self.k8s_client_manager.client() as client:\n                api = k8s.CoreV1Api(client)\n                return _app_status(api)\n        return _app_status(client)\n\n    def wait_for_app(\n        self,\n        *,\n        namespace: str,\n        pod_name: str | None = None,\n        app_id: str | None = None,\n        poll_interval: float = 10,\n        should_print: bool = False,\n    ):\n        \"\"\"Wait for a Spark app to finish.\n\n        Args:\n            namespace (str): Namespace.\n            pod_name (str): Pod name.\n            app_id (str): App ID.\n            poll_interval (float, optional): Poll interval in seconds. Defaults to 10.\n            should_print (bool, optional): Whether to print logs instead of logging them.\n        \"\"\"\n        termination_statuses = {SparkAppStatus.Succeeded, SparkAppStatus.Failed, SparkAppStatus.Unknown}\n        with self.k8s_client_manager.client() as client:\n            api = k8s.CoreV1Api(client)\n            while True:\n                try:\n                    status = self.app_status(\n                        namespace=namespace, pod_name=pod_name, app_id=app_id, client=api\n                    )\n                    if status in termination_statuses:\n                        break\n                except ApiException as e:\n                    if e.status == 404:\n                        self.log(\n                            msg=f\"Pod {pod_name} was deleted\", level=logging.INFO, should_print=should_print\n                        )\n                        return\n                self.log(\n                    msg=f\"Pod {pod_name} status is {status}, sleep {poll_interval}s\",\n                    level=logging.INFO,\n                    should_print=should_print,\n                )\n                time.sleep(poll_interval)\n            self.log(\n                msg=f\"Pod {pod_name} finished with status {status.value}\",\n                level=logging.INFO,\n                should_print=should_print,\n            )\n\n    def stream_logs(\n        self,\n        *,\n        namespace: str,\n        pod_name: str | None = None,\n        app_id: str | None = None,\n        should_print: bool = False,\n    ):\n        \"\"\"Stream logs from a Spark app.\n\n        Args:\n            namespace (str): Namespace.\n            pod_name (str): Pod name.\n            app_id (str): App ID.\n            should_print (bool, optional): Whether to print logs instead of logging them.\n        \"\"\"\n        if pod_name is None and app_id is None:\n            raise ValueError(\"Either pod_name or app_id must be specified\")\n        if pod_name is None:\n            with self.k8s_client_manager.client() as client:\n                api = k8s.CoreV1Api(client)\n                pods = api.list_namespaced_pod(\n                    namespace=namespace,\n                    label_selector=f\"spark-app-id={app_id}\",\n                ).items\n                if len(pods) == 0:\n                    raise ValueError(f\"No pods found for app {app_id}\")\n                pod_name = pods[0].metadata.name\n        with self.k8s_client_manager.client() as client:\n            api = k8s.CoreV1Api(client)\n            while True:\n                pod = api.read_namespaced_pod(\n                    namespace=namespace,\n                    name=pod_name,\n                )\n                if pod.status.phase != \"Pending\":\n                    break\n            watcher = watch.Watch()\n            for line in watcher.stream(\n                api.read_namespaced_pod_log,\n                namespace=namespace,\n                name=pod_name,\n            ):\n                self.log(msg=line, level=logging.INFO, should_print=should_print)\n            watcher.stop()\n\n    def list_apps(self, namespace: str) -&gt; list[str]:\n        \"\"\"List apps.\n\n        Args:\n            namespace (str): Namespace.\n\n        Returns:\n            list[str]: Spark apps in the namespace.\n        \"\"\"\n        with self.k8s_client_manager.client() as client:\n            api = k8s.CoreV1Api(client)\n            return [\n                pod.metadata.labels[\"spark-app-id\"]\n                for pod in api.list_namespaced_pod(\n                    namespace=namespace,\n                    label_selector=\"spark-role=driver\",\n                ).items\n            ]\n\n    def kill_app(\n        self,\n        namespace: str,\n        pod_name: str | None = None,\n        app_id: str | None = None,\n        should_print: bool = False,\n    ):\n        \"\"\"Kill an app.\n\n        Args:\n            namespace (str): Namespace.\n            pod_name (str): Pod name.\n            app_id (str): App ID.\n            should_print (bool, optional): Whether to print logs instead of logging them.\n        \"\"\"\n        if pod_name is None and app_id is None:\n            raise ValueError(\"Either pod_name or app_id must be specified\")\n        with self.k8s_client_manager.client() as client:\n            api = k8s.CoreV1Api(client)\n            if pod_name is None:\n                pods = api.list_namespaced_pod(\n                    namespace=namespace,\n                    label_selector=f\"spark-app-id={app_id}\",\n                ).items\n                if len(pods) == 0:\n                    raise ValueError(f\"No pods found for app {app_id}\")\n                pod = pods[0]\n            else:\n                pod = api.read_namespaced_pod(\n                    namespace=namespace,\n                    name=pod_name,\n                )\n            container_name = pod.spec.containers[0].name\n            if pod.status.phase != \"Running\":\n                self.log(\n                    f\"App is not running, it is {get_app_status(pod).value}\",\n                    level=logging.INFO,\n                    should_print=should_print,\n                )\n                return\n            stream(\n                api.connect_get_namespaced_pod_exec,\n                pod.metadata.name,\n                namespace,\n                command=[\"/bin/sh\", \"-c\", \"kill 1\"],\n                container=container_name,\n                stderr=True,\n                stdin=False,\n                stdout=True,\n                tty=False,\n                _preload_content=False,\n            )\n\n    def delete_app(\n        self, namespace: str, pod_name: str | None = None, app_id: str | None = None, force: bool = False\n    ):\n        \"\"\"Delete an app.\n\n        Args:\n            namespace (str): Namespace.\n            pod_name (str): Pod name.\n            app_id (str): App ID.\n            force (bool, optional): Whether to force delete the app. Defaults to False.\n        \"\"\"\n        if pod_name is None and app_id is None:\n            raise ValueError(\"Either pod_name or app_id must be specified\")\n        with self.k8s_client_manager.client() as client:\n            api = k8s.CoreV1Api(client)\n            if app_id:\n                # we don't use `delete_collection_namespaced_pod` to know if the app exists or not\n                pods = api.list_namespaced_pod(\n                    namespace=namespace,\n                    label_selector=f\"spark-app-id={app_id}\",\n                ).items\n                if len(pods) == 0:\n                    raise ValueError(f\"No pods found for app {app_id}\")\n                pod_name = pods[0].metadata.name\n            api.delete_namespaced_pod(\n                name=pod_name,\n                namespace=namespace,\n                body=k8s.V1DeleteOptions(\n                    grace_period_seconds=0 if force else None,\n                    propagation_policy=\"Foreground\",\n                ),\n            )\n\n    @staticmethod\n    def create_spark_pod_spec(\n        *,\n        app_name: str,\n        app_id: str,\n        image: str,\n        namespace: str = \"default\",\n        service_account: str = \"spark\",\n        container_name: str = \"driver\",\n        env_variables: dict[str, str] | None = None,\n        pod_resources: dict[str, dict[str, str]] | None = None,\n        args: list[str] | None = None,\n        image_pull_policy: Literal[\"Always\", \"Never\", \"IfNotPresent\"] = \"IfNotPresent\",\n        extra_labels: dict[str, str] | None = None,\n        annotations: dict[str, str] | None = None,\n        env_from_secrets: list[str] | None = None,\n        volumes: list[k8s.V1Volume] | None = None,\n        volume_mounts: list[k8s.V1VolumeMount] | None = None,\n        node_selector: dict[str, str] | None = None,\n        tolerations: list[k8s.V1Toleration] | None = None,\n    ) -&gt; k8s.V1PodTemplateSpec:\n        \"\"\"Create a pod spec for a Spark application\n\n        Args:\n            app_name: Name of the Spark application\n            app_id: ID of the Spark application\n            image: Docker image to use for the Spark driver and executors\n            namespace: Kubernetes namespace to use, defaults to \"default\"\n            service_account: Kubernetes service account to use for the Spark driver, defaults to \"spark\"\n            container_name: Name of the container, defaults to \"driver\"\n            env_variables: Dictionary of environment variables to pass to the container\n            pod_resources: Dictionary of resources to request for the container\n            args: List of arguments to pass to the container\n            image_pull_policy: Image pull policy for the driver and executors, defaults to \"IfNotPresent\"\n            extra_labels: Dictionary of extra labels to add to the pod template\n            annotations: Dictionary of annotations to add to the pod template\n            env_from_secrets: List of secrets to load environment variables from\n            volumes: List of volumes to mount in the pod\n            volume_mounts: List of volume mounts to mount in the container\n            node_selector: Node selector to use for the pod\n            tolerations: List of tolerations to use for the pod\n\n        Returns:\n            Pod template spec for the Spark application\n        \"\"\"\n        pod_metadata = k8s.V1ObjectMeta(\n            name=f\"{app_id}-driver\",\n            namespace=namespace,\n            labels=SparkAppManager.spark_app_labels(\n                app_name=app_name,\n                app_id=app_id,\n                extra_labels=extra_labels,\n            ),\n            annotations=annotations,\n        )\n        pod_spec = k8s.V1PodSpec(\n            service_account_name=service_account,\n            restart_policy=\"Never\",\n            containers=[\n                SparkAppManager.create_driver_container(\n                    image=image,\n                    container_name=container_name,\n                    env_variables=env_variables,\n                    pod_resources=pod_resources,\n                    args=args,\n                    image_pull_policy=image_pull_policy,\n                    env_from_secrets=env_from_secrets,\n                    volume_mounts=volume_mounts,\n                )\n            ],\n            volumes=volumes,\n            node_selector=node_selector,\n            tolerations=tolerations,\n        )\n        template = k8s.V1PodTemplateSpec(\n            metadata=pod_metadata,\n            spec=pod_spec,\n        )\n        return template\n\n    @staticmethod\n    def create_driver_container(\n        *,\n        image: str,\n        container_name: str = \"driver\",\n        env_variables: dict[str, str] | None = None,\n        pod_resources: dict[str, dict[str, str]] | None = None,\n        args: list[str] | None = None,\n        image_pull_policy: Literal[\"Always\", \"Never\", \"IfNotPresent\"] = \"IfNotPresent\",\n        env_from_secrets: list[str] | None = None,\n        volume_mounts: list[k8s.V1VolumeMount] | None = None,\n    ) -&gt; k8s.V1Container:\n        \"\"\"Create a container spec for the Spark driver\n\n        Args:\n            image: Docker image to use for the Spark driver and executors\n            container_name: Name of the container, defaults to \"driver\"\n            env_variables: Dictionary of environment variables to pass to the container\n            pod_resources: Dictionary of resources to request for the container\n            args: List of arguments to pass to the container\n            image_pull_policy: Image pull policy for the driver and executors, defaults to \"IfNotPresent\"\n            env_from_secrets: List of secrets to load environment variables from\n            volume_mounts: List of volume mounts to mount in the container\n\n        Returns:\n            Container spec for the Spark driver\n        \"\"\"\n        return k8s.V1Container(\n            name=container_name,\n            image=image,\n            image_pull_policy=image_pull_policy,\n            env=[k8s.V1EnvVar(name=key, value=value) for key, value in (env_variables or {}).items()]\n            + [\n                k8s.V1EnvVar(\n                    name=\"SPARK_DRIVER_BIND_ADDRESS\",\n                    value_from=k8s.V1EnvVarSource(\n                        field_ref=k8s.V1ObjectFieldSelector(\n                            field_path=\"status.podIP\",\n                        )\n                    ),\n                ),\n            ],\n            resources=k8s.V1ResourceRequirements(\n                **(pod_resources or {}),\n            ),\n            args=args or [],\n            ports=[\n                k8s.V1ContainerPort(\n                    container_port=7077,\n                    name=\"driver-port\",\n                ),\n                k8s.V1ContainerPort(\n                    container_port=4040,\n                    name=\"ui-port\",\n                ),\n            ],\n            env_from=[\n                k8s.V1EnvFromSource(\n                    secret_ref=k8s.V1SecretEnvSource(\n                        name=secret_name,\n                    ),\n                )\n                for secret_name in (env_from_secrets or [])\n            ],\n            volume_mounts=volume_mounts,\n        )\n\n    @staticmethod\n    def spark_app_labels(\n        *,\n        app_name: str,\n        app_id: str,\n        extra_labels: dict[str, str] | None = None,\n    ) -&gt; dict[str, str]:\n        \"\"\"Create labels for a Spark application\n\n        Args:\n            app_name: Name of the Spark application\n            app_id: ID of the Spark application\n            extra_labels: Dictionary of extra labels to add to the labels\n\n        Returns:\n            Dictionary of labels for the Spark application resources\n        \"\"\"\n        return {\n            \"spark-app-name\": app_name,\n            \"spark-app-id\": app_id,\n            \"spark-role\": \"driver\",\n            **(extra_labels or {}),\n        }\n\n    @staticmethod\n    def create_headless_service_object(\n        *,\n        app_name: str,\n        app_id: str,\n        namespace: str = \"default\",\n        pod_owner_uid: str | None = None,\n        extra_labels: dict[str, str] | None = None,\n    ) -&gt; k8s.V1Service:\n        \"\"\"Create a headless service for a Spark application\n\n        Args:\n            app_name: Name of the Spark application\n            app_id: ID of the Spark application\n            namespace: Kubernetes namespace to use, defaults to \"default\"\n            pod_owner_uid: UID of the pod to use as owner reference for the service\n            extra_labels: Dictionary of extra labels to add to the service\n\n        Returns:\n            The created headless service for the Spark application\n        \"\"\"\n        labels = SparkAppManager.spark_app_labels(\n            app_name=app_name,\n            app_id=app_id,\n            extra_labels=extra_labels,\n        )\n        owner = (\n            [\n                k8s.V1OwnerReference(\n                    api_version=\"v1\",\n                    kind=\"Pod\",\n                    name=f\"{app_id}-driver\",\n                    uid=pod_owner_uid,\n                )\n            ]\n            if pod_owner_uid\n            else None\n        )\n        return k8s.V1Service(\n            metadata=k8s.V1ObjectMeta(\n                name=app_id,\n                labels=labels,\n                namespace=namespace,\n                owner_references=owner,\n            ),\n            spec=k8s.V1ServiceSpec(\n                selector=labels,\n                ports=[\n                    k8s.V1ServicePort(\n                        port=7077,\n                        name=\"driver-port\",\n                    ),\n                    k8s.V1ServicePort(\n                        port=4040,\n                        name=\"ui-port\",\n                    ),\n                ],\n                type=\"ClusterIP\",\n                cluster_ip=\"None\",\n            ),\n        )\n\n    @staticmethod\n    def create_secret_object(\n        *,\n        app_name: str,\n        app_id: str,\n        secrets_values: dict[str, str],\n        namespace: str = \"default\",\n    ) -&gt; k8s.V1Secret:\n        \"\"\"Create a secret for a Spark application to store secrets values\n\n        Args:\n            app_name: Name of the Spark application\n            app_id: ID of the Spark application\n            secrets_values: Dictionary of secrets values\n            namespace: Kubernetes namespace to use, defaults to \"default\"\n\n        Returns:\n            The created secret for the Spark application\n        \"\"\"\n        return k8s.V1Secret(\n            metadata=k8s.V1ObjectMeta(\n                name=app_id,\n                namespace=namespace,\n                labels=SparkAppManager.spark_app_labels(\n                    app_name=app_name,\n                    app_id=app_id,\n                ),\n            ),\n            string_data=secrets_values,\n        )\n</code></pre>"},{"location":"reference/spark_on_k8s/utils/app_manager/#spark_on_k8s.utils.app_manager.SparkAppManager.app_status","title":"<code>app_status(*, namespace, pod_name=None, app_id=None, client=None)</code>","text":"<p>Get app status.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>str</code> <p>Namespace.</p> required <code>pod_name</code> <code>str</code> <p>Pod name. Defaults to None.</p> <code>None</code> <code>app_id</code> <code>str</code> <p>App ID. Defaults to None.</p> <code>None</code> <code>client</code> <code>CoreV1Api</code> <p>Kubernetes client. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>SparkAppStatus</code> <code>SparkAppStatus</code> <p>App status.</p> Source code in <code>spark_on_k8s/utils/app_manager.py</code> <pre><code>def app_status(\n    self,\n    *,\n    namespace: str,\n    pod_name: str | None = None,\n    app_id: str | None = None,\n    client: k8s.CoreV1Api | None = None,\n) -&gt; SparkAppStatus:\n    \"\"\"Get app status.\n\n    Args:\n        namespace (str): Namespace.\n        pod_name (str): Pod name. Defaults to None.\n        app_id (str): App ID. Defaults to None.\n        client (k8s.CoreV1Api, optional): Kubernetes client. Defaults to None.\n\n    Returns:\n        SparkAppStatus: App status.\n    \"\"\"\n\n    def _app_status(_client: k8s.CoreV1Api) -&gt; SparkAppStatus:\n        if pod_name is None and app_id is None:\n            raise ValueError(\"Either pod_name or app_id must be specified\")\n        if pod_name is not None:\n            _pod = _client.read_namespaced_pod(\n                namespace=namespace,\n                name=pod_name,\n            )\n        else:\n            _pod = _client.list_namespaced_pod(\n                namespace=namespace,\n                label_selector=f\"spark-app-id={app_id}\",\n            ).items[0]\n        return get_app_status(_pod)\n\n    if client is None:\n        with self.k8s_client_manager.client() as client:\n            api = k8s.CoreV1Api(client)\n            return _app_status(api)\n    return _app_status(client)\n</code></pre>"},{"location":"reference/spark_on_k8s/utils/app_manager/#spark_on_k8s.utils.app_manager.SparkAppManager.create_driver_container","title":"<code>create_driver_container(*, image, container_name='driver', env_variables=None, pod_resources=None, args=None, image_pull_policy='IfNotPresent', env_from_secrets=None, volume_mounts=None)</code>  <code>staticmethod</code>","text":"<p>Create a container spec for the Spark driver</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>str</code> <p>Docker image to use for the Spark driver and executors</p> required <code>container_name</code> <code>str</code> <p>Name of the container, defaults to \"driver\"</p> <code>'driver'</code> <code>env_variables</code> <code>dict[str, str] | None</code> <p>Dictionary of environment variables to pass to the container</p> <code>None</code> <code>pod_resources</code> <code>dict[str, dict[str, str]] | None</code> <p>Dictionary of resources to request for the container</p> <code>None</code> <code>args</code> <code>list[str] | None</code> <p>List of arguments to pass to the container</p> <code>None</code> <code>image_pull_policy</code> <code>Literal['Always', 'Never', 'IfNotPresent']</code> <p>Image pull policy for the driver and executors, defaults to \"IfNotPresent\"</p> <code>'IfNotPresent'</code> <code>env_from_secrets</code> <code>list[str] | None</code> <p>List of secrets to load environment variables from</p> <code>None</code> <code>volume_mounts</code> <code>list[V1VolumeMount] | None</code> <p>List of volume mounts to mount in the container</p> <code>None</code> <p>Returns:</p> Type Description <code>V1Container</code> <p>Container spec for the Spark driver</p> Source code in <code>spark_on_k8s/utils/app_manager.py</code> <pre><code>@staticmethod\ndef create_driver_container(\n    *,\n    image: str,\n    container_name: str = \"driver\",\n    env_variables: dict[str, str] | None = None,\n    pod_resources: dict[str, dict[str, str]] | None = None,\n    args: list[str] | None = None,\n    image_pull_policy: Literal[\"Always\", \"Never\", \"IfNotPresent\"] = \"IfNotPresent\",\n    env_from_secrets: list[str] | None = None,\n    volume_mounts: list[k8s.V1VolumeMount] | None = None,\n) -&gt; k8s.V1Container:\n    \"\"\"Create a container spec for the Spark driver\n\n    Args:\n        image: Docker image to use for the Spark driver and executors\n        container_name: Name of the container, defaults to \"driver\"\n        env_variables: Dictionary of environment variables to pass to the container\n        pod_resources: Dictionary of resources to request for the container\n        args: List of arguments to pass to the container\n        image_pull_policy: Image pull policy for the driver and executors, defaults to \"IfNotPresent\"\n        env_from_secrets: List of secrets to load environment variables from\n        volume_mounts: List of volume mounts to mount in the container\n\n    Returns:\n        Container spec for the Spark driver\n    \"\"\"\n    return k8s.V1Container(\n        name=container_name,\n        image=image,\n        image_pull_policy=image_pull_policy,\n        env=[k8s.V1EnvVar(name=key, value=value) for key, value in (env_variables or {}).items()]\n        + [\n            k8s.V1EnvVar(\n                name=\"SPARK_DRIVER_BIND_ADDRESS\",\n                value_from=k8s.V1EnvVarSource(\n                    field_ref=k8s.V1ObjectFieldSelector(\n                        field_path=\"status.podIP\",\n                    )\n                ),\n            ),\n        ],\n        resources=k8s.V1ResourceRequirements(\n            **(pod_resources or {}),\n        ),\n        args=args or [],\n        ports=[\n            k8s.V1ContainerPort(\n                container_port=7077,\n                name=\"driver-port\",\n            ),\n            k8s.V1ContainerPort(\n                container_port=4040,\n                name=\"ui-port\",\n            ),\n        ],\n        env_from=[\n            k8s.V1EnvFromSource(\n                secret_ref=k8s.V1SecretEnvSource(\n                    name=secret_name,\n                ),\n            )\n            for secret_name in (env_from_secrets or [])\n        ],\n        volume_mounts=volume_mounts,\n    )\n</code></pre>"},{"location":"reference/spark_on_k8s/utils/app_manager/#spark_on_k8s.utils.app_manager.SparkAppManager.create_headless_service_object","title":"<code>create_headless_service_object(*, app_name, app_id, namespace='default', pod_owner_uid=None, extra_labels=None)</code>  <code>staticmethod</code>","text":"<p>Create a headless service for a Spark application</p> <p>Parameters:</p> Name Type Description Default <code>app_name</code> <code>str</code> <p>Name of the Spark application</p> required <code>app_id</code> <code>str</code> <p>ID of the Spark application</p> required <code>namespace</code> <code>str</code> <p>Kubernetes namespace to use, defaults to \"default\"</p> <code>'default'</code> <code>pod_owner_uid</code> <code>str | None</code> <p>UID of the pod to use as owner reference for the service</p> <code>None</code> <code>extra_labels</code> <code>dict[str, str] | None</code> <p>Dictionary of extra labels to add to the service</p> <code>None</code> <p>Returns:</p> Type Description <code>V1Service</code> <p>The created headless service for the Spark application</p> Source code in <code>spark_on_k8s/utils/app_manager.py</code> <pre><code>@staticmethod\ndef create_headless_service_object(\n    *,\n    app_name: str,\n    app_id: str,\n    namespace: str = \"default\",\n    pod_owner_uid: str | None = None,\n    extra_labels: dict[str, str] | None = None,\n) -&gt; k8s.V1Service:\n    \"\"\"Create a headless service for a Spark application\n\n    Args:\n        app_name: Name of the Spark application\n        app_id: ID of the Spark application\n        namespace: Kubernetes namespace to use, defaults to \"default\"\n        pod_owner_uid: UID of the pod to use as owner reference for the service\n        extra_labels: Dictionary of extra labels to add to the service\n\n    Returns:\n        The created headless service for the Spark application\n    \"\"\"\n    labels = SparkAppManager.spark_app_labels(\n        app_name=app_name,\n        app_id=app_id,\n        extra_labels=extra_labels,\n    )\n    owner = (\n        [\n            k8s.V1OwnerReference(\n                api_version=\"v1\",\n                kind=\"Pod\",\n                name=f\"{app_id}-driver\",\n                uid=pod_owner_uid,\n            )\n        ]\n        if pod_owner_uid\n        else None\n    )\n    return k8s.V1Service(\n        metadata=k8s.V1ObjectMeta(\n            name=app_id,\n            labels=labels,\n            namespace=namespace,\n            owner_references=owner,\n        ),\n        spec=k8s.V1ServiceSpec(\n            selector=labels,\n            ports=[\n                k8s.V1ServicePort(\n                    port=7077,\n                    name=\"driver-port\",\n                ),\n                k8s.V1ServicePort(\n                    port=4040,\n                    name=\"ui-port\",\n                ),\n            ],\n            type=\"ClusterIP\",\n            cluster_ip=\"None\",\n        ),\n    )\n</code></pre>"},{"location":"reference/spark_on_k8s/utils/app_manager/#spark_on_k8s.utils.app_manager.SparkAppManager.create_secret_object","title":"<code>create_secret_object(*, app_name, app_id, secrets_values, namespace='default')</code>  <code>staticmethod</code>","text":"<p>Create a secret for a Spark application to store secrets values</p> <p>Parameters:</p> Name Type Description Default <code>app_name</code> <code>str</code> <p>Name of the Spark application</p> required <code>app_id</code> <code>str</code> <p>ID of the Spark application</p> required <code>secrets_values</code> <code>dict[str, str]</code> <p>Dictionary of secrets values</p> required <code>namespace</code> <code>str</code> <p>Kubernetes namespace to use, defaults to \"default\"</p> <code>'default'</code> <p>Returns:</p> Type Description <code>V1Secret</code> <p>The created secret for the Spark application</p> Source code in <code>spark_on_k8s/utils/app_manager.py</code> <pre><code>@staticmethod\ndef create_secret_object(\n    *,\n    app_name: str,\n    app_id: str,\n    secrets_values: dict[str, str],\n    namespace: str = \"default\",\n) -&gt; k8s.V1Secret:\n    \"\"\"Create a secret for a Spark application to store secrets values\n\n    Args:\n        app_name: Name of the Spark application\n        app_id: ID of the Spark application\n        secrets_values: Dictionary of secrets values\n        namespace: Kubernetes namespace to use, defaults to \"default\"\n\n    Returns:\n        The created secret for the Spark application\n    \"\"\"\n    return k8s.V1Secret(\n        metadata=k8s.V1ObjectMeta(\n            name=app_id,\n            namespace=namespace,\n            labels=SparkAppManager.spark_app_labels(\n                app_name=app_name,\n                app_id=app_id,\n            ),\n        ),\n        string_data=secrets_values,\n    )\n</code></pre>"},{"location":"reference/spark_on_k8s/utils/app_manager/#spark_on_k8s.utils.app_manager.SparkAppManager.create_spark_pod_spec","title":"<code>create_spark_pod_spec(*, app_name, app_id, image, namespace='default', service_account='spark', container_name='driver', env_variables=None, pod_resources=None, args=None, image_pull_policy='IfNotPresent', extra_labels=None, annotations=None, env_from_secrets=None, volumes=None, volume_mounts=None, node_selector=None, tolerations=None)</code>  <code>staticmethod</code>","text":"<p>Create a pod spec for a Spark application</p> <p>Parameters:</p> Name Type Description Default <code>app_name</code> <code>str</code> <p>Name of the Spark application</p> required <code>app_id</code> <code>str</code> <p>ID of the Spark application</p> required <code>image</code> <code>str</code> <p>Docker image to use for the Spark driver and executors</p> required <code>namespace</code> <code>str</code> <p>Kubernetes namespace to use, defaults to \"default\"</p> <code>'default'</code> <code>service_account</code> <code>str</code> <p>Kubernetes service account to use for the Spark driver, defaults to \"spark\"</p> <code>'spark'</code> <code>container_name</code> <code>str</code> <p>Name of the container, defaults to \"driver\"</p> <code>'driver'</code> <code>env_variables</code> <code>dict[str, str] | None</code> <p>Dictionary of environment variables to pass to the container</p> <code>None</code> <code>pod_resources</code> <code>dict[str, dict[str, str]] | None</code> <p>Dictionary of resources to request for the container</p> <code>None</code> <code>args</code> <code>list[str] | None</code> <p>List of arguments to pass to the container</p> <code>None</code> <code>image_pull_policy</code> <code>Literal['Always', 'Never', 'IfNotPresent']</code> <p>Image pull policy for the driver and executors, defaults to \"IfNotPresent\"</p> <code>'IfNotPresent'</code> <code>extra_labels</code> <code>dict[str, str] | None</code> <p>Dictionary of extra labels to add to the pod template</p> <code>None</code> <code>annotations</code> <code>dict[str, str] | None</code> <p>Dictionary of annotations to add to the pod template</p> <code>None</code> <code>env_from_secrets</code> <code>list[str] | None</code> <p>List of secrets to load environment variables from</p> <code>None</code> <code>volumes</code> <code>list[V1Volume] | None</code> <p>List of volumes to mount in the pod</p> <code>None</code> <code>volume_mounts</code> <code>list[V1VolumeMount] | None</code> <p>List of volume mounts to mount in the container</p> <code>None</code> <code>node_selector</code> <code>dict[str, str] | None</code> <p>Node selector to use for the pod</p> <code>None</code> <code>tolerations</code> <code>list[V1Toleration] | None</code> <p>List of tolerations to use for the pod</p> <code>None</code> <p>Returns:</p> Type Description <code>V1PodTemplateSpec</code> <p>Pod template spec for the Spark application</p> Source code in <code>spark_on_k8s/utils/app_manager.py</code> <pre><code>@staticmethod\ndef create_spark_pod_spec(\n    *,\n    app_name: str,\n    app_id: str,\n    image: str,\n    namespace: str = \"default\",\n    service_account: str = \"spark\",\n    container_name: str = \"driver\",\n    env_variables: dict[str, str] | None = None,\n    pod_resources: dict[str, dict[str, str]] | None = None,\n    args: list[str] | None = None,\n    image_pull_policy: Literal[\"Always\", \"Never\", \"IfNotPresent\"] = \"IfNotPresent\",\n    extra_labels: dict[str, str] | None = None,\n    annotations: dict[str, str] | None = None,\n    env_from_secrets: list[str] | None = None,\n    volumes: list[k8s.V1Volume] | None = None,\n    volume_mounts: list[k8s.V1VolumeMount] | None = None,\n    node_selector: dict[str, str] | None = None,\n    tolerations: list[k8s.V1Toleration] | None = None,\n) -&gt; k8s.V1PodTemplateSpec:\n    \"\"\"Create a pod spec for a Spark application\n\n    Args:\n        app_name: Name of the Spark application\n        app_id: ID of the Spark application\n        image: Docker image to use for the Spark driver and executors\n        namespace: Kubernetes namespace to use, defaults to \"default\"\n        service_account: Kubernetes service account to use for the Spark driver, defaults to \"spark\"\n        container_name: Name of the container, defaults to \"driver\"\n        env_variables: Dictionary of environment variables to pass to the container\n        pod_resources: Dictionary of resources to request for the container\n        args: List of arguments to pass to the container\n        image_pull_policy: Image pull policy for the driver and executors, defaults to \"IfNotPresent\"\n        extra_labels: Dictionary of extra labels to add to the pod template\n        annotations: Dictionary of annotations to add to the pod template\n        env_from_secrets: List of secrets to load environment variables from\n        volumes: List of volumes to mount in the pod\n        volume_mounts: List of volume mounts to mount in the container\n        node_selector: Node selector to use for the pod\n        tolerations: List of tolerations to use for the pod\n\n    Returns:\n        Pod template spec for the Spark application\n    \"\"\"\n    pod_metadata = k8s.V1ObjectMeta(\n        name=f\"{app_id}-driver\",\n        namespace=namespace,\n        labels=SparkAppManager.spark_app_labels(\n            app_name=app_name,\n            app_id=app_id,\n            extra_labels=extra_labels,\n        ),\n        annotations=annotations,\n    )\n    pod_spec = k8s.V1PodSpec(\n        service_account_name=service_account,\n        restart_policy=\"Never\",\n        containers=[\n            SparkAppManager.create_driver_container(\n                image=image,\n                container_name=container_name,\n                env_variables=env_variables,\n                pod_resources=pod_resources,\n                args=args,\n                image_pull_policy=image_pull_policy,\n                env_from_secrets=env_from_secrets,\n                volume_mounts=volume_mounts,\n            )\n        ],\n        volumes=volumes,\n        node_selector=node_selector,\n        tolerations=tolerations,\n    )\n    template = k8s.V1PodTemplateSpec(\n        metadata=pod_metadata,\n        spec=pod_spec,\n    )\n    return template\n</code></pre>"},{"location":"reference/spark_on_k8s/utils/app_manager/#spark_on_k8s.utils.app_manager.SparkAppManager.delete_app","title":"<code>delete_app(namespace, pod_name=None, app_id=None, force=False)</code>","text":"<p>Delete an app.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>str</code> <p>Namespace.</p> required <code>pod_name</code> <code>str</code> <p>Pod name.</p> <code>None</code> <code>app_id</code> <code>str</code> <p>App ID.</p> <code>None</code> <code>force</code> <code>bool</code> <p>Whether to force delete the app. Defaults to False.</p> <code>False</code> Source code in <code>spark_on_k8s/utils/app_manager.py</code> <pre><code>def delete_app(\n    self, namespace: str, pod_name: str | None = None, app_id: str | None = None, force: bool = False\n):\n    \"\"\"Delete an app.\n\n    Args:\n        namespace (str): Namespace.\n        pod_name (str): Pod name.\n        app_id (str): App ID.\n        force (bool, optional): Whether to force delete the app. Defaults to False.\n    \"\"\"\n    if pod_name is None and app_id is None:\n        raise ValueError(\"Either pod_name or app_id must be specified\")\n    with self.k8s_client_manager.client() as client:\n        api = k8s.CoreV1Api(client)\n        if app_id:\n            # we don't use `delete_collection_namespaced_pod` to know if the app exists or not\n            pods = api.list_namespaced_pod(\n                namespace=namespace,\n                label_selector=f\"spark-app-id={app_id}\",\n            ).items\n            if len(pods) == 0:\n                raise ValueError(f\"No pods found for app {app_id}\")\n            pod_name = pods[0].metadata.name\n        api.delete_namespaced_pod(\n            name=pod_name,\n            namespace=namespace,\n            body=k8s.V1DeleteOptions(\n                grace_period_seconds=0 if force else None,\n                propagation_policy=\"Foreground\",\n            ),\n        )\n</code></pre>"},{"location":"reference/spark_on_k8s/utils/app_manager/#spark_on_k8s.utils.app_manager.SparkAppManager.kill_app","title":"<code>kill_app(namespace, pod_name=None, app_id=None, should_print=False)</code>","text":"<p>Kill an app.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>str</code> <p>Namespace.</p> required <code>pod_name</code> <code>str</code> <p>Pod name.</p> <code>None</code> <code>app_id</code> <code>str</code> <p>App ID.</p> <code>None</code> <code>should_print</code> <code>bool</code> <p>Whether to print logs instead of logging them.</p> <code>False</code> Source code in <code>spark_on_k8s/utils/app_manager.py</code> <pre><code>def kill_app(\n    self,\n    namespace: str,\n    pod_name: str | None = None,\n    app_id: str | None = None,\n    should_print: bool = False,\n):\n    \"\"\"Kill an app.\n\n    Args:\n        namespace (str): Namespace.\n        pod_name (str): Pod name.\n        app_id (str): App ID.\n        should_print (bool, optional): Whether to print logs instead of logging them.\n    \"\"\"\n    if pod_name is None and app_id is None:\n        raise ValueError(\"Either pod_name or app_id must be specified\")\n    with self.k8s_client_manager.client() as client:\n        api = k8s.CoreV1Api(client)\n        if pod_name is None:\n            pods = api.list_namespaced_pod(\n                namespace=namespace,\n                label_selector=f\"spark-app-id={app_id}\",\n            ).items\n            if len(pods) == 0:\n                raise ValueError(f\"No pods found for app {app_id}\")\n            pod = pods[0]\n        else:\n            pod = api.read_namespaced_pod(\n                namespace=namespace,\n                name=pod_name,\n            )\n        container_name = pod.spec.containers[0].name\n        if pod.status.phase != \"Running\":\n            self.log(\n                f\"App is not running, it is {get_app_status(pod).value}\",\n                level=logging.INFO,\n                should_print=should_print,\n            )\n            return\n        stream(\n            api.connect_get_namespaced_pod_exec,\n            pod.metadata.name,\n            namespace,\n            command=[\"/bin/sh\", \"-c\", \"kill 1\"],\n            container=container_name,\n            stderr=True,\n            stdin=False,\n            stdout=True,\n            tty=False,\n            _preload_content=False,\n        )\n</code></pre>"},{"location":"reference/spark_on_k8s/utils/app_manager/#spark_on_k8s.utils.app_manager.SparkAppManager.list_apps","title":"<code>list_apps(namespace)</code>","text":"<p>List apps.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>str</code> <p>Namespace.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: Spark apps in the namespace.</p> Source code in <code>spark_on_k8s/utils/app_manager.py</code> <pre><code>def list_apps(self, namespace: str) -&gt; list[str]:\n    \"\"\"List apps.\n\n    Args:\n        namespace (str): Namespace.\n\n    Returns:\n        list[str]: Spark apps in the namespace.\n    \"\"\"\n    with self.k8s_client_manager.client() as client:\n        api = k8s.CoreV1Api(client)\n        return [\n            pod.metadata.labels[\"spark-app-id\"]\n            for pod in api.list_namespaced_pod(\n                namespace=namespace,\n                label_selector=\"spark-role=driver\",\n            ).items\n        ]\n</code></pre>"},{"location":"reference/spark_on_k8s/utils/app_manager/#spark_on_k8s.utils.app_manager.SparkAppManager.spark_app_labels","title":"<code>spark_app_labels(*, app_name, app_id, extra_labels=None)</code>  <code>staticmethod</code>","text":"<p>Create labels for a Spark application</p> <p>Parameters:</p> Name Type Description Default <code>app_name</code> <code>str</code> <p>Name of the Spark application</p> required <code>app_id</code> <code>str</code> <p>ID of the Spark application</p> required <code>extra_labels</code> <code>dict[str, str] | None</code> <p>Dictionary of extra labels to add to the labels</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>Dictionary of labels for the Spark application resources</p> Source code in <code>spark_on_k8s/utils/app_manager.py</code> <pre><code>@staticmethod\ndef spark_app_labels(\n    *,\n    app_name: str,\n    app_id: str,\n    extra_labels: dict[str, str] | None = None,\n) -&gt; dict[str, str]:\n    \"\"\"Create labels for a Spark application\n\n    Args:\n        app_name: Name of the Spark application\n        app_id: ID of the Spark application\n        extra_labels: Dictionary of extra labels to add to the labels\n\n    Returns:\n        Dictionary of labels for the Spark application resources\n    \"\"\"\n    return {\n        \"spark-app-name\": app_name,\n        \"spark-app-id\": app_id,\n        \"spark-role\": \"driver\",\n        **(extra_labels or {}),\n    }\n</code></pre>"},{"location":"reference/spark_on_k8s/utils/app_manager/#spark_on_k8s.utils.app_manager.SparkAppManager.stream_logs","title":"<code>stream_logs(*, namespace, pod_name=None, app_id=None, should_print=False)</code>","text":"<p>Stream logs from a Spark app.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>str</code> <p>Namespace.</p> required <code>pod_name</code> <code>str</code> <p>Pod name.</p> <code>None</code> <code>app_id</code> <code>str</code> <p>App ID.</p> <code>None</code> <code>should_print</code> <code>bool</code> <p>Whether to print logs instead of logging them.</p> <code>False</code> Source code in <code>spark_on_k8s/utils/app_manager.py</code> <pre><code>def stream_logs(\n    self,\n    *,\n    namespace: str,\n    pod_name: str | None = None,\n    app_id: str | None = None,\n    should_print: bool = False,\n):\n    \"\"\"Stream logs from a Spark app.\n\n    Args:\n        namespace (str): Namespace.\n        pod_name (str): Pod name.\n        app_id (str): App ID.\n        should_print (bool, optional): Whether to print logs instead of logging them.\n    \"\"\"\n    if pod_name is None and app_id is None:\n        raise ValueError(\"Either pod_name or app_id must be specified\")\n    if pod_name is None:\n        with self.k8s_client_manager.client() as client:\n            api = k8s.CoreV1Api(client)\n            pods = api.list_namespaced_pod(\n                namespace=namespace,\n                label_selector=f\"spark-app-id={app_id}\",\n            ).items\n            if len(pods) == 0:\n                raise ValueError(f\"No pods found for app {app_id}\")\n            pod_name = pods[0].metadata.name\n    with self.k8s_client_manager.client() as client:\n        api = k8s.CoreV1Api(client)\n        while True:\n            pod = api.read_namespaced_pod(\n                namespace=namespace,\n                name=pod_name,\n            )\n            if pod.status.phase != \"Pending\":\n                break\n        watcher = watch.Watch()\n        for line in watcher.stream(\n            api.read_namespaced_pod_log,\n            namespace=namespace,\n            name=pod_name,\n        ):\n            self.log(msg=line, level=logging.INFO, should_print=should_print)\n        watcher.stop()\n</code></pre>"},{"location":"reference/spark_on_k8s/utils/app_manager/#spark_on_k8s.utils.app_manager.SparkAppManager.wait_for_app","title":"<code>wait_for_app(*, namespace, pod_name=None, app_id=None, poll_interval=10, should_print=False)</code>","text":"<p>Wait for a Spark app to finish.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>str</code> <p>Namespace.</p> required <code>pod_name</code> <code>str</code> <p>Pod name.</p> <code>None</code> <code>app_id</code> <code>str</code> <p>App ID.</p> <code>None</code> <code>poll_interval</code> <code>float</code> <p>Poll interval in seconds. Defaults to 10.</p> <code>10</code> <code>should_print</code> <code>bool</code> <p>Whether to print logs instead of logging them.</p> <code>False</code> Source code in <code>spark_on_k8s/utils/app_manager.py</code> <pre><code>def wait_for_app(\n    self,\n    *,\n    namespace: str,\n    pod_name: str | None = None,\n    app_id: str | None = None,\n    poll_interval: float = 10,\n    should_print: bool = False,\n):\n    \"\"\"Wait for a Spark app to finish.\n\n    Args:\n        namespace (str): Namespace.\n        pod_name (str): Pod name.\n        app_id (str): App ID.\n        poll_interval (float, optional): Poll interval in seconds. Defaults to 10.\n        should_print (bool, optional): Whether to print logs instead of logging them.\n    \"\"\"\n    termination_statuses = {SparkAppStatus.Succeeded, SparkAppStatus.Failed, SparkAppStatus.Unknown}\n    with self.k8s_client_manager.client() as client:\n        api = k8s.CoreV1Api(client)\n        while True:\n            try:\n                status = self.app_status(\n                    namespace=namespace, pod_name=pod_name, app_id=app_id, client=api\n                )\n                if status in termination_statuses:\n                    break\n            except ApiException as e:\n                if e.status == 404:\n                    self.log(\n                        msg=f\"Pod {pod_name} was deleted\", level=logging.INFO, should_print=should_print\n                    )\n                    return\n            self.log(\n                msg=f\"Pod {pod_name} status is {status}, sleep {poll_interval}s\",\n                level=logging.INFO,\n                should_print=should_print,\n            )\n            time.sleep(poll_interval)\n        self.log(\n            msg=f\"Pod {pod_name} finished with status {status.value}\",\n            level=logging.INFO,\n            should_print=should_print,\n        )\n</code></pre>"},{"location":"reference/spark_on_k8s/utils/async_app_manager/","title":"async_app_manager","text":""},{"location":"reference/spark_on_k8s/utils/async_app_manager/#spark_on_k8s.utils.async_app_manager.AsyncSparkAppManager","title":"<code>AsyncSparkAppManager</code>","text":"<p>               Bases: <code>LoggingMixin</code></p> <p>Manage Spark apps on Kubernetes asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>k8s_client_manager</code> <code>KubernetesClientManager</code> <p>Kubernetes client manager. Defaults to None.</p> <code>None</code> <code>logger_name</code> <code>str</code> <p>logger name. Defaults to \"SparkAppManager\".</p> <code>None</code> Source code in <code>spark_on_k8s/utils/async_app_manager.py</code> <pre><code>class AsyncSparkAppManager(LoggingMixin):\n    \"\"\"Manage Spark apps on Kubernetes asynchronously.\n\n    Args:\n        k8s_client_manager (KubernetesClientManager, optional): Kubernetes client manager. Defaults to None.\n        logger_name (str, optional): logger name. Defaults to \"SparkAppManager\".\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        k8s_client_manager: KubernetesAsyncClientManager | None = None,\n        logger_name: str | None = None,\n    ):\n        super().__init__(logger_name=logger_name or \"SparkAppManager\")\n        self.k8s_client_manager = k8s_client_manager or KubernetesAsyncClientManager()\n\n    async def app_status(\n        self,\n        *,\n        namespace: str,\n        pod_name: str | None = None,\n        app_id: str | None = None,\n        client: k8s_async.CoreV1Api | None = None,\n    ) -&gt; SparkAppStatus:\n        \"\"\"Get app status asynchronously.\n\n        Args:\n            namespace (str): Namespace.\n            pod_name (str): Pod name. Defaults to None.\n            app_id (str): App ID. Defaults to None.\n            client (k8s.CoreV1Api, optional): Kubernetes client. Defaults to None.\n\n        Returns:\n            SparkAppStatus: App status.\n        \"\"\"\n\n        async def _app_status(_client: k8s_async.CoreV1Api) -&gt; SparkAppStatus:\n            if pod_name is None and app_id is None:\n                raise ValueError(\"Either pod_name or app_id must be specified\")\n            if pod_name is not None:\n                _pod = await _client.read_namespaced_pod(\n                    namespace=namespace,\n                    name=pod_name,\n                )\n            else:\n                _pod = (\n                    await _client.list_namespaced_pod(\n                        namespace=namespace,\n                        label_selector=f\"spark-app-id={app_id}\",\n                    )\n                ).items[0]\n            return get_app_status(_pod)\n\n        if client is None:\n            async with self.k8s_client_manager.client() as client:\n                api = k8s_async.CoreV1Api(client)\n                return await _app_status(api)\n        return await _app_status(client)\n\n    async def wait_for_app(\n        self,\n        *,\n        namespace: str,\n        pod_name: str | None = None,\n        app_id: str | None = None,\n        poll_interval: float = 10,\n        should_print: bool = False,\n    ):\n        \"\"\"Wait for a Spark app to finish asynchronously.\n\n        Args:\n            namespace (str): Namespace.\n            pod_name (str): Pod name.\n            app_id (str): App ID.\n            poll_interval (float, optional): Poll interval in seconds. Defaults to 10.\n            should_print (bool, optional): Whether to print logs instead of logging them.\n        \"\"\"\n        termination_statuses = {SparkAppStatus.Succeeded, SparkAppStatus.Failed, SparkAppStatus.Unknown}\n        async with self.k8s_client_manager.client() as client:\n            api = k8s_async.CoreV1Api(client)\n            while True:\n                try:\n                    status = await self.app_status(\n                        namespace=namespace, pod_name=pod_name, app_id=app_id, client=api\n                    )\n                    if status in termination_statuses:\n                        break\n                except ApiException as e:\n                    if e.status == 404:\n                        self.log(\n                            msg=f\"Pod {pod_name} was deleted\", level=logging.INFO, should_print=should_print\n                        )\n                        return\n                self.log(\n                    msg=f\"Pod {pod_name} status is {status}, sleep {poll_interval}s\",\n                    level=logging.INFO,\n                    should_print=should_print,\n                )\n                await asyncio.sleep(poll_interval)\n            self.log(\n                msg=f\"Pod {pod_name} finished with status {status.value}\",\n                level=logging.INFO,\n                should_print=should_print,\n            )\n\n    async def logs_streamer(\n        self,\n        *,\n        namespace: str,\n        pod_name: str | None = None,\n        app_id: str | None = None,\n        tail_lines: int = -1,\n    ):\n        \"\"\"Stream logs from a Spark app asynchronously.\n\n        Args:\n            namespace (str): Namespace.\n            pod_name (str): Pod name.\n            app_id (str): App ID.\n            tail_lines (int, optional): Number of lines to tail. Defaults to -1.\n        \"\"\"\n        if pod_name is None and app_id is None:\n            raise ValueError(\"Either pod_name or app_id must be specified\")\n        if pod_name is None:\n            async with self.k8s_client_manager.client() as client:\n                api = k8s_async.CoreV1Api(client)\n                pods = (\n                    await api.list_namespaced_pod(\n                        namespace=namespace,\n                        label_selector=f\"spark-app-id={app_id}\",\n                    )\n                ).items\n                if len(pods) == 0:\n                    raise ValueError(f\"No pods found for app {app_id}\")\n                pod_name = pods[0].metadata.name\n\n        async with self.k8s_client_manager.client() as client:\n            api = k8s_async.CoreV1Api(client)\n            while True:\n                pod = await api.read_namespaced_pod(\n                    namespace=namespace,\n                    name=pod_name,\n                )\n                if pod.status.phase != \"Pending\":\n                    break\n\n            watcher = watch.Watch()\n            log_streamer = watcher.stream(\n                api.read_namespaced_pod_log,\n                namespace=namespace,\n                name=pod_name,\n                tail_lines=tail_lines if tail_lines &gt; 0 else None,\n                follow=True,\n            )\n            async for line in log_streamer:\n                yield line\n            watcher.stop()\n\n    async def kill_app(\n        self,\n        namespace: str,\n        pod_name: str | None = None,\n        app_id: str | None = None,\n    ):\n        \"\"\"Kill an app asynchronously.\n\n        Args:\n            namespace (str): Namespace.\n            pod_name (str): Pod name.\n            app_id (str): App ID.\n        \"\"\"\n        if pod_name is None and app_id is None:\n            raise ValueError(\"Either pod_name or app_id must be specified\")\n        async with self.k8s_client_manager.client() as client:\n            api = k8s_async.CoreV1Api(client)\n            if pod_name is None:\n                pods = (\n                    await api.list_namespaced_pod(\n                        namespace=namespace,\n                        label_selector=f\"spark-app-id={app_id}\",\n                    )\n                ).items\n                if len(pods) == 0:\n                    raise ValueError(f\"No pods found for app {app_id}\")\n                pod = pods[0]\n            else:\n                pod = await api.read_namespaced_pod(\n                    namespace=namespace,\n                    name=pod_name,\n                )\n            container_name = pod.spec.containers[0].name\n            if pod.status.phase != \"Running\":\n                raise ValueError(f\"Pod {pod.metadata.name} is not running\")\n            v1_ws = k8s_async.CoreV1Api(api_client=WsApiClient())\n            await stream(\n                v1_ws.connect_get_namespaced_pod_exec,\n                pod.metadata.name,\n                namespace,\n                command=[\"/bin/sh\", \"-c\", \"kill 1\"],\n                container=container_name,\n                stderr=True,\n                stdin=False,\n                stdout=True,\n                tty=False,\n                _preload_content=True,\n            )\n            await v1_ws.api_client.close()\n\n    async def delete_app(\n        self, namespace: str, pod_name: str | None = None, app_id: str | None = None, force: bool = False\n    ):\n        \"\"\"Delete an app asynchronously.\n\n        Args:\n            namespace (str): Namespace.\n            pod_name (str): Pod name.\n            app_id (str): App ID.\n            force (bool, optional): Whether to force delete the app. Defaults to False.\n        \"\"\"\n        if pod_name is None and app_id is None:\n            raise ValueError(\"Either pod_name or app_id must be specified\")\n        async with self.k8s_client_manager.client() as client:\n            api = k8s_async.CoreV1Api(client)\n            if app_id:\n                # we don't use `delete_collection_namespaced_pod` to know if the app exists or not\n                pods = (\n                    await api.list_namespaced_pod(\n                        namespace=namespace,\n                        label_selector=f\"spark-app-id={app_id}\",\n                    )\n                ).items\n                if len(pods) == 0:\n                    raise ValueError(f\"No pods found for app {app_id}\")\n                pod_name = pods[0].metadata.name\n            await api.delete_namespaced_pod(\n                name=pod_name,\n                namespace=namespace,\n                body=k8s_async.V1DeleteOptions(\n                    grace_period_seconds=0 if force else None,\n                    propagation_policy=\"Foreground\",\n                ),\n            )\n</code></pre>"},{"location":"reference/spark_on_k8s/utils/async_app_manager/#spark_on_k8s.utils.async_app_manager.AsyncSparkAppManager.app_status","title":"<code>app_status(*, namespace, pod_name=None, app_id=None, client=None)</code>  <code>async</code>","text":"<p>Get app status asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>str</code> <p>Namespace.</p> required <code>pod_name</code> <code>str</code> <p>Pod name. Defaults to None.</p> <code>None</code> <code>app_id</code> <code>str</code> <p>App ID. Defaults to None.</p> <code>None</code> <code>client</code> <code>CoreV1Api</code> <p>Kubernetes client. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>SparkAppStatus</code> <code>SparkAppStatus</code> <p>App status.</p> Source code in <code>spark_on_k8s/utils/async_app_manager.py</code> <pre><code>async def app_status(\n    self,\n    *,\n    namespace: str,\n    pod_name: str | None = None,\n    app_id: str | None = None,\n    client: k8s_async.CoreV1Api | None = None,\n) -&gt; SparkAppStatus:\n    \"\"\"Get app status asynchronously.\n\n    Args:\n        namespace (str): Namespace.\n        pod_name (str): Pod name. Defaults to None.\n        app_id (str): App ID. Defaults to None.\n        client (k8s.CoreV1Api, optional): Kubernetes client. Defaults to None.\n\n    Returns:\n        SparkAppStatus: App status.\n    \"\"\"\n\n    async def _app_status(_client: k8s_async.CoreV1Api) -&gt; SparkAppStatus:\n        if pod_name is None and app_id is None:\n            raise ValueError(\"Either pod_name or app_id must be specified\")\n        if pod_name is not None:\n            _pod = await _client.read_namespaced_pod(\n                namespace=namespace,\n                name=pod_name,\n            )\n        else:\n            _pod = (\n                await _client.list_namespaced_pod(\n                    namespace=namespace,\n                    label_selector=f\"spark-app-id={app_id}\",\n                )\n            ).items[0]\n        return get_app_status(_pod)\n\n    if client is None:\n        async with self.k8s_client_manager.client() as client:\n            api = k8s_async.CoreV1Api(client)\n            return await _app_status(api)\n    return await _app_status(client)\n</code></pre>"},{"location":"reference/spark_on_k8s/utils/async_app_manager/#spark_on_k8s.utils.async_app_manager.AsyncSparkAppManager.delete_app","title":"<code>delete_app(namespace, pod_name=None, app_id=None, force=False)</code>  <code>async</code>","text":"<p>Delete an app asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>str</code> <p>Namespace.</p> required <code>pod_name</code> <code>str</code> <p>Pod name.</p> <code>None</code> <code>app_id</code> <code>str</code> <p>App ID.</p> <code>None</code> <code>force</code> <code>bool</code> <p>Whether to force delete the app. Defaults to False.</p> <code>False</code> Source code in <code>spark_on_k8s/utils/async_app_manager.py</code> <pre><code>async def delete_app(\n    self, namespace: str, pod_name: str | None = None, app_id: str | None = None, force: bool = False\n):\n    \"\"\"Delete an app asynchronously.\n\n    Args:\n        namespace (str): Namespace.\n        pod_name (str): Pod name.\n        app_id (str): App ID.\n        force (bool, optional): Whether to force delete the app. Defaults to False.\n    \"\"\"\n    if pod_name is None and app_id is None:\n        raise ValueError(\"Either pod_name or app_id must be specified\")\n    async with self.k8s_client_manager.client() as client:\n        api = k8s_async.CoreV1Api(client)\n        if app_id:\n            # we don't use `delete_collection_namespaced_pod` to know if the app exists or not\n            pods = (\n                await api.list_namespaced_pod(\n                    namespace=namespace,\n                    label_selector=f\"spark-app-id={app_id}\",\n                )\n            ).items\n            if len(pods) == 0:\n                raise ValueError(f\"No pods found for app {app_id}\")\n            pod_name = pods[0].metadata.name\n        await api.delete_namespaced_pod(\n            name=pod_name,\n            namespace=namespace,\n            body=k8s_async.V1DeleteOptions(\n                grace_period_seconds=0 if force else None,\n                propagation_policy=\"Foreground\",\n            ),\n        )\n</code></pre>"},{"location":"reference/spark_on_k8s/utils/async_app_manager/#spark_on_k8s.utils.async_app_manager.AsyncSparkAppManager.kill_app","title":"<code>kill_app(namespace, pod_name=None, app_id=None)</code>  <code>async</code>","text":"<p>Kill an app asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>str</code> <p>Namespace.</p> required <code>pod_name</code> <code>str</code> <p>Pod name.</p> <code>None</code> <code>app_id</code> <code>str</code> <p>App ID.</p> <code>None</code> Source code in <code>spark_on_k8s/utils/async_app_manager.py</code> <pre><code>async def kill_app(\n    self,\n    namespace: str,\n    pod_name: str | None = None,\n    app_id: str | None = None,\n):\n    \"\"\"Kill an app asynchronously.\n\n    Args:\n        namespace (str): Namespace.\n        pod_name (str): Pod name.\n        app_id (str): App ID.\n    \"\"\"\n    if pod_name is None and app_id is None:\n        raise ValueError(\"Either pod_name or app_id must be specified\")\n    async with self.k8s_client_manager.client() as client:\n        api = k8s_async.CoreV1Api(client)\n        if pod_name is None:\n            pods = (\n                await api.list_namespaced_pod(\n                    namespace=namespace,\n                    label_selector=f\"spark-app-id={app_id}\",\n                )\n            ).items\n            if len(pods) == 0:\n                raise ValueError(f\"No pods found for app {app_id}\")\n            pod = pods[0]\n        else:\n            pod = await api.read_namespaced_pod(\n                namespace=namespace,\n                name=pod_name,\n            )\n        container_name = pod.spec.containers[0].name\n        if pod.status.phase != \"Running\":\n            raise ValueError(f\"Pod {pod.metadata.name} is not running\")\n        v1_ws = k8s_async.CoreV1Api(api_client=WsApiClient())\n        await stream(\n            v1_ws.connect_get_namespaced_pod_exec,\n            pod.metadata.name,\n            namespace,\n            command=[\"/bin/sh\", \"-c\", \"kill 1\"],\n            container=container_name,\n            stderr=True,\n            stdin=False,\n            stdout=True,\n            tty=False,\n            _preload_content=True,\n        )\n        await v1_ws.api_client.close()\n</code></pre>"},{"location":"reference/spark_on_k8s/utils/async_app_manager/#spark_on_k8s.utils.async_app_manager.AsyncSparkAppManager.logs_streamer","title":"<code>logs_streamer(*, namespace, pod_name=None, app_id=None, tail_lines=-1)</code>  <code>async</code>","text":"<p>Stream logs from a Spark app asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>str</code> <p>Namespace.</p> required <code>pod_name</code> <code>str</code> <p>Pod name.</p> <code>None</code> <code>app_id</code> <code>str</code> <p>App ID.</p> <code>None</code> <code>tail_lines</code> <code>int</code> <p>Number of lines to tail. Defaults to -1.</p> <code>-1</code> Source code in <code>spark_on_k8s/utils/async_app_manager.py</code> <pre><code>async def logs_streamer(\n    self,\n    *,\n    namespace: str,\n    pod_name: str | None = None,\n    app_id: str | None = None,\n    tail_lines: int = -1,\n):\n    \"\"\"Stream logs from a Spark app asynchronously.\n\n    Args:\n        namespace (str): Namespace.\n        pod_name (str): Pod name.\n        app_id (str): App ID.\n        tail_lines (int, optional): Number of lines to tail. Defaults to -1.\n    \"\"\"\n    if pod_name is None and app_id is None:\n        raise ValueError(\"Either pod_name or app_id must be specified\")\n    if pod_name is None:\n        async with self.k8s_client_manager.client() as client:\n            api = k8s_async.CoreV1Api(client)\n            pods = (\n                await api.list_namespaced_pod(\n                    namespace=namespace,\n                    label_selector=f\"spark-app-id={app_id}\",\n                )\n            ).items\n            if len(pods) == 0:\n                raise ValueError(f\"No pods found for app {app_id}\")\n            pod_name = pods[0].metadata.name\n\n    async with self.k8s_client_manager.client() as client:\n        api = k8s_async.CoreV1Api(client)\n        while True:\n            pod = await api.read_namespaced_pod(\n                namespace=namespace,\n                name=pod_name,\n            )\n            if pod.status.phase != \"Pending\":\n                break\n\n        watcher = watch.Watch()\n        log_streamer = watcher.stream(\n            api.read_namespaced_pod_log,\n            namespace=namespace,\n            name=pod_name,\n            tail_lines=tail_lines if tail_lines &gt; 0 else None,\n            follow=True,\n        )\n        async for line in log_streamer:\n            yield line\n        watcher.stop()\n</code></pre>"},{"location":"reference/spark_on_k8s/utils/async_app_manager/#spark_on_k8s.utils.async_app_manager.AsyncSparkAppManager.wait_for_app","title":"<code>wait_for_app(*, namespace, pod_name=None, app_id=None, poll_interval=10, should_print=False)</code>  <code>async</code>","text":"<p>Wait for a Spark app to finish asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>str</code> <p>Namespace.</p> required <code>pod_name</code> <code>str</code> <p>Pod name.</p> <code>None</code> <code>app_id</code> <code>str</code> <p>App ID.</p> <code>None</code> <code>poll_interval</code> <code>float</code> <p>Poll interval in seconds. Defaults to 10.</p> <code>10</code> <code>should_print</code> <code>bool</code> <p>Whether to print logs instead of logging them.</p> <code>False</code> Source code in <code>spark_on_k8s/utils/async_app_manager.py</code> <pre><code>async def wait_for_app(\n    self,\n    *,\n    namespace: str,\n    pod_name: str | None = None,\n    app_id: str | None = None,\n    poll_interval: float = 10,\n    should_print: bool = False,\n):\n    \"\"\"Wait for a Spark app to finish asynchronously.\n\n    Args:\n        namespace (str): Namespace.\n        pod_name (str): Pod name.\n        app_id (str): App ID.\n        poll_interval (float, optional): Poll interval in seconds. Defaults to 10.\n        should_print (bool, optional): Whether to print logs instead of logging them.\n    \"\"\"\n    termination_statuses = {SparkAppStatus.Succeeded, SparkAppStatus.Failed, SparkAppStatus.Unknown}\n    async with self.k8s_client_manager.client() as client:\n        api = k8s_async.CoreV1Api(client)\n        while True:\n            try:\n                status = await self.app_status(\n                    namespace=namespace, pod_name=pod_name, app_id=app_id, client=api\n                )\n                if status in termination_statuses:\n                    break\n            except ApiException as e:\n                if e.status == 404:\n                    self.log(\n                        msg=f\"Pod {pod_name} was deleted\", level=logging.INFO, should_print=should_print\n                    )\n                    return\n            self.log(\n                msg=f\"Pod {pod_name} status is {status}, sleep {poll_interval}s\",\n                level=logging.INFO,\n                should_print=should_print,\n            )\n            await asyncio.sleep(poll_interval)\n        self.log(\n            msg=f\"Pod {pod_name} finished with status {status.value}\",\n            level=logging.INFO,\n            should_print=should_print,\n        )\n</code></pre>"},{"location":"reference/spark_on_k8s/utils/configuration/","title":"configuration","text":""},{"location":"reference/spark_on_k8s/utils/configuration/#spark_on_k8s.utils.configuration.Configuration","title":"<code>Configuration</code>","text":"<p>Spark on Kubernetes configuration.</p> Source code in <code>spark_on_k8s/utils/configuration.py</code> <pre><code>class Configuration:\n    \"\"\"Spark on Kubernetes configuration.\"\"\"\n\n    SPARK_ON_K8S_DOCKER_IMAGE = getenv(\"SPARK_ON_K8S_DOCKER_IMAGE\")\n    SPARK_ON_K8S_APP_PATH = getenv(\"SPARK_ON_K8S_APP_PATH\")\n    SPARK_ON_K8S_NAMESPACE = getenv(\"SPARK_ON_K8S_NAMESPACE\", \"default\")\n    SPARK_ON_K8S_SERVICE_ACCOUNT = getenv(\"SPARK_ON_K8S_SERVICE_ACCOUNT\", \"spark\")\n    SPARK_ON_K8S_APP_NAME = getenv(\"SPARK_ON_K8S_APP_NAME\")\n    SPARK_ON_K8S_SPARK_CONF = json.loads(getenv(\"SPARK_ON_K8S_SPARK_CONF\", \"{}\"))\n    SPARK_ON_K8S_CLASS_NAME = getenv(\"SPARK_ON_K8S_CLASS_NAME\")\n    SPARK_ON_K8S_APP_ARGUMENTS = json.loads(getenv(\"SPARK_ON_K8S_APP_ARGUMENTS\", \"[]\"))\n    SPARK_ON_K8S_APP_WAITER = getenv(\"SPARK_ON_K8S_APP_WAITER\", \"no_wait\")\n    SPARK_ON_K8S_IMAGE_PULL_POLICY = getenv(\"SPARK_ON_K8S_IMAGE_PULL_POLICY\", \"IfNotPresent\")\n    SPARK_ON_K8S_UI_REVERSE_PROXY = getenv(\"SPARK_ON_K8S_UI_REVERSE_PROXY\", \"false\").lower() == \"true\"\n    SPARK_ON_K8S_DRIVER_CPU = int(getenv(\"SPARK_ON_K8S_DRIVER_CPU\", 1))\n    SPARK_ON_K8S_DRIVER_MEMORY = int(getenv(\"SPARK_ON_K8S_DRIVER_MEMORY\", 1024))\n    SPARK_ON_K8S_DRIVER_MEMORY_OVERHEAD = int(getenv(\"SPARK_ON_K8S_DRIVER_MEMORY_OVERHEAD\", 512))\n    SPARK_ON_K8S_EXECUTOR_CPU = int(getenv(\"SPARK_ON_K8S_EXECUTOR_CPU\", 1))\n    SPARK_ON_K8S_EXECUTOR_MEMORY = int(getenv(\"SPARK_ON_K8S_EXECUTOR_MEMORY\", 1024))\n    SPARK_ON_K8S_EXECUTOR_MEMORY_OVERHEAD = int(getenv(\"SPARK_ON_K8S_EXECUTOR_MEMORY_OVERHEAD\", 512))\n    SPARK_ON_K8S_EXECUTOR_MIN_INSTANCES = (\n        int(getenv(\"SPARK_ON_K8S_EXECUTOR_MIN_INSTANCES\"))\n        if getenv(\"SPARK_ON_K8S_EXECUTOR_MIN_INSTANCES\")\n        else None\n    )\n    SPARK_ON_K8S_EXECUTOR_MAX_INSTANCES = (\n        int(getenv(\"SPARK_ON_K8S_EXECUTOR_MAX_INSTANCES\"))\n        if getenv(\"SPARK_ON_K8S_EXECUTOR_MAX_INSTANCES\")\n        else None\n    )\n    SPARK_ON_K8S_EXECUTOR_INITIAL_INSTANCES = (\n        int(getenv(\"SPARK_ON_K8S_EXECUTOR_INITIAL_INSTANCES\"))\n        if getenv(\"SPARK_ON_K8S_EXECUTOR_INITIAL_INSTANCES\")\n        else None\n    )\n    SPARK_ON_K8S_SECRET_ENV_VAR = json.loads(getenv(\"SPARK_ON_K8S_SECRET_ENV_VAR\", \"{}\"))\n    SPARK_ON_K8S_DRIVER_ENV_VARS_FROM_SECRET = (\n        getenv(\"SPARK_ON_K8S_DRIVER_ENV_VARS_FROM_SECRET\").split(\",\")\n        if getenv(\"SPARK_ON_K8S_DRIVER_ENV_VARS_FROM_SECRET\")\n        else []\n    )\n\n    # Kubernetes client configuration\n    # K8S client configuration\n    SPARK_ON_K8S_CONFIG_FILE = getenv(\"SPARK_ON_K8S_CONFIG_FILE\", None)\n    SPARK_ON_K8S_CONTEXT = getenv(\"SPARK_ON_K8S_CONTEXT\", None)\n    SPARK_ON_K8S_CLIENT_CONFIG = (\n        k8s.Configuration(json.loads(getenv(\"SPARK_ON_K8S_CLIENT_CONFIG\")))\n        if getenv(\"SPARK_ON_K8S_CLIENT_CONFIG\", None)\n        else None\n    )\n    SPARK_ON_K8S_IN_CLUSTER = bool(getenv(\"SPARK_ON_K8S_IN_CLUSTER\", False))\n    SPARK_ON_K8S_SPARK_DRIVER_NODE_SELECTOR = json.loads(\n        getenv(\"SPARK_ON_K8S_SPARK_DRIVER_NODE_SELECTOR\", \"{}\")\n    )\n    SPARK_ON_K8S_SPARK_EXECUTOR_NODE_SELECTOR = json.loads(\n        getenv(\"SPARK_ON_K8S_SPARK_EXECUTOR_NODE_SELECTOR\", \"{}\")\n    )\n    SPARK_ON_K8S_SPARK_DRIVER_LABELS = json.loads(getenv(\"SPARK_ON_K8S_SPARK_DRIVER_LABELS\", \"{}\"))\n    SPARK_ON_K8S_SPARK_EXECUTOR_LABELS = json.loads(getenv(\"SPARK_ON_K8S_SPARK_EXECUTOR_LABELS\", \"{}\"))\n    SPARK_ON_K8S_SPARK_DRIVER_ANNOTATIONS = json.loads(getenv(\"SPARK_ON_K8S_SPARK_DRIVER_ANNOTATIONS\", \"{}\"))\n    SPARK_ON_K8S_SPARK_EXECUTOR_ANNOTATIONS = json.loads(\n        getenv(\"SPARK_ON_K8S_SPARK_EXECUTOR_ANNOTATIONS\", \"{}\")\n    )\n    SPARK_ON_K8S_EXECUTOR_POD_TEMPLATE_PATH = getenv(\"SPARK_ON_K8S_EXECUTOR_POD_TEMPLATE_PATH\", None)\n    try:\n        from kubernetes_asyncio import client as async_k8s\n\n        SPARK_ON_K8S_ASYNC_CLIENT_CONFIG = (\n            async_k8s.Configuration(json.loads(getenv(\"SPARK_ON_K8S_ASYNC_CLIENT_CONFIG\")))\n            if getenv(\"SPARK_ON_K8S_ASYNC_CLIENT_CONFIG\", None)\n            else None\n        )\n    except ImportError:\n        SPARK_ON_K8S_ASYNC_CLIENT_CONFIG = None\n</code></pre>"},{"location":"reference/spark_on_k8s/utils/logging_mixin/","title":"logging_mixin","text":""},{"location":"reference/spark_on_k8s/utils/setup_namespace/","title":"setup_namespace","text":""},{"location":"reference/spark_on_k8s/utils/setup_namespace/#spark_on_k8s.utils.setup_namespace.SparkOnK8SNamespaceSetup","title":"<code>SparkOnK8SNamespaceSetup</code>","text":"<p>               Bases: <code>LoggingMixin</code></p> <p>Utility class to set up a namespace for Spark on Kubernetes.</p> <p>Parameters:</p> Name Type Description Default <code>k8s_client_manager</code> <code>KubernetesClientManager</code> <p>Kubernetes client manager. Defaults to None.</p> <code>None</code> <code>logger_name</code> <code>str</code> <p>logger name. Defaults to \"SparkOnK8SNamespaceSetup\".</p> <code>None</code> Source code in <code>spark_on_k8s/utils/setup_namespace.py</code> <pre><code>class SparkOnK8SNamespaceSetup(LoggingMixin):\n    \"\"\"Utility class to set up a namespace for Spark on Kubernetes.\n\n    Args:\n        k8s_client_manager (KubernetesClientManager, optional): Kubernetes client manager.\n            Defaults to None.\n        logger_name (str, optional): logger name. Defaults to \"SparkOnK8SNamespaceSetup\".\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        k8s_client_manager: KubernetesClientManager | None = None,\n        logger_name: str | None = None,\n    ):\n        super().__init__(logger_name=logger_name or \"SparkOnK8SNamespaceSetup\")\n        self.k8s_client_manager = k8s_client_manager or KubernetesClientManager()\n\n    def setup_namespace(self, namespace: str, should_print: bool = False):\n        \"\"\"Set up a namespace for Spark on Kubernetes.\n\n        This method creates a namespace if it doesn't exist, creates a service account for Spark\n        if it doesn't exist, and creates a cluster role binding for the service account and the\n        edit cluster role if it doesn't exist.\n\n        Args:\n            namespace (str): the namespace to set up\n            should_print (bool, optional): whether to print logs instead of logging them.\n                Defaults to False.\n        \"\"\"\n        with self.k8s_client_manager.client() as client:\n            api = k8s.CoreV1Api(client)\n            namespaces = [ns.metadata.name for ns in api.list_namespace().items]\n            if namespace not in namespaces:\n                self.log(msg=f\"Creating namespace {namespace}\", level=logging.INFO, should_print=should_print)\n                api.create_namespace(\n                    body=k8s.V1Namespace(\n                        metadata=k8s.V1ObjectMeta(\n                            name=namespace,\n                        ),\n                    ),\n                )\n            service_accounts = [\n                sa.metadata.name for sa in api.list_namespaced_service_account(namespace=namespace).items\n            ]\n            if \"spark\" not in service_accounts:\n                self.log(\n                    msg=f\"Creating spark service account in namespace {namespace}\",\n                    level=logging.INFO,\n                    should_print=should_print,\n                )\n                api.create_namespaced_service_account(\n                    namespace=namespace,\n                    body=k8s.V1ServiceAccount(\n                        metadata=k8s.V1ObjectMeta(\n                            name=\"spark\",\n                        ),\n                    ),\n                )\n            rbac_api = k8s.RbacAuthorizationV1Api(client)\n            cluster_role_bindings = [crb.metadata.name for crb in rbac_api.list_cluster_role_binding().items]\n            role_binding_name = f\"spark-role-binding-{namespace}\"\n            if role_binding_name not in cluster_role_bindings:\n                self.log(msg=\"Creating spark role binding\", level=logging.INFO, should_print=should_print)\n                rbac_api.create_cluster_role_binding(\n                    body=k8s.V1ClusterRoleBinding(\n                        metadata=k8s.V1ObjectMeta(\n                            name=role_binding_name,\n                        ),\n                        role_ref=k8s.V1RoleRef(\n                            api_group=\"rbac.authorization.k8s.io\",\n                            kind=\"ClusterRole\",\n                            name=\"edit\",\n                        ),\n                        subjects=[\n                            k8s.RbacV1Subject(\n                                kind=\"ServiceAccount\",\n                                name=\"spark\",\n                                namespace=namespace,\n                            )\n                        ],\n                    ),\n                )\n</code></pre>"},{"location":"reference/spark_on_k8s/utils/setup_namespace/#spark_on_k8s.utils.setup_namespace.SparkOnK8SNamespaceSetup.setup_namespace","title":"<code>setup_namespace(namespace, should_print=False)</code>","text":"<p>Set up a namespace for Spark on Kubernetes.</p> <p>This method creates a namespace if it doesn't exist, creates a service account for Spark if it doesn't exist, and creates a cluster role binding for the service account and the edit cluster role if it doesn't exist.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>str</code> <p>the namespace to set up</p> required <code>should_print</code> <code>bool</code> <p>whether to print logs instead of logging them. Defaults to False.</p> <code>False</code> Source code in <code>spark_on_k8s/utils/setup_namespace.py</code> <pre><code>def setup_namespace(self, namespace: str, should_print: bool = False):\n    \"\"\"Set up a namespace for Spark on Kubernetes.\n\n    This method creates a namespace if it doesn't exist, creates a service account for Spark\n    if it doesn't exist, and creates a cluster role binding for the service account and the\n    edit cluster role if it doesn't exist.\n\n    Args:\n        namespace (str): the namespace to set up\n        should_print (bool, optional): whether to print logs instead of logging them.\n            Defaults to False.\n    \"\"\"\n    with self.k8s_client_manager.client() as client:\n        api = k8s.CoreV1Api(client)\n        namespaces = [ns.metadata.name for ns in api.list_namespace().items]\n        if namespace not in namespaces:\n            self.log(msg=f\"Creating namespace {namespace}\", level=logging.INFO, should_print=should_print)\n            api.create_namespace(\n                body=k8s.V1Namespace(\n                    metadata=k8s.V1ObjectMeta(\n                        name=namespace,\n                    ),\n                ),\n            )\n        service_accounts = [\n            sa.metadata.name for sa in api.list_namespaced_service_account(namespace=namespace).items\n        ]\n        if \"spark\" not in service_accounts:\n            self.log(\n                msg=f\"Creating spark service account in namespace {namespace}\",\n                level=logging.INFO,\n                should_print=should_print,\n            )\n            api.create_namespaced_service_account(\n                namespace=namespace,\n                body=k8s.V1ServiceAccount(\n                    metadata=k8s.V1ObjectMeta(\n                        name=\"spark\",\n                    ),\n                ),\n            )\n        rbac_api = k8s.RbacAuthorizationV1Api(client)\n        cluster_role_bindings = [crb.metadata.name for crb in rbac_api.list_cluster_role_binding().items]\n        role_binding_name = f\"spark-role-binding-{namespace}\"\n        if role_binding_name not in cluster_role_bindings:\n            self.log(msg=\"Creating spark role binding\", level=logging.INFO, should_print=should_print)\n            rbac_api.create_cluster_role_binding(\n                body=k8s.V1ClusterRoleBinding(\n                    metadata=k8s.V1ObjectMeta(\n                        name=role_binding_name,\n                    ),\n                    role_ref=k8s.V1RoleRef(\n                        api_group=\"rbac.authorization.k8s.io\",\n                        kind=\"ClusterRole\",\n                        name=\"edit\",\n                    ),\n                    subjects=[\n                        k8s.RbacV1Subject(\n                            kind=\"ServiceAccount\",\n                            name=\"spark\",\n                            namespace=namespace,\n                        )\n                    ],\n                ),\n            )\n</code></pre>"},{"location":"reference/spark_on_k8s/utils/spark_app_status/","title":"spark_app_status","text":""},{"location":"reference/spark_on_k8s/utils/spark_app_status/#spark_on_k8s.utils.spark_app_status.SparkAppStatus","title":"<code>SparkAppStatus</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Spark app status.</p> Source code in <code>spark_on_k8s/utils/spark_app_status.py</code> <pre><code>class SparkAppStatus(str, Enum):\n    \"\"\"Spark app status.\"\"\"\n\n    Pending = \"Pending\"\n    Running = \"Running\"\n    Succeeded = \"Succeeded\"\n    Failed = \"Failed\"\n    Unknown = \"Unknown\"\n</code></pre>"},{"location":"reference/spark_on_k8s/utils/spark_app_status/#spark_on_k8s.utils.spark_app_status.get_app_status","title":"<code>get_app_status(pod)</code>","text":"<p>Get app status.</p> Source code in <code>spark_on_k8s/utils/spark_app_status.py</code> <pre><code>def get_app_status(pod: k8s.V1Pod | k8s_async.V1Pod) -&gt; SparkAppStatus:\n    \"\"\"Get app status.\"\"\"\n    if pod.status.phase == \"Pending\":\n        return SparkAppStatus.Pending\n    elif pod.status.phase == \"Running\":\n        return SparkAppStatus.Running\n    elif pod.status.phase == \"Succeeded\":\n        return SparkAppStatus.Succeeded\n    elif pod.status.phase == \"Failed\":\n        return SparkAppStatus.Failed\n    else:\n        return SparkAppStatus.Unknown\n</code></pre>"},{"location":"reference/spark_on_k8s/utils/types/","title":"types","text":""},{"location":"reference/spark_on_k8s/utils/types/#spark_on_k8s.utils.types.ArgNotSet","title":"<code>ArgNotSet</code>","text":"<p>A type used to indicate that an argument was not set.</p> Source code in <code>spark_on_k8s/utils/types.py</code> <pre><code>class ArgNotSet:\n    \"\"\"A type used to indicate that an argument was not set.\"\"\"\n</code></pre>"},{"location":"reference/spark_on_k8s/utils/warnings/","title":"warnings","text":""},{"location":"user-guide/examples/","title":"Examples","text":"<p>Here are some examples of how to package and submit spark apps with this package. In the examples, the base image is built with the spark image tool, as described in the spark documentation.</p>"},{"location":"user-guide/examples/#python","title":"Python","text":"<p>In this example, we use a small PySpark application that takes a parameter <code>num_points</code> to calculate the value of Pi: <pre><code>from __future__ import annotations\n\nimport random\nimport sys\n\nfrom pyspark.sql import SparkSession\n\nif __name__ == \"__main__\":\n    # Check if the number of arguments is correct\n    if len(sys.argv) != 2:\n        print(\"Usage: python script.py &lt;num_points&gt;\")\n        sys.exit(1)\n\n    # Get the number of points from command-line arguments\n    num_points = int(sys.argv[1])\n\n    # Create a Spark session\n    spark = SparkSession.builder.appName(\"Pi-Estimation\").getOrCreate()\n\n    # Generate random points within the unit square\n    points = spark.sparkContext.parallelize(range(1, num_points + 1)).map(\n        lambda _: (random.random(), random.random())\n    )\n\n    # Count points within the unit circle\n    inside_circle = points.filter(lambda point: point[0] ** 2 + point[1] ** 2 &lt;= 1)\n\n    # Estimate Pi\n    pi_estimate = 4 * inside_circle.count() / num_points\n\n    # Display the result\n    print(f\"Pi is approximately {pi_estimate}\")\n\n    # Stop the Spark session\n    spark.stop()\n</code></pre> and a Dockerfile to package the application with the spark image: <pre><code>FROM husseinawala/spark-py:v3.5.0\n\nADD job.py /opt/spark/work-dir/\n</code></pre></p> <p>First, build the docker image and push it to a registry accessible by your cluster, or load it into your cluster's local registry if you're using minikube or kind: <pre><code>docker build -t pyspark-job examples/python\n\n# For minikube\nminikube image load pyspark-job\n# For kind\nkind load docker-image pyspark-job\n# For remote clusters, you will need to change the image name to match your registry,\n# and then push it to that registry\ndocker push pyspark-job\n</code></pre> Then, you can submit the job using the python client: <pre><code>from __future__ import annotations\n\nimport logging\n\nfrom spark_on_k8s.client import ExecutorInstances, PodResources, SparkOnK8S\n\nif __name__ == \"__main__\":\n    logging.basicConfig(level=logging.INFO)\n    spark_client = SparkOnK8S()\n    spark_client.submit_app(\n        image=\"pyspark-job\",\n        app_path=\"local:///opt/spark/work-dir/job.py\",\n        app_arguments=[\"100000\"],\n        app_name=\"pyspark-job-example\",\n        namespace=\"spark\",\n        service_account=\"spark\",\n        app_waiter=\"log\",\n        # If you test this locally (minikube or kind) without pushing the image to a registry,\n        # you need to set the image_pull_policy to Never.\n        image_pull_policy=\"Never\",\n        ui_reverse_proxy=True,\n        driver_resources=PodResources(cpu=1, memory=512, memory_overhead=128),\n        executor_resources=PodResources(cpu=1, memory=512, memory_overhead=128),\n        # Run with 5 executors\n        executor_instances=ExecutorInstances(initial=5),\n    )\n</code></pre> or using the CLI: <pre><code>spark-on-k8s app submit \\\n  --image pyspark-job \\\n  --path local:///opt/spark/work-dir/job.py \\\n  --namespace spark \\\n  --name pyspark-job-example \\\n  --service-account spark \\\n  --image-pull-policy Never \\\n  --driver-cpu 1 \\\n  --driver-memory 512 \\\n  --driver-memory-overhead 128 \\\n  --executor-cpu 1 \\\n  --executor-memory 512 \\\n  --executor-memory-overhead 128 \\\n  --executor-initial-instances 5 \\\n  --ui-reverse-proxy \\\n  --logs \\\n  100000\n</code></pre></p>"},{"location":"user-guide/examples/#java","title":"Java","text":"<p>This example is similar to the previous one, but it's implemented in java (with maven).</p> <p>pom.xml: <pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\"\n         xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt;\n    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;\n\n    &lt;groupId&gt;com.oss-tech.examples&lt;/groupId&gt;\n    &lt;artifactId&gt;java-job&lt;/artifactId&gt;\n    &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;\n\n    &lt;properties&gt;\n        &lt;maven.compiler.source&gt;11&lt;/maven.compiler.source&gt;\n        &lt;maven.compiler.target&gt;11&lt;/maven.compiler.target&gt;\n        &lt;spark.version&gt;3.5.0&lt;/spark.version&gt;\n    &lt;/properties&gt;\n\n    &lt;dependencies&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;\n            &lt;artifactId&gt;spark-core_2.12&lt;/artifactId&gt;\n            &lt;version&gt;${spark.version}&lt;/version&gt;\n        &lt;/dependency&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;\n            &lt;artifactId&gt;spark-sql_2.12&lt;/artifactId&gt;\n            &lt;version&gt;${spark.version}&lt;/version&gt;\n        &lt;/dependency&gt;\n    &lt;/dependencies&gt;\n&lt;/project&gt;\n</code></pre> the job class (<code>src/main/java/com/oss_tech/examples/TestJob.java</code>): <pre><code>package com.oss_tech.examples;\n\nimport org.apache.spark.sql.SparkSession;\nimport org.apache.spark.sql.Dataset;\nimport org.apache.spark.sql.Row;\n\n\npublic class TestJob {\n  public static void main(String[] args) {\n    // Check if the correct number of arguments is provided\n    if (args.length != 1) {\n      System.err.println(\"Usage: PiEstimation &lt;num_points&gt;\");\n      System.exit(1);\n    }\n\n    // Initialize Spark session\n    final SparkSession spark = SparkSession.builder()\n        .appName(\"Spark Java example\")\n        .getOrCreate();\n\n    // Get the number of points from command-line arguments\n    final int numPoints = Integer.parseInt(args[0]);\n\n    Dataset&lt;Row&gt; points = spark.range(1, numPoints + 1)\n        .selectExpr(\"rand() as x\", \"rand() as y\");\n\n    // Count points within the unit circle\n    long insideCircle = points.filter(\"x * x + y * y &lt;= 1\").count();\n\n    // Estimate Pi\n    double piEstimate = 4.0 * insideCircle / numPoints;\n\n    // Display the result\n    System.out.println(\"Pi is approximately \" + piEstimate);\n  }\n}\n</code></pre></p> <p>Similar to the PySpark application, you need to build the docker image and push it to a registry accessible by your cluster, or load it into your cluster's local registry if you're using minikube or kind: <pre><code>docker build -t java-spark-job examples/java\n\n# For minikube\nminikube image load java-spark-job\n# For kind\nkind load docker-image java-spark-job\n# For remote clusters, you will need to change the image name to match your registry,\n# and then push it to that registry\ndocker push java-spark-job\n</code></pre> Then, submit the job using the python client: <pre><code>from __future__ import annotations\n\nimport logging\n\nfrom spark_on_k8s.client import ExecutorInstances, SparkOnK8S\n\nif __name__ == \"__main__\":\n    logging.basicConfig(level=logging.INFO)\n    spark_client = SparkOnK8S()\n    spark_client.submit_app(\n        image=\"java-spark-job\",\n        app_path=\"local:///java-job.jar\",\n        app_arguments=[\"100000\"],\n        app_name=\"spark-java-job-example\",\n        namespace=\"spark\",\n        service_account=\"spark\",\n        app_waiter=\"log\",\n        class_name=\"com.oss_tech.examples.TestJob\",\n        # If you test this locally (minikube or kind) without pushing the image to a registry,\n        # you need to set the image_pull_policy to Never.\n        image_pull_policy=\"Never\",\n        ui_reverse_proxy=True,\n        # Run with dynamic allocation enabled, with minimum of 0 executors and maximum of 5 executors\n        executor_instances=ExecutorInstances(min=0, max=5),\n    )\n</code></pre> or using the CLI: <pre><code>spark-on-k8s app submit \\\n  --image java-spark-job \\\n  --path local:///java-job.jar \\\n  --namespace spark \\\n  --name spark-java-job-example \\\n  --service-account spark \\\n  --image-pull-policy Never \\\n  --executor-min-instances 0 \\\n  --executor-max-instances 5 \\\n  --executor-initial-instances 5 \\\n  --ui-reverse-proxy \\\n  --logs \\\n  --class com.oss_tech.examples.TestJob \\\n  100000\n</code></pre></p>"},{"location":"user-guide/getting_started/","title":"Getting Started","text":"<p>This guide will help you get started with the <code>spark-on-k8s</code> package.</p>"},{"location":"user-guide/getting_started/#set-up-the-kubernetes-namespace","title":"Set up the Kubernetes namespace","text":"<p>When submitting a Spark application to Kubernetes, we only create the driver pod, which is responsible for creating and managing the executors pods. To give the driver pod the permissions to create the executors pods, we need to create a service account with the required permissions, and use it in the driver pod.</p> <p>To simplify this process, we provide a helper function that creates a namespace if needed, and a service account with the required permissions, and you have two options to use this helper:</p> <p>With Python client: <pre><code>from spark_on_k8s.utils.setup_namespace import SparkOnK8SNamespaceSetup\n\nspark_on_k8s_setuper = SparkOnK8SNamespaceSetup()\nspark_on_k8s_setuper.setup_namespace(namespace=\"&lt;namespace name&gt;\")\n</code></pre></p> <p>With the CLI: <pre><code>spark-on-k8s namespace setup -n &lt;namespace name&gt;\n</code></pre></p>"},{"location":"user-guide/getting_started/#python-client","title":"Python Client","text":"<p>The Python client can be used to submit apps from your Python code, instead of using spark-submit:</p> <pre><code>from spark_on_k8s.client import SparkOnK8S\n\nclient = SparkOnK8S()\nclient.submit_app(\n    image=\"my-registry/my-image:latest\",\n    app_path=\"local:///opt/spark/work-dir/my-app.py\",\n    app_arguments=[\"arg1\", \"arg2\"],\n    app_name=\"my-app\",\n    namespace=\"spark-namespace\",\n    service_account=\"spark-service-account\",\n    app_waiter=\"log\",\n    image_pull_policy=\"Never\",\n    ui_reverse_proxy=True,\n)\n</code></pre>"},{"location":"user-guide/getting_started/#cli","title":"CLI","text":"<p>The CLI can be used to submit apps from the command line, instead of using spark-submit, it can also be used to manage apps submitted with the Python client (list, get, delete, logs, etc.):</p> <p>Submit a app: <pre><code>spark-on-k8s app submit \\\n  --image my-registry/my-image:latest \\\n  --path local:///opt/spark/work-dir/my-app.py \\\n  -n spark \\\n  --name my-app \\\n  --image-pull-policy Never \\\n  --ui-reverse-proxy \\\n  --log \\\n  param1 param2\n</code></pre> Kill a app: <pre><code>spark-on-k8s app kill -n spark-namespace --app-id my-app\n</code></pre></p> <p>List apps: <pre><code>spark-on-k8s apps list -n spark-namespace\n</code></pre></p> <p>You can check the help for more information: <pre><code>spark-on-k8s --help\nspark-on-k8s app --help\nspark-on-k8s apps --help\n</code></pre></p>"},{"location":"user-guide/getting_started/#rest-api","title":"REST API","text":"<p>The REST API implements some of the same functionality as the CLI but in async way, and also provides a web UI that can be used to list the apps in the cluster and access the spark UI through a reverse proxy. The UI will be improved in the future and more functionality will be added to both UI and API.</p> <p>To run the API, you can use the CLI: <pre><code>spark-on-k8s api start \\\n    --host \"0.0.0.0\" \\\n    --port 8080 \\\n    --workers 4 \\\n    --log-level error \\\n    --limit-concurrency 100\n</code></pre></p> <p>To list the apps, you can use the API: <pre><code>curl -X 'GET' \\\n  'http://0.0.0.0:8080/apps/list_apps/spark-namespace' \\\n  -H 'accept: application/json'\n</code></pre></p> <p>To access the spark UI of the app APP_ID, in the namespace NAMESPACE, you can use the web UI link: <code>http://0.0.0.0:8080/webserver/ui/NAMESPACE/APP_ID</code>, or getting all the application and then clicking on the button <code>Open Spark UI</code> from the link <code>http://0.0.0.0:8080/webserver/apps?namespace=NAMESPACE</code>.</p>"},{"location":"user-guide/getting_started/#api-in-production","title":"API in production","text":"<p>To deploy the API in production, you can use the project helm chart, that setups all the required resources in the cluster, including the API deployment, the service, the ingress and the RBAC resources. The API has a configuration class that loads the configuration from environment variables, so you can use the helm chart <code>env</code> values to configure the API and its Kubernetes client.</p> <p>To install the helm chart, you can run: <pre><code>helm repo add spark-on-k8s http://hussein.awala.fr/spark-on-k8s-chart\nhelm repo update\nhelm install spark-on-k8s-release spark-on-k8s/spark-on-k8s --values examples/helm/values.yaml\n</code></pre></p>"},{"location":"user-guide/getting_started/#configuration","title":"Configuration","text":"<p>The Python client and the CLI can be configured with environment variables to avoid passing the same arguments every time if you have a common configuration for all your apps. The environment variables are the same for both the client and the CLI. Here is a list of the available environment variables:</p> Environment Variable Description Default SPARK_ON_K8S_DOCKER_IMAGE The docker image to use for the spark pods SPARK_ON_K8S_APP_PATH The path to the app file SPARK_ON_K8S_NAMESPACE The namespace to use default SPARK_ON_K8S_SERVICE_ACCOUNT The service account to use spark SPARK_ON_K8S_SPARK_CONF The spark configuration to use {} SPARK_ON_K8S_CLASS_NAME The class name to use SPARK_ON_K8S_APP_ARGUMENTS The arguments to pass to the app [] SPARK_ON_K8S_APP_WAITER The waiter to use to wait for the app to finish no_wait SPARK_ON_K8S_IMAGE_PULL_POLICY The image pull policy to use IfNotPresent SPARK_ON_K8S_UI_REVERSE_PROXY Whether to use a reverse proxy to access the spark UI false SPARK_ON_K8S_DRIVER_CPU The driver CPU 1 SPARK_ON_K8S_DRIVER_MEMORY The driver memory 1024 SPARK_ON_K8S_DRIVER_MEMORY_OVERHEAD The driver memory overhead 512 SPARK_ON_K8S_EXECUTOR_CPU The executor CPU 1 SPARK_ON_K8S_EXECUTOR_MEMORY The executor memory 1024 SPARK_ON_K8S_EXECUTOR_MEMORY_OVERHEAD The executor memory overhead 512 SPARK_ON_K8S_EXECUTOR_MIN_INSTANCES The minimum number of executor instances SPARK_ON_K8S_EXECUTOR_MAX_INSTANCES The maximum number of executor instances SPARK_ON_K8S_EXECUTOR_INITIAL_INSTANCES The initial number of executor instances SPARK_ON_K8S_CONFIG_FILE The path to the config file SPARK_ON_K8S_CONTEXT The context to use SPARK_ON_K8S_CLIENT_CONFIG The sync Kubernetes client configuration to use SPARK_ON_K8S_ASYNC_CLIENT_CONFIG The async Kubernetes client configuration to use SPARK_ON_K8S_IN_CLUSTER Whether to use the in cluster Kubernetes config false SPARK_ON_K8S_API_DEFAULT_NAMESPACE The default namespace to use for the API default SPARK_ON_K8S_API_HOST The host to use for the API 127.0.0.1 SPARK_ON_K8S_API_PORT The port to use for the API 8000 SPARK_ON_K8S_API_WORKERS The number of workers to use for the API 4 SPARK_ON_K8S_API_LOG_LEVEL The log level to use for the API info SPARK_ON_K8S_API_LIMIT_CONCURRENCY The limit concurrency to use for the API 1000 SPARK_ON_K8S_API_SPARK_HISTORY_HOST The host to use for the spark history server SPARK_ON_K8S_SPARK_DRIVER_NODE_SELECTOR The node selector to use for the driver pod {} SPARK_ON_K8S_SPARK_EXECUTOR_NODE_SELECTOR The node selector to use for the executor pods {} SPARK_ON_K8S_SPARK_DRIVER_LABELS The labels to use for the driver pod {} SPARK_ON_K8S_SPARK_EXECUTOR_LABELS The labels to use for the executor pods {} SPARK_ON_K8S_SPARK_DRIVER_ANNOTATIONS The annotations to use for the driver pod {} SPARK_ON_K8S_SPARK_EXECUTOR_ANNOTATIONS The annotations to use for the executor pods {} SPARK_ON_K8S_EXECUTOR_POD_TEMPLATE_PATH The path to the executor pod template"},{"location":"user-guide/installation/","title":"Installation","text":"<p>To install the core python package (only the Python client, the CLI and the helpers), run: <pre><code>pip install spark-on-k8s\n</code></pre> If you want to use the REST API and the web UI, you will also need to install the api package: <pre><code>pip install spark-on-k8s[api]\n</code></pre></p> <p>You can also install the package from source with pip or poetry: <pre><code># With pip\npip install . # For the core package\npip install \".[api]\" # For the API package\n\n# With poetry\npoetry install # For the core package\npoetry install -E api # For the API package\n</code></pre></p>"}]}